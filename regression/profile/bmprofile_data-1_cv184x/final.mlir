#loc = loc(unknown)
#loc1 = loc("x.1")
module @blazeface attributes {module.FLOPs = 63714176 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "cv184x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["x.1"], module.mode = "F16", module.outputs = ["199_Concat_f32", "180_Concat_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.q_symmetric = false, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "blazeface_tpu_addressed_cv184x_f16_weight.npz"} {
  module @blazeface attributes {module.coeff_addr = 1101659111424 : i64, module.coeff_size = 454656 : i64, module.device_id = 0 : i64, module.dynamic_coeff_offset = 454656 : i64, module.neuron_addr = 2201170739200 : i64, module.neuron_size = 909312 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x128x128xf32> loc(unknown)) -> (tensor<1x896x16xf32, 2201170739200 : i64>, tensor<1x896x1xf32, 2201170825216 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = false, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "bgr", resize_dims = [128, 128], scale = [1.000000e+00, 1.000000e+00, 1.000000e+00], yuv_type = ""} : (tensor<1x3x128x128xf32>) -> tensor<1x3x128x128xf32, 2201171394560 : i64> loc(#loc1)
      %1:2 = call @subfunc_0(%0) : (tensor<1x3x128x128xf32, 2201171394560 : i64>) -> (tensor<1x896x16xf32, 2201170739200 : i64>, tensor<1x896x1xf32, 2201170825216 : i64>) loc(#loc)
      return %1#0, %1#1 : tensor<1x896x16xf32, 2201170739200 : i64>, tensor<1x896x1xf32, 2201170825216 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x128x128xf32, 2201171394560 : i64> loc("x.1")) -> (tensor<1x896x16xf32, 2201170739200 : i64>, tensor<1x896x1xf32, 2201170825216 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x24x1x1xf32, 1101659111424 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x24x1x200xbf16, 1101659115520 : i64> loc(#loc3)
      %3 = "tpu.Group"(%arg0) ({
        %115 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 23552, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 43, 85], h_slice = [46, 45, 43], w_idx = [0], w_slice = [128], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x3x128x128xf32, 2201171394560 : i64>) -> tensor<1x3x128x128xf32> loc(#loc5)
        %116 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1200, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [200], id = 1, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x1x200xbf16, 1101659115520 : i64>) -> tensor<1x24x1x200xbf16> loc(#loc6)
        %117 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 12, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x1x1xf32, 1101659111424 : i64>) -> tensor<1x24x1x1xf32> loc(#loc7)
        %118 = "tpu.Cast"(%115) {ginfo = #tpu.lg<out_addr = 24576, out_size = 11776, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 43, 85], h_slice = [46, 45, 43], w_idx = [0], w_slice = [128], id = 3, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x128x128xf32>) -> tensor<1x3x128x128xbf16> loc(#loc8)
        %119 = "tpu.Conv2D"(%118, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = true, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 36864, out_size = 8448, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 22, 43], h_slice = [22, 21, 21], w_idx = [0], w_slice = [64], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [5, 5], kernel_zp = 0 : i64, multicore = true, pads = [1, 1, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x128x128xbf16>, tensor<1x24x1x200xbf16>, tensor<1x24x1x1xf32>) -> tensor<1x24x64x64xbf16> loc(#loc4)
        %120 = "tpu.Store"(%119, %0) {ginfo = #tpu.lg<out_addr = 36864, out_size = 8448, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 22, 43], h_slice = [22, 21, 21], w_idx = [0], w_slice = [64], id = 5, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x24x64x64xbf16>, none) -> tensor<1x24x64x64xbf16, 2201170935808 : i64> loc(#loc4)
        "tpu.Yield"(%120) : (tensor<1x24x64x64xbf16, 2201170935808 : i64>) -> () loc(#loc4)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 1, 2, 5, -2, 4, 0], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x128x128xf32, 2201171394560 : i64>) -> tensor<1x24x64x64xbf16, 2201170935808 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x24x3x3xbf16, 1101659127808 : i64> loc(#loc9)
      %5 = "top.Weight"() : () -> tensor<1x24x1x1xbf16, 1101659131904 : i64> loc(#loc10)
      %6 = "tpu.Conv2D"(%3, %4, %5) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 24 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x24x64x64xbf16, 2201170935808 : i64>, tensor<1x24x3x3xbf16, 1101659127808 : i64>, tensor<1x24x1x1xbf16, 1101659131904 : i64>) -> tensor<1x24x64x64xbf16, 2201170739200 : i64> loc(#loc11)
      %7 = "top.Weight"() : () -> tensor<1x24x1x1xf32, 1101659136000 : i64> loc(#loc12)
      %8 = "top.Weight"() : () -> tensor<1x24x1x24xbf16, 1101659140096 : i64> loc(#loc13)
      %9 = "tpu.Group"(%6, %3) ({
        %115 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x64x64xbf16, 2201170739200 : i64>) -> tensor<1x24x64x64xbf16> loc(#loc15)
        %116 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [24], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x1x24xbf16, 1101659140096 : i64>) -> tensor<1x24x1x24xbf16> loc(#loc16)
        %117 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 12, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x1x1xf32, 1101659136000 : i64>) -> tensor<1x24x1x1xf32> loc(#loc17)
        %118 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x64x64xbf16, 2201170935808 : i64>) -> tensor<1x24x64x64xbf16> loc(#loc18)
        %119 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x24x64x64xbf16>, tensor<1x24x1x24xbf16>, tensor<1x24x1x1xf32>) -> tensor<1x24x64x64xbf16> loc(#loc19)
        %120 = "tpu.Add"(%119, %118) {do_relu = true, ginfo = #tpu.lg<out_addr = 36864, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 5, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x24x64x64xbf16>, tensor<1x24x64x64xbf16>) -> tensor<1x24x64x64xbf16> loc(#loc14)
        %121 = "tpu.Store"(%120, %0) {ginfo = #tpu.lg<out_addr = 36864, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 6, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x24x64x64xbf16>, none) -> tensor<1x24x64x64xbf16, 2201171197952 : i64> loc(#loc14)
        "tpu.Yield"(%121) : (tensor<1x24x64x64xbf16, 2201171197952 : i64>) -> () loc(#loc14)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 4, 3, 6, -2, 5, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x24x64x64xbf16, 2201170739200 : i64>, tensor<1x24x64x64xbf16, 2201170935808 : i64>) -> tensor<1x24x64x64xbf16, 2201171197952 : i64> loc(#loc14)
      %10 = "tpu.Pad"(%9, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 4, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x24x64x64xbf16, 2201171197952 : i64>, none, none, none, none) -> tensor<1x28x64x64xbf16, 2201170968576 : i64> loc(#loc20)
      %11 = "top.Weight"() : () -> tensor<1x24x3x3xbf16, 1101659144192 : i64> loc(#loc21)
      %12 = "top.Weight"() : () -> tensor<1x24x1x1xbf16, 1101659148288 : i64> loc(#loc22)
      %13 = "top.Weight"() : () -> tensor<1x28x1x1xf32, 1101659152384 : i64> loc(#loc23)
      %14 = "top.Weight"() : () -> tensor<1x28x1x24xbf16, 1101659156480 : i64> loc(#loc24)
      %15:2 = "tpu.Group"(%9, %10) ({
        %115 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12672, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 31], h_slice = [33, 33], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x64x64xbf16, 2201171197952 : i64>) -> tensor<1x24x64x64xbf16> loc(#loc27)
        %116 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12864, out_size = 54, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x3x3xbf16, 1101659144192 : i64>) -> tensor<1x24x3x3xbf16> loc(#loc28)
        %117 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12944, out_size = 6, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x24x1x1xbf16, 1101659148288 : i64>) -> tensor<1x24x1x1xbf16> loc(#loc29)
        %118 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12672, out_size = 192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [24], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x1x24xbf16, 1101659156480 : i64>) -> tensor<1x28x1x24xbf16> loc(#loc30)
        %119 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12928, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x1x1xf32, 1101659152384 : i64>) -> tensor<1x28x1x1xf32> loc(#loc31)
        %120 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 12288, buffer_addr = 20480, buffer_size = 28831, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [24], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 24 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x24x64x64xbf16>, tensor<1x24x3x3xbf16>, tensor<1x24x1x1xbf16>) -> tensor<1x24x64x64xbf16> loc(#loc32)
        %121 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x64x64xbf16, 2201170968576 : i64>) -> tensor<1x28x64x64xbf16> loc(#loc33)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x24x64x64xbf16>, tensor<1x28x1x24xbf16>, tensor<1x28x1x1xf32>) -> tensor<1x28x64x64xbf16> loc(#loc34)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 49152, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x28x64x64xbf16>, tensor<1x28x64x64xbf16>) -> tensor<1x28x64x64xbf16> loc(#loc25)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 49152, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [32, 32], w_idx = [0], w_slice = [64], id = 9, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x28x64x64xbf16>, none) -> tensor<1x28x64x64xbf16, 2201170739200 : i64> loc(#loc25)
        %125 = "tpu.Pool2D"(%123) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 16384, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x28x64x64xbf16>) -> tensor<1x28x32x32xbf16> loc(#loc26)
        %126 = "tpu.Store"(%125, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x28x32x32xbf16>, none) -> tensor<1x28x32x32xbf16, 2201171591168 : i64> loc(#loc26)
        "tpu.Yield"(%124, %126) : (tensor<1x28x64x64xbf16, 2201170739200 : i64>, tensor<1x28x32x32xbf16, 2201171591168 : i64>) -> () loc(#loc247)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 11, -2, 7, 6, -3, 8, 0, -4, 10, 9, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x24x64x64xbf16, 2201171197952 : i64>, tensor<1x28x64x64xbf16, 2201170968576 : i64>) -> (tensor<1x28x64x64xbf16, 2201170739200 : i64>, tensor<1x28x32x32xbf16, 2201171591168 : i64>) loc(#loc247)
      %16 = "tpu.Pad"(%15#1, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 4, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x28x32x32xbf16, 2201171591168 : i64>, none, none, none, none) -> tensor<1x32x32x32xbf16, 2201171050496 : i64> loc(#loc35)
      %17 = "top.Weight"() : () -> tensor<1x28x3x3xbf16, 1101659160576 : i64> loc(#loc36)
      %18 = "top.Weight"() : () -> tensor<1x28x1x1xbf16, 1101659164672 : i64> loc(#loc37)
      %19 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 1101659168768 : i64> loc(#loc38)
      %20 = "top.Weight"() : () -> tensor<1x32x1x32xbf16, 1101659172864 : i64> loc(#loc39)
      %21 = "tpu.Group"(%15#0, %16) ({
        %115 = "tpu.Load"(%15#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 16896, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 32], h_slice = [33, 32], w_idx = [0], w_slice = [64], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x64x64xbf16, 2201170739200 : i64>) -> tensor<1x28x64x64xbf16> loc(#loc41)
        %116 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x3x3xbf16, 1101659160576 : i64>) -> tensor<1x28x3x3xbf16> loc(#loc42)
        %117 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 57344, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x28x1x1xbf16, 1101659164672 : i64>) -> tensor<1x28x1x1xbf16> loc(#loc43)
        %118 = "tpu.Load"(%20) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x32xbf16, 1101659172864 : i64>) -> tensor<1x32x1x32xbf16> loc(#loc44)
        %119 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 1101659168768 : i64>) -> tensor<1x32x1x1xf32> loc(#loc45)
        %120 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 4096, buffer_addr = 24576, buffer_size = 12456, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [28], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 28 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [0, 0, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x28x64x64xbf16>, tensor<1x28x3x3xbf16>, tensor<1x28x1x1xbf16>) -> tensor<1x28x32x32xbf16> loc(#loc46)
        %121 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x32x32xbf16, 2201171050496 : i64>) -> tensor<1x32x32x32xbf16> loc(#loc47)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x28x32x32xbf16>, tensor<1x32x1x32xbf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x32x32xbf16> loc(#loc48)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 40960, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x32x32xbf16>, tensor<1x32x32x32xbf16>) -> tensor<1x32x32x32xbf16> loc(#loc40)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 40960, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 16], h_slice = [16, 16], w_idx = [0], w_slice = [32], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x32x32x32xbf16>, none) -> tensor<1x32x32x32xbf16, 2201170984960 : i64> loc(#loc40)
        "tpu.Yield"(%124) : (tensor<1x32x32x32xbf16, 2201170984960 : i64>) -> () loc(#loc40)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 2 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x28x64x64xbf16, 2201170739200 : i64>, tensor<1x32x32x32xbf16, 2201171050496 : i64>) -> tensor<1x32x32x32xbf16, 2201170984960 : i64> loc(#loc40)
      %22 = "tpu.Pad"(%21, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 4, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x32x32x32xbf16, 2201170984960 : i64>, none, none, none, none) -> tensor<1x36x32x32xbf16, 2201170739200 : i64> loc(#loc49)
      %23 = "top.Weight"() : () -> tensor<1x32x3x3xbf16, 1101659176960 : i64> loc(#loc50)
      %24 = "top.Weight"() : () -> tensor<1x32x1x1xbf16, 1101659181056 : i64> loc(#loc51)
      %25 = "top.Weight"() : () -> tensor<1x36x1x1xf32, 1101659185152 : i64> loc(#loc52)
      %26 = "top.Weight"() : () -> tensor<1x36x1x32xbf16, 1101659189248 : i64> loc(#loc53)
      %27 = "tpu.Group"(%21, %22) ({
        %115 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x32x32xbf16, 2201170984960 : i64>) -> tensor<1x32x32x32xbf16> loc(#loc55)
        %116 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 72, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x3x3xbf16, 1101659176960 : i64>) -> tensor<1x32x3x3xbf16> loc(#loc56)
        %117 = "tpu.Load"(%24) {do_bcast = false, ginfo = #tpu.lg<out_addr = 61440, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xbf16, 1101659181056 : i64>) -> tensor<1x32x1x1xbf16> loc(#loc57)
        %118 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 34816, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x1x32xbf16, 1101659189248 : i64>) -> tensor<1x36x1x32xbf16> loc(#loc58)
        %119 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 57344, out_size = 20, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x1x1xf32, 1101659185152 : i64>) -> tensor<1x36x1x1xf32> loc(#loc59)
        %120 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 36864, out_size = 8192, buffer_addr = 0, buffer_size = 18472, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 32 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x32x32xbf16>, tensor<1x32x3x3xbf16>, tensor<1x32x1x1xbf16>) -> tensor<1x32x32x32xbf16> loc(#loc60)
        %121 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x32x32xbf16, 2201170739200 : i64>) -> tensor<1x36x32x32xbf16> loc(#loc61)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x32x32xbf16>, tensor<1x36x1x32xbf16>, tensor<1x36x1x1xf32>) -> tensor<1x36x32x32xbf16> loc(#loc62)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 24576, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x36x32x32xbf16>, tensor<1x36x32x32xbf16>) -> tensor<1x36x32x32xbf16> loc(#loc54)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 24576, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x36x32x32xbf16>, none) -> tensor<1x36x32x32xbf16, 2201170911232 : i64> loc(#loc54)
        "tpu.Yield"(%124) : (tensor<1x36x32x32xbf16, 2201170911232 : i64>) -> () loc(#loc54)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x32x32x32xbf16, 2201170984960 : i64>, tensor<1x36x32x32xbf16, 2201170739200 : i64>) -> tensor<1x36x32x32xbf16, 2201170911232 : i64> loc(#loc54)
      %28 = "tpu.Pad"(%27, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 6, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x36x32x32xbf16, 2201170911232 : i64>, none, none, none, none) -> tensor<1x42x32x32xbf16, 2201170825216 : i64> loc(#loc63)
      %29 = "top.Weight"() : () -> tensor<1x36x3x3xbf16, 1101659193344 : i64> loc(#loc64)
      %30 = "top.Weight"() : () -> tensor<1x36x1x1xbf16, 1101659197440 : i64> loc(#loc65)
      %31 = "top.Weight"() : () -> tensor<1x42x1x1xf32, 1101659201536 : i64> loc(#loc66)
      %32 = "top.Weight"() : () -> tensor<1x42x1x40xbf16, 1101659205632 : i64> loc(#loc67)
      %33:2 = "tpu.Group"(%27, %28) ({
        %115 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 10240, out_size = 10, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x1x1xbf16, 1101659197440 : i64>) -> tensor<1x36x1x1xbf16> loc(#loc70)
        %116 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 10240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x32x32xbf16, 2201170911232 : i64>) -> tensor<1x36x32x32xbf16> loc(#loc71)
        %117 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 90, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x36x3x3xbf16, 1101659193344 : i64>) -> tensor<1x36x3x3xbf16> loc(#loc72)
        %118 = "tpu.Load"(%32) {do_bcast = false, ginfo = #tpu.lg<out_addr = 57344, out_size = 480, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [40], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x1x40xbf16, 1101659205632 : i64>) -> tensor<1x42x1x40xbf16> loc(#loc73)
        %119 = "tpu.Load"(%31) {do_bcast = false, ginfo = #tpu.lg<out_addr = 61440, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x1x1xf32, 1101659201536 : i64>) -> tensor<1x42x1x1xf32> loc(#loc74)
        %120 = "tpu.Conv2D"(%116, %117, %115) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 10240, buffer_addr = 20480, buffer_size = 22577, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [36], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 36 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x36x32x32xbf16>, tensor<1x36x3x3xbf16>, tensor<1x36x1x1xbf16>) -> tensor<1x36x32x32xbf16> loc(#loc75)
        %121 = "tpu.Load"(%28) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x32x32xbf16, 2201170825216 : i64>) -> tensor<1x42x32x32xbf16> loc(#loc76)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x36x32x32xbf16>, tensor<1x42x1x40xbf16>, tensor<1x42x1x1xf32>) -> tensor<1x42x32x32xbf16> loc(#loc77)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 36864, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x42x32x32xbf16>, tensor<1x42x32x32xbf16>) -> tensor<1x42x32x32xbf16> loc(#loc68)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 36864, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 9, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x42x32x32xbf16>, none) -> tensor<1x42x32x32xbf16, 2201170739200 : i64> loc(#loc68)
        %125 = "tpu.Pool2D"(%123) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 12288, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x42x32x32xbf16>) -> tensor<1x42x16x16xbf16> loc(#loc69)
        %126 = "tpu.Store"(%125, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x42x16x16xbf16>, none) -> tensor<1x42x16x16xbf16, 2201170984960 : i64> loc(#loc69)
        "tpu.Yield"(%124, %126) : (tensor<1x42x32x32xbf16, 2201170739200 : i64>, tensor<1x42x16x16xbf16, 2201170984960 : i64>) -> () loc(#loc248)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 11, -2, 7, 6, 0, -3, 8, 1, -4, 10, 9, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x36x32x32xbf16, 2201170911232 : i64>, tensor<1x42x32x32xbf16, 2201170825216 : i64>) -> (tensor<1x42x32x32xbf16, 2201170739200 : i64>, tensor<1x42x16x16xbf16, 2201170984960 : i64>) loc(#loc248)
      %34 = "tpu.Pad"(%33#1, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 6, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x42x16x16xbf16, 2201170984960 : i64>, none, none, none, none) -> tensor<1x48x16x16xbf16, 2201170849792 : i64> loc(#loc78)
      %35 = "top.Weight"() : () -> tensor<1x42x3x3xbf16, 1101659209728 : i64> loc(#loc79)
      %36 = "top.Weight"() : () -> tensor<1x42x1x1xbf16, 1101659213824 : i64> loc(#loc80)
      %37 = "top.Weight"() : () -> tensor<1x48x1x1xf32, 1101659217920 : i64> loc(#loc81)
      %38 = "top.Weight"() : () -> tensor<1x48x1x48xbf16, 1101659222016 : i64> loc(#loc82)
      %39 = "tpu.Group"(%33#0, %34) ({
        %115 = "tpu.Load"(%36) {do_bcast = false, ginfo = #tpu.lg<out_addr = 40960, out_size = 12, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x1x1xbf16, 1101659213824 : i64>) -> tensor<1x42x1x1xbf16> loc(#loc84)
        %116 = "tpu.Load"(%33#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 12288, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [32], w_idx = [0], w_slice = [32], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x32x32xbf16, 2201170739200 : i64>) -> tensor<1x42x32x32xbf16> loc(#loc85)
        %117 = "tpu.Load"(%35) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 108, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x42x3x3xbf16, 1101659209728 : i64>) -> tensor<1x42x3x3xbf16> loc(#loc86)
        %118 = "tpu.Load"(%38) {do_bcast = false, ginfo = #tpu.lg<out_addr = 31744, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [48], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x1x48xbf16, 1101659222016 : i64>) -> tensor<1x48x1x48xbf16> loc(#loc87)
        %119 = "tpu.Load"(%37) {do_bcast = false, ginfo = #tpu.lg<out_addr = 36864, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x1x1xf32, 1101659217920 : i64>) -> tensor<1x48x1x1xf32> loc(#loc88)
        %120 = "tpu.Conv2D"(%116, %117, %115) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3072, buffer_addr = 16384, buffer_size = 8250, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [42], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 42 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [0, 0, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x42x32x32xbf16>, tensor<1x42x3x3xbf16>, tensor<1x42x1x1xbf16>) -> tensor<1x42x16x16xbf16> loc(#loc89)
        %121 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x16x16xbf16, 2201170849792 : i64>) -> tensor<1x48x16x16xbf16> loc(#loc90)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x42x16x16xbf16>, tensor<1x48x1x48xbf16>, tensor<1x48x1x1xf32>) -> tensor<1x48x16x16xbf16> loc(#loc91)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 28672, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x48x16x16xbf16>, tensor<1x48x16x16xbf16>) -> tensor<1x48x16x16xbf16> loc(#loc83)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 28672, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x48x16x16xbf16>, none) -> tensor<1x48x16x16xbf16, 2201170825216 : i64> loc(#loc83)
        "tpu.Yield"(%124) : (tensor<1x48x16x16xbf16, 2201170825216 : i64>) -> () loc(#loc83)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, 0, -3, 8, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x42x32x32xbf16, 2201170739200 : i64>, tensor<1x48x16x16xbf16, 2201170849792 : i64>) -> tensor<1x48x16x16xbf16, 2201170825216 : i64> loc(#loc83)
      %40 = "tpu.Pad"(%39, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x48x16x16xbf16, 2201170825216 : i64>, none, none, none, none) -> tensor<1x56x16x16xbf16, 2201170739200 : i64> loc(#loc92)
      %41 = "top.Weight"() : () -> tensor<1x48x3x3xbf16, 1101659230208 : i64> loc(#loc93)
      %42 = "top.Weight"() : () -> tensor<1x48x1x1xbf16, 1101659234304 : i64> loc(#loc94)
      %43 = "top.Weight"() : () -> tensor<1x56x1x1xf32, 1101659238400 : i64> loc(#loc95)
      %44 = "top.Weight"() : () -> tensor<1x56x1x48xbf16, 1101659242496 : i64> loc(#loc96)
      %45 = "tpu.Group"(%39, %40) ({
        %115 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 12, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x1x1xbf16, 1101659234304 : i64>) -> tensor<1x48x1x1xbf16> loc(#loc98)
        %116 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 3072, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x16x16xbf16, 2201170825216 : i64>) -> tensor<1x48x16x16xbf16> loc(#loc99)
        %117 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 108, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x48x3x3xbf16, 1101659230208 : i64>) -> tensor<1x48x3x3xbf16> loc(#loc100)
        %118 = "tpu.Load"(%44) {do_bcast = false, ginfo = #tpu.lg<out_addr = 11776, out_size = 672, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [48], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x1x48xbf16, 1101659242496 : i64>) -> tensor<1x56x1x48xbf16> loc(#loc101)
        %119 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 28, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x1x1xf32, 1101659238400 : i64>) -> tensor<1x56x1x1xf32> loc(#loc102)
        %120 = "tpu.Conv2D"(%116, %117, %115) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3072, buffer_addr = 0, buffer_size = 6714, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [48], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 48 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x48x16x16xbf16>, tensor<1x48x3x3xbf16>, tensor<1x48x1x1xbf16>) -> tensor<1x48x16x16xbf16> loc(#loc103)
        %121 = "tpu.Load"(%40) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x16x16xbf16, 2201170739200 : i64>) -> tensor<1x56x16x16xbf16> loc(#loc104)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x48x16x16xbf16>, tensor<1x56x1x48xbf16>, tensor<1x56x1x1xf32>) -> tensor<1x56x16x16xbf16> loc(#loc105)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 8192, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x56x16x16xbf16>, tensor<1x56x16x16xbf16>) -> tensor<1x56x16x16xbf16> loc(#loc97)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x56x16x16xbf16>, none) -> tensor<1x56x16x16xbf16, 2201170771968 : i64> loc(#loc97)
        "tpu.Yield"(%124) : (tensor<1x56x16x16xbf16, 2201170771968 : i64>) -> () loc(#loc97)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, 0, -3, 8, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x48x16x16xbf16, 2201170825216 : i64>, tensor<1x56x16x16xbf16, 2201170739200 : i64>) -> tensor<1x56x16x16xbf16, 2201170771968 : i64> loc(#loc97)
      %46 = "tpu.Pad"(%45, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x56x16x16xbf16, 2201170771968 : i64>, none, none, none, none) -> tensor<1x64x16x16xbf16, 2201170739200 : i64> loc(#loc106)
      %47 = "top.Weight"() : () -> tensor<1x56x3x3xbf16, 1101659250688 : i64> loc(#loc107)
      %48 = "top.Weight"() : () -> tensor<1x56x1x1xbf16, 1101659254784 : i64> loc(#loc108)
      %49 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 1101659258880 : i64> loc(#loc109)
      %50 = "top.Weight"() : () -> tensor<1x64x1x56xbf16, 1101659262976 : i64> loc(#loc110)
      %51 = "tpu.Group"(%45, %46) ({
        %115 = "tpu.Load"(%48) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 14, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x1x1xbf16, 1101659254784 : i64>) -> tensor<1x56x1x1xbf16> loc(#loc112)
        %116 = "tpu.Load"(%47) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 126, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x3x3xbf16, 1101659250688 : i64>) -> tensor<1x56x3x3xbf16> loc(#loc113)
        %117 = "tpu.Load"(%45) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x56x16x16xbf16, 2201170771968 : i64>) -> tensor<1x56x16x16xbf16> loc(#loc114)
        %118 = "tpu.Load"(%50) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 896, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [56], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x56xbf16, 1101659262976 : i64>) -> tensor<1x64x1x56xbf16> loc(#loc115)
        %119 = "tpu.Load"(%49) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 32, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 1101659258880 : i64>) -> tensor<1x64x1x1xf32> loc(#loc116)
        %120 = "tpu.Conv2D"(%117, %116, %115) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 3584, buffer_addr = 0, buffer_size = 7747, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [56], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 56 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x56x16x16xbf16>, tensor<1x56x3x3xbf16>, tensor<1x56x1x1xbf16>) -> tensor<1x56x16x16xbf16> loc(#loc117)
        %121 = "tpu.Load"(%46) {do_bcast = false, ginfo = #tpu.lg<out_addr = 4096, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x16x16xbf16, 2201170739200 : i64>) -> tensor<1x64x16x16xbf16> loc(#loc118)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x56x16x16xbf16>, tensor<1x64x1x56xbf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x16x16xbf16> loc(#loc119)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 8192, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x16x16xbf16>, tensor<1x64x16x16xbf16>) -> tensor<1x64x16x16xbf16> loc(#loc111)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x16x16xbf16>, none) -> tensor<1x64x16x16xbf16, 2201170817024 : i64> loc(#loc111)
        "tpu.Yield"(%124) : (tensor<1x64x16x16xbf16, 2201170817024 : i64>) -> () loc(#loc111)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 9, -2, 7, 6, 0, 1, -3, 8, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x56x16x16xbf16, 2201170771968 : i64>, tensor<1x64x16x16xbf16, 2201170739200 : i64>) -> tensor<1x64x16x16xbf16, 2201170817024 : i64> loc(#loc111)
      %52 = "tpu.Pad"(%51, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x64x16x16xbf16, 2201170817024 : i64>, none, none, none, none) -> tensor<1x72x16x16xbf16, 2201170739200 : i64> loc(#loc120)
      %53 = "top.Weight"() : () -> tensor<1x64x3x3xbf16, 1101659271168 : i64> loc(#loc121)
      %54 = "top.Weight"() : () -> tensor<1x64x1x1xbf16, 1101659275264 : i64> loc(#loc122)
      %55 = "top.Weight"() : () -> tensor<1x72x1x1xf32, 1101659279360 : i64> loc(#loc123)
      %56 = "top.Weight"() : () -> tensor<1x72x1x64xbf16, 1101659283456 : i64> loc(#loc124)
      %57 = "tpu.Group"(%51, %52) ({
        %115 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x16x16xbf16, 2201170817024 : i64>) -> tensor<1x64x16x16xbf16> loc(#loc126)
        %116 = "tpu.Load"(%53) {do_bcast = false, ginfo = #tpu.lg<out_addr = 36864, out_size = 144, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x3x3xbf16, 1101659271168 : i64>) -> tensor<1x64x3x3xbf16> loc(#loc127)
        %117 = "tpu.Load"(%54) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xbf16, 1101659275264 : i64>) -> tensor<1x64x1x1xbf16> loc(#loc128)
        %118 = "tpu.Load"(%56) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x1x64xbf16, 1101659283456 : i64>) -> tensor<1x72x1x64xbf16> loc(#loc129)
        %119 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 40960, out_size = 36, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x1x1xf32, 1101659279360 : i64>) -> tensor<1x72x1x1xf32> loc(#loc130)
        %120 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4096, buffer_addr = 4608, buffer_size = 8780, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 64 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x16x16xbf16>, tensor<1x64x3x3xbf16>, tensor<1x64x1x1xbf16>) -> tensor<1x64x16x16xbf16> loc(#loc131)
        %121 = "tpu.Load"(%52) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x16x16xbf16, 2201170739200 : i64>) -> tensor<1x72x16x16xbf16> loc(#loc132)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x16x16xbf16>, tensor<1x72x1x64xbf16>, tensor<1x72x1x1xf32>) -> tensor<1x72x16x16xbf16> loc(#loc133)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x72x16x16xbf16>, tensor<1x72x16x16xbf16>) -> tensor<1x72x16x16xbf16> loc(#loc125)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x72x16x16xbf16>, none) -> tensor<1x72x16x16xbf16, 2201170780160 : i64> loc(#loc125)
        "tpu.Yield"(%124) : (tensor<1x72x16x16xbf16, 2201170780160 : i64>) -> () loc(#loc125)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, -2, 7, 6, 9, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x16x16xbf16, 2201170817024 : i64>, tensor<1x72x16x16xbf16, 2201170739200 : i64>) -> tensor<1x72x16x16xbf16, 2201170780160 : i64> loc(#loc125)
      %58 = "tpu.Pad"(%57, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x72x16x16xbf16, 2201170780160 : i64>, none, none, none, none) -> tensor<1x80x16x16xbf16, 2201170739200 : i64> loc(#loc134)
      %59 = "top.Weight"() : () -> tensor<1x72x3x3xbf16, 1101659295744 : i64> loc(#loc135)
      %60 = "top.Weight"() : () -> tensor<1x72x1x1xbf16, 1101659299840 : i64> loc(#loc136)
      %61 = "top.Weight"() : () -> tensor<1x80x1x1xf32, 1101659303936 : i64> loc(#loc137)
      %62 = "top.Weight"() : () -> tensor<1x80x1x72xbf16, 1101659308032 : i64> loc(#loc138)
      %63 = "tpu.Group"(%57, %58) ({
        %115 = "tpu.Load"(%57) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x16x16xbf16, 2201170780160 : i64>) -> tensor<1x72x16x16xbf16> loc(#loc140)
        %116 = "tpu.Load"(%59) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 162, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x3x3xbf16, 1101659295744 : i64>) -> tensor<1x72x3x3xbf16> loc(#loc141)
        %117 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 53248, out_size = 18, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x72x1x1xbf16, 1101659299840 : i64>) -> tensor<1x72x1x1xbf16> loc(#loc142)
        %118 = "tpu.Load"(%62) {do_bcast = false, ginfo = #tpu.lg<out_addr = 40960, out_size = 1440, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [72], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x1x72xbf16, 1101659308032 : i64>) -> tensor<1x80x1x72xbf16> loc(#loc143)
        %119 = "tpu.Load"(%61) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 40, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x1x1xf32, 1101659303936 : i64>) -> tensor<1x80x1x1xf32> loc(#loc144)
        %120 = "tpu.Conv2D"(%115, %116, %117) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 4608, buffer_addr = 5120, buffer_size = 9813, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [72], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 72 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x72x16x16xbf16>, tensor<1x72x3x3xbf16>, tensor<1x72x1x1xbf16>) -> tensor<1x72x16x16xbf16> loc(#loc145)
        %121 = "tpu.Load"(%58) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x16x16xbf16, 2201170739200 : i64>) -> tensor<1x80x16x16xbf16> loc(#loc146)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x72x16x16xbf16>, tensor<1x80x1x72xbf16>, tensor<1x80x1x1xf32>) -> tensor<1x80x16x16xbf16> loc(#loc147)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x80x16x16xbf16>, tensor<1x80x16x16xbf16>) -> tensor<1x80x16x16xbf16> loc(#loc139)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x80x16x16xbf16>, none) -> tensor<1x80x16x16xbf16, 2201170829312 : i64> loc(#loc139)
        "tpu.Yield"(%124) : (tensor<1x80x16x16xbf16, 2201170829312 : i64>) -> () loc(#loc139)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, -2, 7, 6, 9, -3, 8, 0, 1, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x72x16x16xbf16, 2201170780160 : i64>, tensor<1x80x16x16xbf16, 2201170739200 : i64>) -> tensor<1x80x16x16xbf16, 2201170829312 : i64> loc(#loc139)
      %64 = "tpu.Pad"(%63, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x80x16x16xbf16, 2201170829312 : i64>, none, none, none, none) -> tensor<1x88x16x16xbf16, 2201170784256 : i64> loc(#loc148)
      %65 = "top.Weight"() : () -> tensor<1x80x3x3xbf16, 1101659320320 : i64> loc(#loc149)
      %66 = "top.Weight"() : () -> tensor<1x80x1x1xbf16, 1101659324416 : i64> loc(#loc150)
      %67 = "top.Weight"() : () -> tensor<1x88x1x1xf32, 1101659328512 : i64> loc(#loc151)
      %68 = "top.Weight"() : () -> tensor<1x88x1x80xbf16, 1101659332608 : i64> loc(#loc152)
      %69:2 = "tpu.Group"(%63, %64) ({
        %115 = "tpu.Load"(%63) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5120, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x16x16xbf16, 2201170829312 : i64>) -> tensor<1x80x16x16xbf16> loc(#loc155)
        %116 = "tpu.Load"(%66) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45056, out_size = 20, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x1x1xbf16, 1101659324416 : i64>) -> tensor<1x80x1x1xbf16> loc(#loc156)
        %117 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 36864, out_size = 180, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x80x3x3xbf16, 1101659320320 : i64>) -> tensor<1x80x3x3xbf16> loc(#loc157)
        %118 = "tpu.Load"(%68) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 1760, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [80], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x1x80xbf16, 1101659332608 : i64>) -> tensor<1x88x1x80xbf16> loc(#loc158)
        %119 = "tpu.Load"(%67) {do_bcast = false, ginfo = #tpu.lg<out_addr = 40960, out_size = 44, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x1x1xf32, 1101659328512 : i64>) -> tensor<1x88x1x1xf32> loc(#loc159)
        %120 = "tpu.Conv2D"(%115, %117, %116) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 5120, buffer_addr = 12288, buffer_size = 10846, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [80], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 80 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x80x16x16xbf16>, tensor<1x80x3x3xbf16>, tensor<1x80x1x1xbf16>) -> tensor<1x80x16x16xbf16> loc(#loc160)
        %121 = "tpu.Load"(%64) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5120, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x16x16xbf16, 2201170784256 : i64>) -> tensor<1x88x16x16xbf16> loc(#loc161)
        %122 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x80x16x16xbf16>, tensor<1x88x1x80xbf16>, tensor<1x88x1x1xf32>) -> tensor<1x88x16x16xbf16> loc(#loc162)
        %123 = "tpu.Add"(%122, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 20480, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x88x16x16xbf16>, tensor<1x88x16x16xbf16>) -> tensor<1x88x16x16xbf16> loc(#loc153)
        %124 = "tpu.Store"(%123, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 9, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x88x16x16xbf16>, none) -> tensor<1x88x16x16xbf16, 2201170739200 : i64> loc(#loc153)
        %125 = "tpu.Pool2D"(%123) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 8192, out_size = 1408, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [2, 2], pad_value = 0 : i64, pads = [0, 0, 0, 0], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [2, 2]} : (tensor<1x88x16x16xbf16>) -> tensor<1x88x8x8xbf16> loc(#loc154)
        %126 = "tpu.Store"(%125, %0) {ginfo = #tpu.lg<out_addr = 8192, out_size = 1408, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x88x8x8xbf16>, none) -> tensor<1x88x8x8xbf16, 2201170870272 : i64> loc(#loc154)
        "tpu.Yield"(%124, %126) : (tensor<1x88x16x16xbf16, 2201170739200 : i64>, tensor<1x88x8x8xbf16, 2201170870272 : i64>) -> () loc(#loc249)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, 11, -2, 7, 6, 0, -3, 8, 1, 2, -4, 10, 9], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x80x16x16xbf16, 2201170829312 : i64>, tensor<1x88x16x16xbf16, 2201170784256 : i64>) -> (tensor<1x88x16x16xbf16, 2201170739200 : i64>, tensor<1x88x8x8xbf16, 2201170870272 : i64>) loc(#loc249)
      %70 = "tpu.Pad"(%69#1, %0, %0, %0, %0) {mode = #tpu<Pad_Mode constant>, paddings = [0, 0, 0, 0, 0, 8, 0, 0], val = 0.000000e+00 : f64, with_insert_zero = false} : (tensor<1x88x8x8xbf16, 2201170870272 : i64>, none, none, none, none) -> tensor<1x96x8x8xbf16, 2201170784256 : i64> loc(#loc163)
      %71 = "top.Weight"() : () -> tensor<1x88x3x3xbf16, 1101659348992 : i64> loc(#loc164)
      %72 = "top.Weight"() : () -> tensor<1x88x1x1xbf16, 1101659353088 : i64> loc(#loc165)
      %73 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659357184 : i64> loc(#loc166)
      %74 = "top.Weight"() : () -> tensor<1x96x1x88xbf16, 1101659361280 : i64> loc(#loc167)
      %75 = "top.Weight"() : () -> tensor<1x96x3x3xbf16, 1101659381760 : i64> loc(#loc168)
      %76 = "top.Weight"() : () -> tensor<1x96x1x1xbf16, 1101659385856 : i64> loc(#loc169)
      %77 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659389952 : i64> loc(#loc170)
      %78 = "top.Weight"() : () -> tensor<1x96x1x96xbf16, 1101659394048 : i64> loc(#loc171)
      %79 = "top.Weight"() : () -> tensor<1x96x3x3xbf16, 1101659414528 : i64> loc(#loc172)
      %80 = "top.Weight"() : () -> tensor<1x96x1x1xbf16, 1101659418624 : i64> loc(#loc173)
      %81 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659422720 : i64> loc(#loc174)
      %82 = "top.Weight"() : () -> tensor<1x96x1x96xbf16, 1101659426816 : i64> loc(#loc175)
      %83 = "top.Weight"() : () -> tensor<1x96x3x3xbf16, 1101659447296 : i64> loc(#loc176)
      %84 = "top.Weight"() : () -> tensor<1x96x1x1xbf16, 1101659451392 : i64> loc(#loc177)
      %85 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659455488 : i64> loc(#loc178)
      %86 = "top.Weight"() : () -> tensor<1x96x1x96xbf16, 1101659459584 : i64> loc(#loc179)
      %87 = "top.Weight"() : () -> tensor<1x96x3x3xbf16, 1101659480064 : i64> loc(#loc180)
      %88 = "top.Weight"() : () -> tensor<1x96x1x1xbf16, 1101659484160 : i64> loc(#loc181)
      %89 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659488256 : i64> loc(#loc182)
      %90 = "top.Weight"() : () -> tensor<1x96x1x96xbf16, 1101659492352 : i64> loc(#loc183)
      %91 = "top.Weight"() : () -> tensor<1x2x1x1xf32, 1101659512832 : i64> loc(#loc184)
      %92 = "top.Weight"() : () -> tensor<1x2x1x88xbf16, 1101659516928 : i64> loc(#loc185)
      %93:2 = "tpu.Group"(%69#0, %70) ({
        %115 = "tpu.Load"(%72) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 22, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 0, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x1x1xbf16, 1101659353088 : i64>) -> tensor<1x88x1x1xbf16> loc(#loc188)
        %116 = "tpu.Load"(%71) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 198, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 1, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x3x3xbf16, 1101659348992 : i64>) -> tensor<1x88x3x3xbf16> loc(#loc189)
        %117 = "tpu.Load"(%69#0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 2, stage = 0, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x88x16x16xbf16, 2201170739200 : i64>) -> tensor<1x88x16x16xbf16> loc(#loc190)
        %118 = "tpu.Load"(%74) {do_bcast = false, ginfo = #tpu.lg<out_addr = 8192, out_size = 2112, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [88], id = 3, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x88xbf16, 1101659361280 : i64>) -> tensor<1x96x1x88xbf16> loc(#loc191)
        %119 = "tpu.Load"(%73) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20992, out_size = 48, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xf32, 1101659357184 : i64>) -> tensor<1x96x1x1xf32> loc(#loc192)
        %120 = "tpu.Conv2D"(%117, %116, %115) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1408, buffer_addr = 16384, buffer_size = 3431, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [88], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 88 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [0, 0, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x88x16x16xbf16>, tensor<1x88x3x3xbf16>, tensor<1x88x1x1xbf16>) -> tensor<1x88x8x8xbf16> loc(#loc193)
        %121 = "tpu.Load"(%70) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 6, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x8x8xbf16, 2201170784256 : i64>) -> tensor<1x96x8x8xbf16> loc(#loc194)
        %122 = "tpu.Load"(%76) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28720, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 7, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xbf16, 1101659385856 : i64>) -> tensor<1x96x1x1xbf16> loc(#loc195)
        %123 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 48, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 8, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xf32, 1101659389952 : i64>) -> tensor<1x96x1x1xf32> loc(#loc196)
        %124 = "tpu.Conv2D"(%120, %118, %119) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 5632, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x88x8x8xbf16>, tensor<1x96x1x88xbf16>, tensor<1x96x1x1xf32>) -> tensor<1x96x8x8xbf16> loc(#loc197)
        %125 = "tpu.Load"(%75) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20992, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 10, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x3x3xbf16, 1101659381760 : i64>) -> tensor<1x96x3x3xbf16> loc(#loc198)
        %126 = "tpu.Add"(%124, %121) {do_relu = true, ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 11, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x96x8x8xbf16>, tensor<1x96x8x8xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc199)
        %127 = "tpu.Load"(%78) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 12, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x96xbf16, 1101659394048 : i64>) -> tensor<1x96x1x96xbf16> loc(#loc200)
        %128 = "tpu.Conv2D"(%126, %125, %122) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 1536, buffer_addr = 5632, buffer_size = 3312, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 96 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x3x3xbf16>, tensor<1x96x1x1xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc201)
        %129 = "tpu.Load"(%80) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 14, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xbf16, 1101659418624 : i64>) -> tensor<1x96x1x1xbf16> loc(#loc202)
        %130 = "tpu.Load"(%79) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7936, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 15, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x3x3xbf16, 1101659414528 : i64>) -> tensor<1x96x3x3xbf16> loc(#loc203)
        %131 = "tpu.Load"(%82) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5632, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 16, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x96xbf16, 1101659426816 : i64>) -> tensor<1x96x1x96xbf16> loc(#loc204)
        %132 = "tpu.Conv2D"(%128, %127, %123) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9728, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 17, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x1x96xbf16>, tensor<1x96x1x1xf32>) -> tensor<1x96x8x8xbf16> loc(#loc205)
        %133 = "tpu.Add"(%132, %126) {do_relu = true, ginfo = #tpu.lg<out_addr = 8192, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 18, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x96x8x8xbf16>, tensor<1x96x8x8xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc206)
        %134 = "tpu.Load"(%81) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 48, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 19, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xf32, 1101659422720 : i64>) -> tensor<1x96x1x1xf32> loc(#loc207)
        %135 = "tpu.Conv2D"(%133, %130, %129) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 17920, out_size = 1536, buffer_addr = 12288, buffer_size = 3312, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 20, stage = 1, slice_idx = 0, group_type = 0>, group = 96 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x3x3xbf16>, tensor<1x96x1x1xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc208)
        %136 = "tpu.Load"(%84) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 21, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xbf16, 1101659451392 : i64>) -> tensor<1x96x1x1xbf16> loc(#loc209)
        %137 = "tpu.Load"(%83) {do_bcast = false, ginfo = #tpu.lg<out_addr = 14592, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 22, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x3x3xbf16, 1101659447296 : i64>) -> tensor<1x96x3x3xbf16> loc(#loc210)
        %138 = "tpu.Load"(%86) {do_bcast = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 23, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x96xbf16, 1101659459584 : i64>) -> tensor<1x96x1x96xbf16> loc(#loc211)
        %139 = "tpu.Conv2D"(%135, %131, %134) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9728, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 24, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x1x96xbf16>, tensor<1x96x1x1xf32>) -> tensor<1x96x8x8xbf16> loc(#loc212)
        %140 = "tpu.Add"(%139, %133) {do_relu = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 25, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x96x8x8xbf16>, tensor<1x96x8x8xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc213)
        %141 = "tpu.Load"(%85) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24576, out_size = 48, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 26, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xf32, 1101659455488 : i64>) -> tensor<1x96x1x1xf32> loc(#loc214)
        %142 = "tpu.Conv2D"(%140, %137, %136) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 1536, buffer_addr = 5632, buffer_size = 3312, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 27, stage = 1, slice_idx = 0, group_type = 0>, group = 96 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x3x3xbf16>, tensor<1x96x1x1xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc215)
        %143 = "tpu.Load"(%88) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28672, out_size = 24, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 28, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xbf16, 1101659484160 : i64>) -> tensor<1x96x1x1xbf16> loc(#loc216)
        %144 = "tpu.Load"(%87) {do_bcast = false, ginfo = #tpu.lg<out_addr = 7936, out_size = 216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [3], w_idx = [0], w_slice = [3], id = 29, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x3x3xbf16, 1101659480064 : i64>) -> tensor<1x96x3x3xbf16> loc(#loc217)
        %145 = "tpu.Load"(%90) {do_bcast = false, ginfo = #tpu.lg<out_addr = 5632, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [96], id = 30, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x96xbf16, 1101659492352 : i64>) -> tensor<1x96x1x96xbf16> loc(#loc218)
        %146 = "tpu.Conv2D"(%142, %138, %141) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 9728, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 31, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x1x96xbf16>, tensor<1x96x1x1xf32>) -> tensor<1x96x8x8xbf16> loc(#loc219)
        %147 = "tpu.Add"(%146, %140) {do_relu = true, ginfo = #tpu.lg<out_addr = 8192, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 32, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x96x8x8xbf16>, tensor<1x96x8x8xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc220)
        %148 = "tpu.Load"(%89) {do_bcast = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 48, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 33, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x96x1x1xf32, 1101659488256 : i64>) -> tensor<1x96x1x1xf32> loc(#loc221)
        %149 = "tpu.Conv2D"(%147, %144, %143) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 16384, buffer_size = 3312, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 34, stage = 1, slice_idx = 0, group_type = 0>, group = 96 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x3x3xbf16>, tensor<1x96x1x1xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc222)
        %150 = "tpu.Conv2D"(%149, %145, %148) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 35, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16>, tensor<1x96x1x96xbf16>, tensor<1x96x1x1xf32>) -> tensor<1x96x8x8xbf16> loc(#loc223)
        %151 = "tpu.Load"(%92) {do_bcast = false, ginfo = #tpu.lg<out_addr = 24784, out_size = 176, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [2], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [88], id = 36, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x2x1x88xbf16, 1101659516928 : i64>) -> tensor<1x2x1x88xbf16> loc(#loc224)
        %152 = "tpu.Load"(%91) {do_bcast = false, ginfo = #tpu.lg<out_addr = 28704, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [2], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 37, stage = 1, slice_idx = 0, group_type = 0>, is_idx_weight = false, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x2x1x1xf32, 1101659512832 : i64>) -> tensor<1x2x1x1xf32> loc(#loc225)
        %153 = "tpu.Add"(%150, %147) {do_relu = true, ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 38, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x96x8x8xbf16>, tensor<1x96x8x8xbf16>) -> tensor<1x96x8x8xbf16> loc(#loc186)
        %154 = "tpu.Store"(%153, %0) {ginfo = #tpu.lg<out_addr = 12288, out_size = 1536, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [96], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [8], w_idx = [0], w_slice = [8], id = 39, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x96x8x8xbf16>, none) -> tensor<1x96x8x8xbf16, 2201170800640 : i64> loc(#loc186)
        %155 = "tpu.Conv2D"(%117, %151, %152) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 20480, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [2], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 40, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x88x16x16xbf16>, tensor<1x2x1x88xbf16>, tensor<1x2x1x1xf32>) -> tensor<1x2x16x16xbf16> loc(#loc187)
        %156 = "tpu.Store"(%155, %0) {ginfo = #tpu.lg<out_addr = 20480, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [2], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [16], w_idx = [0], w_slice = [16], id = 41, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x2x16x16xbf16>, none) -> tensor<1x2x16x16xbf16, 2201170796544 : i64> loc(#loc187)
        "tpu.Yield"(%154, %156) : (tensor<1x96x8x8xbf16, 2201170800640 : i64>, tensor<1x2x16x16xbf16, 2201170796544 : i64>) -> () loc(#loc250)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 5, 3, 4, -2, 9, 6, 7, 8, -3, 11, 10, -4, 13, 12, -5, 17, 18, 14, 15, 16, 41, -6, 20, 19, -7, 24, 25, 21, 22, 23, -8, 27, 26, -9, 31, 32, 28, 29, 30, -10, 34, 33, -11, 35, 0, 1, -12, 38, 36, 37, -13, 40, 39, -14, 2], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x88x16x16xbf16, 2201170739200 : i64>, tensor<1x96x8x8xbf16, 2201170784256 : i64>) -> (tensor<1x96x8x8xbf16, 2201170800640 : i64>, tensor<1x2x16x16xbf16, 2201170796544 : i64>) loc(#loc250)
      %94 = "tpu.Permute"(%93#1, %0) {order = [0, 2, 3, 1]} : (tensor<1x2x16x16xbf16, 2201170796544 : i64>, none) -> tensor<1x16x16x2xbf16, 2201170792448 : i64> loc(#loc226)
      %95 = "tpu.Reshape"(%94) {flatten_start_dim = -1 : i64, shape = [1, -1, 1]} : (tensor<1x16x16x2xbf16, 2201170792448 : i64>) -> tensor<1x512x1xbf16, 2201170792448 : i64> loc(#loc227)
      %96 = "top.Weight"() : () -> tensor<1x6x1x1xf32, 1101659521024 : i64> loc(#loc228)
      %97 = "top.Weight"() : () -> tensor<1x6x1x96xbf16, 1101659525120 : i64> loc(#loc229)
      %98 = "tpu.Conv2D"(%93#0, %97, %96) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16, 2201170800640 : i64>, tensor<1x6x1x96xbf16, 1101659525120 : i64>, tensor<1x6x1x1xf32, 1101659521024 : i64>) -> tensor<1x6x8x8xbf16, 2201170788352 : i64> loc(#loc230)
      %99 = "tpu.Permute"(%98, %0) {order = [0, 2, 3, 1]} : (tensor<1x6x8x8xbf16, 2201170788352 : i64>, none) -> tensor<1x8x8x6xbf16, 2201170784256 : i64> loc(#loc231)
      %100 = "tpu.Reshape"(%99) {flatten_start_dim = -1 : i64, shape = [1, -1, 1]} : (tensor<1x8x8x6xbf16, 2201170784256 : i64>) -> tensor<1x384x1xbf16, 2201170784256 : i64> loc(#loc232)
      %101 = "tpu.Concat"(%95, %100) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x512x1xbf16, 2201170792448 : i64>, tensor<1x384x1xbf16, 2201170784256 : i64>) -> tensor<1x896x1xbf16, 2201170812928 : i64> loc(#loc233)
      %102 = "tpu.Cast"(%101) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x896x1xbf16, 2201170812928 : i64>) -> tensor<1x896x1xf32, 2201170825216 : i64> loc(#loc234)
      %103 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 1101659529216 : i64> loc(#loc235)
      %104 = "top.Weight"() : () -> tensor<1x32x1x88xbf16, 1101659533312 : i64> loc(#loc236)
      %105 = "tpu.Conv2D"(%69#0, %104, %103) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x88x16x16xbf16, 2201170739200 : i64>, tensor<1x32x1x88xbf16, 1101659533312 : i64>, tensor<1x32x1x1xf32, 1101659529216 : i64>) -> tensor<1x32x16x16xbf16, 2201170784256 : i64> loc(#loc237)
      %106 = "tpu.Permute"(%105, %0) {order = [0, 2, 3, 1]} : (tensor<1x32x16x16xbf16, 2201170784256 : i64>, none) -> tensor<1x16x16x32xbf16, 2201170739200 : i64> loc(#loc238)
      %107 = "tpu.Reshape"(%106) {flatten_start_dim = -1 : i64, shape = [1, -1, 16]} : (tensor<1x16x16x32xbf16, 2201170739200 : i64>) -> tensor<1x512x16xbf16, 2201170739200 : i64> loc(#loc239)
      %108 = "top.Weight"() : () -> tensor<1x96x1x1xf32, 1101659541504 : i64> loc(#loc240)
      %109 = "top.Weight"() : () -> tensor<1x96x1x96xbf16, 1101659545600 : i64> loc(#loc241)
      %110 = "tpu.Conv2D"(%93#0, %109, %108) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, multicore = true, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x96x8x8xbf16, 2201170800640 : i64>, tensor<1x96x1x96xbf16, 1101659545600 : i64>, tensor<1x96x1x1xf32, 1101659541504 : i64>) -> tensor<1x96x8x8xbf16, 2201170767872 : i64> loc(#loc242)
      %111 = "tpu.Permute"(%110, %0) {order = [0, 2, 3, 1]} : (tensor<1x96x8x8xbf16, 2201170767872 : i64>, none) -> tensor<1x8x8x96xbf16, 2201170755584 : i64> loc(#loc243)
      %112 = "tpu.Reshape"(%111) {flatten_start_dim = -1 : i64, shape = [1, -1, 16]} : (tensor<1x8x8x96xbf16, 2201170755584 : i64>) -> tensor<1x384x16xbf16, 2201170755584 : i64> loc(#loc244)
      %113 = "tpu.Concat"(%107, %112) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x512x16xbf16, 2201170739200 : i64>, tensor<1x384x16xbf16, 2201170755584 : i64>) -> tensor<1x896x16xbf16, 2201170796544 : i64> loc(#loc245)
      %114 = "tpu.Cast"(%113) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x896x16xbf16, 2201170796544 : i64>) -> tensor<1x896x16xf32, 2201170739200 : i64> loc(#loc246)
      return %114, %102 : tensor<1x896x16xf32, 2201170739200 : i64>, tensor<1x896x1xf32, 2201170825216 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("blaze_face.backbone1.0.bias")
#loc3 = loc("80_Relu_filter_reorderd")
#loc4 = loc("80_Relu")
#loc5 = loc("load_0")
#loc6 = loc("load_80_Relu_filter_reorderd")
#loc7 = loc("load_blaze_face.backbone1.0.bias")
#loc8 = loc("x.180_Relu_bf16")
#loc9 = loc("81_Convblaze_face.backbone1.2.convs.0.weight_bf16")
#loc10 = loc("81_Conv_reordered")
#loc11 = loc("81_Conv")
#loc12 = loc("blaze_face.backbone1.2.convs.1.bias")
#loc13 = loc("82_Conv_filter_reorderd")
#loc14 = loc("84_Relu")
#loc15 = loc("load_81_Conv")
#loc16 = loc("load_82_Conv_filter_reorderd")
#loc17 = loc("load_blaze_face.backbone1.2.convs.1.bias")
#loc18 = loc("load_80_Relu")
#loc19 = loc("82_Conv")
#loc20 = loc("85_Pad")
#loc21 = loc("86_Convblaze_face.backbone1.3.convs.0.weight_bf16")
#loc22 = loc("86_Conv_reordered")
#loc23 = loc("blaze_face.backbone1.3.convs.1.bias")
#loc24 = loc("87_Conv_filter_reorderd")
#loc25 = loc("89_Relu")
#loc26 = loc("91_MaxPool")
#loc27 = loc("load_84_Relu")
#loc28 = loc("load_86_Convblaze_face.backbone1.3.convs.0.weight_bf16")
#loc29 = loc("load_86_Conv_reordered")
#loc30 = loc("load_87_Conv_filter_reorderd")
#loc31 = loc("load_blaze_face.backbone1.3.convs.1.bias")
#loc32 = loc("86_Conv")
#loc33 = loc("load_85_Pad")
#loc34 = loc("87_Conv")
#loc35 = loc("92_Pad")
#loc36 = loc("93_Convblaze_face.backbone1.4.convs.0.weight_bf16")
#loc37 = loc("93_Conv_reordered")
#loc38 = loc("blaze_face.backbone1.4.convs.1.bias")
#loc39 = loc("94_Conv_filter_reorderd")
#loc40 = loc("96_Relu")
#loc41 = loc("load_89_Relu")
#loc42 = loc("load_93_Convblaze_face.backbone1.4.convs.0.weight_bf16")
#loc43 = loc("load_93_Conv_reordered")
#loc44 = loc("load_94_Conv_filter_reorderd")
#loc45 = loc("load_blaze_face.backbone1.4.convs.1.bias")
#loc46 = loc("93_Conv")
#loc47 = loc("load_92_Pad")
#loc48 = loc("94_Conv")
#loc49 = loc("97_Pad")
#loc50 = loc("98_Convblaze_face.backbone1.5.convs.0.weight_bf16")
#loc51 = loc("98_Conv_reordered")
#loc52 = loc("blaze_face.backbone1.5.convs.1.bias")
#loc53 = loc("99_Conv_filter_reorderd")
#loc54 = loc("101_Relu")
#loc55 = loc("load_96_Relu")
#loc56 = loc("load_98_Convblaze_face.backbone1.5.convs.0.weight_bf16")
#loc57 = loc("load_98_Conv_reordered")
#loc58 = loc("load_99_Conv_filter_reorderd")
#loc59 = loc("load_blaze_face.backbone1.5.convs.1.bias")
#loc60 = loc("98_Conv")
#loc61 = loc("load_97_Pad")
#loc62 = loc("99_Conv")
#loc63 = loc("102_Pad")
#loc64 = loc("103_Convblaze_face.backbone1.6.convs.0.weight_bf16")
#loc65 = loc("103_Conv_reordered")
#loc66 = loc("blaze_face.backbone1.6.convs.1.bias")
#loc67 = loc("104_Conv_filter_reorderd")
#loc68 = loc("106_Relu")
#loc69 = loc("108_MaxPool")
#loc70 = loc("load_103_Conv_reordered")
#loc71 = loc("load_101_Relu")
#loc72 = loc("load_103_Convblaze_face.backbone1.6.convs.0.weight_bf16")
#loc73 = loc("load_104_Conv_filter_reorderd")
#loc74 = loc("load_blaze_face.backbone1.6.convs.1.bias")
#loc75 = loc("103_Conv")
#loc76 = loc("load_102_Pad")
#loc77 = loc("104_Conv")
#loc78 = loc("109_Pad")
#loc79 = loc("110_Convblaze_face.backbone1.7.convs.0.weight_bf16")
#loc80 = loc("110_Conv_reordered")
#loc81 = loc("blaze_face.backbone1.7.convs.1.bias")
#loc82 = loc("111_Conv_filter_reorderd")
#loc83 = loc("113_Relu")
#loc84 = loc("load_110_Conv_reordered")
#loc85 = loc("load_106_Relu")
#loc86 = loc("load_110_Convblaze_face.backbone1.7.convs.0.weight_bf16")
#loc87 = loc("load_111_Conv_filter_reorderd")
#loc88 = loc("load_blaze_face.backbone1.7.convs.1.bias")
#loc89 = loc("110_Conv")
#loc90 = loc("load_109_Pad")
#loc91 = loc("111_Conv")
#loc92 = loc("114_Pad")
#loc93 = loc("115_Convblaze_face.backbone1.8.convs.0.weight_bf16")
#loc94 = loc("115_Conv_reordered")
#loc95 = loc("blaze_face.backbone1.8.convs.1.bias")
#loc96 = loc("116_Conv_filter_reorderd")
#loc97 = loc("118_Relu")
#loc98 = loc("load_115_Conv_reordered")
#loc99 = loc("load_113_Relu")
#loc100 = loc("load_115_Convblaze_face.backbone1.8.convs.0.weight_bf16")
#loc101 = loc("load_116_Conv_filter_reorderd")
#loc102 = loc("load_blaze_face.backbone1.8.convs.1.bias")
#loc103 = loc("115_Conv")
#loc104 = loc("load_114_Pad")
#loc105 = loc("116_Conv")
#loc106 = loc("119_Pad")
#loc107 = loc("120_Convblaze_face.backbone1.9.convs.0.weight_bf16")
#loc108 = loc("120_Conv_reordered")
#loc109 = loc("blaze_face.backbone1.9.convs.1.bias")
#loc110 = loc("121_Conv_filter_reorderd")
#loc111 = loc("123_Relu")
#loc112 = loc("load_120_Conv_reordered")
#loc113 = loc("load_120_Convblaze_face.backbone1.9.convs.0.weight_bf16")
#loc114 = loc("load_118_Relu")
#loc115 = loc("load_121_Conv_filter_reorderd")
#loc116 = loc("load_blaze_face.backbone1.9.convs.1.bias")
#loc117 = loc("120_Conv")
#loc118 = loc("load_119_Pad")
#loc119 = loc("121_Conv")
#loc120 = loc("124_Pad")
#loc121 = loc("125_Convblaze_face.backbone1.10.convs.0.weight_bf16")
#loc122 = loc("125_Conv_reordered")
#loc123 = loc("blaze_face.backbone1.10.convs.1.bias")
#loc124 = loc("126_Conv_filter_reorderd")
#loc125 = loc("128_Relu")
#loc126 = loc("load_123_Relu")
#loc127 = loc("load_125_Convblaze_face.backbone1.10.convs.0.weight_bf16")
#loc128 = loc("load_125_Conv_reordered")
#loc129 = loc("load_126_Conv_filter_reorderd")
#loc130 = loc("load_blaze_face.backbone1.10.convs.1.bias")
#loc131 = loc("125_Conv")
#loc132 = loc("load_124_Pad")
#loc133 = loc("126_Conv")
#loc134 = loc("129_Pad")
#loc135 = loc("130_Convblaze_face.backbone1.11.convs.0.weight_bf16")
#loc136 = loc("130_Conv_reordered")
#loc137 = loc("blaze_face.backbone1.11.convs.1.bias")
#loc138 = loc("131_Conv_filter_reorderd")
#loc139 = loc("133_Relu")
#loc140 = loc("load_128_Relu")
#loc141 = loc("load_130_Convblaze_face.backbone1.11.convs.0.weight_bf16")
#loc142 = loc("load_130_Conv_reordered")
#loc143 = loc("load_131_Conv_filter_reorderd")
#loc144 = loc("load_blaze_face.backbone1.11.convs.1.bias")
#loc145 = loc("130_Conv")
#loc146 = loc("load_129_Pad")
#loc147 = loc("131_Conv")
#loc148 = loc("134_Pad")
#loc149 = loc("135_Convblaze_face.backbone1.12.convs.0.weight_bf16")
#loc150 = loc("135_Conv_reordered")
#loc151 = loc("blaze_face.backbone1.12.convs.1.bias")
#loc152 = loc("136_Conv_filter_reorderd")
#loc153 = loc("138_Relu")
#loc154 = loc("140_MaxPool")
#loc155 = loc("load_133_Relu")
#loc156 = loc("load_135_Conv_reordered")
#loc157 = loc("load_135_Convblaze_face.backbone1.12.convs.0.weight_bf16")
#loc158 = loc("load_136_Conv_filter_reorderd")
#loc159 = loc("load_blaze_face.backbone1.12.convs.1.bias")
#loc160 = loc("135_Conv")
#loc161 = loc("load_134_Pad")
#loc162 = loc("136_Conv")
#loc163 = loc("141_Pad")
#loc164 = loc("142_Convblaze_face.backbone2.0.convs.0.weight_bf16")
#loc165 = loc("142_Conv_reordered")
#loc166 = loc("blaze_face.backbone2.0.convs.1.bias")
#loc167 = loc("143_Conv_filter_reorderd")
#loc168 = loc("146_Convblaze_face.backbone2.1.convs.0.weight_bf16")
#loc169 = loc("146_Conv_reordered")
#loc170 = loc("blaze_face.backbone2.1.convs.1.bias")
#loc171 = loc("147_Conv_filter_reorderd")
#loc172 = loc("150_Convblaze_face.backbone2.2.convs.0.weight_bf16")
#loc173 = loc("150_Conv_reordered")
#loc174 = loc("blaze_face.backbone2.2.convs.1.bias")
#loc175 = loc("151_Conv_filter_reorderd")
#loc176 = loc("154_Convblaze_face.backbone2.3.convs.0.weight_bf16")
#loc177 = loc("154_Conv_reordered")
#loc178 = loc("blaze_face.backbone2.3.convs.1.bias")
#loc179 = loc("155_Conv_filter_reorderd")
#loc180 = loc("158_Convblaze_face.backbone2.4.convs.0.weight_bf16")
#loc181 = loc("158_Conv_reordered")
#loc182 = loc("blaze_face.backbone2.4.convs.1.bias")
#loc183 = loc("159_Conv_filter_reorderd")
#loc184 = loc("blaze_face.classifier_8.bias")
#loc185 = loc("162_Conv_filter_reorderd")
#loc186 = loc("161_Relu")
#loc187 = loc("162_Conv")
#loc188 = loc("load_142_Conv_reordered")
#loc189 = loc("load_142_Convblaze_face.backbone2.0.convs.0.weight_bf16")
#loc190 = loc("load_138_Relu")
#loc191 = loc("load_143_Conv_filter_reorderd")
#loc192 = loc("load_blaze_face.backbone2.0.convs.1.bias")
#loc193 = loc("142_Conv")
#loc194 = loc("load_141_Pad")
#loc195 = loc("load_146_Conv_reordered")
#loc196 = loc("load_blaze_face.backbone2.1.convs.1.bias")
#loc197 = loc("143_Conv")
#loc198 = loc("load_146_Convblaze_face.backbone2.1.convs.0.weight_bf16")
#loc199 = loc("145_Relu")
#loc200 = loc("load_147_Conv_filter_reorderd")
#loc201 = loc("146_Conv")
#loc202 = loc("load_150_Conv_reordered")
#loc203 = loc("load_150_Convblaze_face.backbone2.2.convs.0.weight_bf16")
#loc204 = loc("load_151_Conv_filter_reorderd")
#loc205 = loc("147_Conv")
#loc206 = loc("149_Relu")
#loc207 = loc("load_blaze_face.backbone2.2.convs.1.bias")
#loc208 = loc("150_Conv")
#loc209 = loc("load_154_Conv_reordered")
#loc210 = loc("load_154_Convblaze_face.backbone2.3.convs.0.weight_bf16")
#loc211 = loc("load_155_Conv_filter_reorderd")
#loc212 = loc("151_Conv")
#loc213 = loc("153_Relu")
#loc214 = loc("load_blaze_face.backbone2.3.convs.1.bias")
#loc215 = loc("154_Conv")
#loc216 = loc("load_158_Conv_reordered")
#loc217 = loc("load_158_Convblaze_face.backbone2.4.convs.0.weight_bf16")
#loc218 = loc("load_159_Conv_filter_reorderd")
#loc219 = loc("155_Conv")
#loc220 = loc("157_Relu")
#loc221 = loc("load_blaze_face.backbone2.4.convs.1.bias")
#loc222 = loc("158_Conv")
#loc223 = loc("159_Conv")
#loc224 = loc("load_162_Conv_filter_reorderd")
#loc225 = loc("load_blaze_face.classifier_8.bias")
#loc226 = loc("163_Transpose")
#loc227 = loc("170_Reshape")
#loc228 = loc("blaze_face.classifier_16.bias")
#loc229 = loc("171_Conv_filter_reorderd")
#loc230 = loc("171_Conv")
#loc231 = loc("172_Transpose")
#loc232 = loc("179_Reshape")
#loc233 = loc("180_Concat")
#loc234 = loc("180_Concat_f32")
#loc235 = loc("blaze_face.regressor_8.bias")
#loc236 = loc("181_Conv_filter_reorderd")
#loc237 = loc("181_Conv")
#loc238 = loc("182_Transpose")
#loc239 = loc("189_Reshape")
#loc240 = loc("blaze_face.regressor_16.bias")
#loc241 = loc("190_Conv_filter_reorderd")
#loc242 = loc("190_Conv")
#loc243 = loc("191_Transpose")
#loc244 = loc("198_Reshape")
#loc245 = loc("199_Concat")
#loc246 = loc("199_Concat_f32")
#loc247 = loc(fused[#loc25, #loc26])
#loc248 = loc(fused[#loc68, #loc69])
#loc249 = loc(fused[#loc153, #loc154])
#loc250 = loc(fused[#loc186, #loc187])

