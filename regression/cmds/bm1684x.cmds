[%R0, %D1 = "dma.tensor"(%G42172416, %B0) {decompress = False} : (memref<1x3x110x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x110x322xf32, strides: [35424, 35424, 322, 1]>, none), %R15.2688, %D2 = "dma.tensor"(%G0, %B0) {decompress = False} : (memref<1x32x3x36xf32, strides: [3456, 108, 36, 1]>, none) -> (memref<1x32x3x36xf32, strides: [108, 108, 36, 1]>, none), %R9.10176, %D3 = "dma.tensor"(%G16384, %B0) {decompress = False} : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R10, %B1 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D3) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [2, 0, 2, 0], res_add = False} : (memref<1x3x110x322xf32, strides: [35424, 35424, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R4.3584, %B2 = "tsbc.s_bc"(%S1024, %D3) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1792, %B3 = "arith.sub"(%C0.0, %R10, %D3) {round_mode = 0} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B4 = "arith.max"(%R2.1792, %C-3.4028198694267105e+35, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B5 = "arith.mul"(%R2.1792, %C1.4426950216293335, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B6 = "arith.cast"(%R0, %D3) {round_mode = 3} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B7 = "arith.mul"(%R13, %C0.6931471824645996, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B8 = "arith.sub"(%R2.1792, %R0, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B9 = "arith.cast"(%R13, %D3) {round_mode = 1} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R13, %B10 = "arith.min"(%R2.1792, %C127, %D3) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B11 = "arith.max"(%R13, %C-127, %D3) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R13, %B12 = "arith.adds"(%R2.1792, %C127, %C23, %D3) {round_mode = 1} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, ui8, none) -> (memref<1x32x54x160xsi32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B13 = "sfu.taylor_4x"(%R0, %R4.3584, %D3) : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B14 = "arith.mul"(%R2.1792, %R13, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B15 = "arith.add"(%R13, %C1.0, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B16 = "arith.div"(%C1.0, %R0, %D3) {iter = 3} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B17 = "arith.mul"(%R13, %R10, %D3) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R9.8960, %D4 = "dma.tensor"(%G20480, %B1) {decompress = False} : (memref<1x64x32x9xf32, strides: [18432, 288, 9, 1]>, none) -> (memref<1x64x32x9xf32, strides: [288, 288, 9, 1]>, none), %R9.10112, %D5 = "dma.tensor"(%G94208, %B1) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R10, %B18 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D5) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], res_add = False} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D6 = "dma.tensor"(%G42173680, %B17) {decompress = False} : (memref<1x3x110x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x110x324xf32, strides: [35648, 35648, 324, 1]>, none), %R12.896, %B19 = "tsbc.s_bc"(%S1024, %D6) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B20 = "arith.sub"(%C0.0, %R10, %D6) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B21 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B22 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B23 = "arith.cast"(%R11, %D6) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B24 = "arith.mul"(%R9.320, %C0.6931471824645996, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B25 = "arith.sub"(%R11.8640, %R11, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B26 = "arith.cast"(%R9.320, %D6) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B27 = "arith.min"(%R11.8640, %C127, %D6) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B28 = "arith.max"(%R9.320, %C-127, %D6) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B29 = "arith.adds"(%R11.8640, %C127, %C23, %D6) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B30 = "sfu.taylor_4x"(%R11, %R12.896, %D6) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B31 = "arith.mul"(%R11.8640, %R9.320, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B32 = "arith.add"(%R9.320, %C1.0, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B33 = "arith.div"(%C1.0, %R11, %D6) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B34 = "arith.mul"(%R9.320, %R10, %D6) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R15.3136, %D7 = "dma.tensor"(%G98304, %B18) {decompress = False} : (memref<1x32x64x1xf32, strides: [2048, 64, 1, 1]>, none) -> (memref<1x32x64x1xf32, strides: [64, 64, 1, 1]>, none), %R9.10240, %D8 = "dma.tensor"(%G106496, %B18) {decompress = False} : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R10, %B35 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D8) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29114368, %D9 = "dma.tensor"(%R9.320, %B34) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B36 = "tsbc.s_bc"(%S1024, %D9) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B37 = "arith.sub"(%C0.0, %R10, %D9) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B38 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B39 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B40 = "arith.cast"(%R11, %D9) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B41 = "arith.mul"(%R9.320, %C0.6931471824645996, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B42 = "arith.sub"(%R11.8640, %R11, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B43 = "arith.cast"(%R9.320, %D9) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B44 = "arith.min"(%R11.8640, %C127, %D9) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B45 = "arith.max"(%R9.320, %C-127, %D9) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B46 = "arith.adds"(%R11.8640, %C127, %C23, %D9) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B47 = "sfu.taylor_4x"(%R11, %R12.896, %D9) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B48 = "arith.mul"(%R11.8640, %R9.320, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B49 = "arith.add"(%R9.320, %C1.0, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B50 = "arith.div"(%C1.0, %R11, %D9) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B51 = "arith.mul"(%R9.320, %R10, %D9) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R15.3392, %D10 = "dma.tensor"(%G110592, %B35) {decompress = False} : (memref<1x32x32x1xf32, strides: [1024, 32, 1, 1]>, none) -> (memref<1x32x32x1xf32, strides: [32, 32, 1, 1]>, none), %R9.10304, %D11 = "dma.tensor"(%G114688, %B35) {decompress = False} : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R10, %B52 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D11) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B53 = "tsbc.s_bc"(%S1024, %D11) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B54 = "arith.sub"(%C0.0, %R10, %D11) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B55 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B56 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B57 = "arith.cast"(%R11, %D11) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B58 = "arith.mul"(%R13, %C0.6931471824645996, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B59 = "arith.sub"(%R11.8640, %R11, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B60 = "arith.cast"(%R13, %D11) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B61 = "arith.min"(%R11.8640, %C127, %D11) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B62 = "arith.max"(%R13, %C-127, %D11) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B63 = "arith.adds"(%R11.8640, %C127, %C23, %D11) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B64 = "sfu.taylor_4x"(%R11, %R12.896, %D11) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B65 = "arith.mul"(%R11.8640, %R13, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B66 = "arith.add"(%R13, %C1.0, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B67 = "arith.div"(%C1.0, %R11, %D11) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B68 = "arith.mul"(%R13, %R10, %D11) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50364416, %D12 = "dma.tensor"(%R9.320, %B52) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B69 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D12) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [2, 0, 0, 2], res_add = False} : (memref<1x3x110x324xf32, strides: [35648, 35648, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %G47087616, %D13 = "dma.tensor"(%R13, %B68) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.4096, %B70 = "tsbc.s_bc"(%S1024, %D13) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2048, %B71 = "arith.sub"(%C0.0, %R10, %D13) {round_mode = 0} : (f32, memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R2.2048, %B72 = "arith.max"(%R2.2048, %C-3.4028198694267105e+35, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, f32, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R0, %B73 = "arith.mul"(%R2.2048, %C1.4426950216293335, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, f32, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R13, %B74 = "arith.cast"(%R0, %D13) {round_mode = 3} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R0, %B75 = "arith.mul"(%R13, %C0.6931471824645996, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, f32, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R0, %B76 = "arith.sub"(%R2.2048, %R0, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R2.2048, %B77 = "arith.cast"(%R13, %D13) {round_mode = 1} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, none), %R13, %B78 = "arith.min"(%R2.2048, %C127, %D13) {round_mode = 0} : (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, si16, none) -> (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, none), %R2.2048, %B79 = "arith.max"(%R13, %C-127, %D13) {round_mode = 0} : (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, si16, none) -> (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, none), %R13, %B80 = "arith.adds"(%R2.2048, %C127, %C23, %D13) {round_mode = 1} : (memref<1x32x54x161xsi16, strides: [8704, 8704, 161, 1]>, si16, ui8, none) -> (memref<1x32x54x161xsi32, strides: [8704, 8704, 161, 1]>, none), %R2.2048, %B81 = "sfu.taylor_4x"(%R0, %R4.4096, %D13) : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R13, %B82 = "arith.mul"(%R2.2048, %R13, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R0, %B83 = "arith.add"(%R13, %C1.0, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, f32, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R13, %B84 = "arith.div"(%C1.0, %R0, %D13) {iter = 3} : (f32, memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R13, %B85 = "arith.mul"(%R13, %R10, %D13) {round_mode = 0} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none) -> (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, none), %R10, %B86 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D13) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 0, 0, 1], res_add = False} : (memref<1x32x54x161xf32, strides: [8704, 8704, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D14 = "dma.tensor"(%G42438656, %B85) {decompress = False} : (memref<1x3x114x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, none), %R12.896, %B87 = "tsbc.s_bc"(%S1024, %D14) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B88 = "arith.sub"(%C0.0, %R10, %D14) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B89 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B90 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B91 = "arith.cast"(%R11, %D14) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B92 = "arith.mul"(%R9.320, %C0.6931471824645996, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B93 = "arith.sub"(%R11.8640, %R11, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B94 = "arith.cast"(%R9.320, %D14) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B95 = "arith.min"(%R11.8640, %C127, %D14) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B96 = "arith.max"(%R9.320, %C-127, %D14) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B97 = "arith.adds"(%R11.8640, %C127, %C23, %D14) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B98 = "sfu.taylor_4x"(%R11, %R12.896, %D14) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B99 = "arith.mul"(%R11.8640, %R9.320, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B100 = "arith.add"(%R9.320, %C1.0, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B101 = "arith.div"(%C1.0, %R11, %D14) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B102 = "arith.mul"(%R9.320, %R10, %D14) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B103 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D14) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29114688, %D15 = "dma.tensor"(%R9.320, %B102) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B104 = "tsbc.s_bc"(%S1024, %D15) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B105 = "arith.sub"(%C0.0, %R10, %D15) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B106 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B107 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B108 = "arith.cast"(%R11, %D15) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B109 = "arith.mul"(%R9.320, %C0.6931471824645996, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B110 = "arith.sub"(%R11.8640, %R11, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B111 = "arith.cast"(%R9.320, %D15) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B112 = "arith.min"(%R11.8640, %C127, %D15) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B113 = "arith.max"(%R9.320, %C-127, %D15) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B114 = "arith.adds"(%R11.8640, %C127, %C23, %D15) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B115 = "sfu.taylor_4x"(%R11, %R12.896, %D15) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B116 = "arith.mul"(%R11.8640, %R9.320, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B117 = "arith.add"(%R9.320, %C1.0, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B118 = "arith.div"(%C1.0, %R11, %D15) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B119 = "arith.mul"(%R9.320, %R10, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B120 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D15) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B121 = "tsbc.s_bc"(%S1024, %D15) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B122 = "arith.sub"(%C0.0, %R10, %D15) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B123 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B124 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B125 = "arith.cast"(%R11, %D15) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B126 = "arith.mul"(%R13, %C0.6931471824645996, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B127 = "arith.sub"(%R11.8640, %R11, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B128 = "arith.cast"(%R13, %D15) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B129 = "arith.min"(%R11.8640, %C127, %D15) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B130 = "arith.max"(%R13, %C-127, %D15) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B131 = "arith.adds"(%R11.8640, %C127, %C23, %D15) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B132 = "sfu.taylor_4x"(%R11, %R12.896, %D15) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B133 = "arith.mul"(%R11.8640, %R13, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B134 = "arith.add"(%R13, %C1.0, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B135 = "arith.div"(%C1.0, %R11, %D15) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B136 = "arith.mul"(%R13, %R10, %D15) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50364736, %D16 = "dma.tensor"(%R9.320, %B120) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B137 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D16) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], res_add = False} : (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %G47087936, %D17 = "dma.tensor"(%R13, %B136) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.4864, %B138 = "tsbc.s_bc"(%S1024, %D17) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2432, %B139 = "arith.sub"(%C0.0, %R10, %D17) {round_mode = 0} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B140 = "arith.max"(%R2.2432, %C-3.4028198694267105e+35, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B141 = "arith.mul"(%R2.2432, %C1.4426950216293335, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B142 = "arith.cast"(%R0, %D17) {round_mode = 3} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B143 = "arith.mul"(%R13, %C0.6931471824645996, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B144 = "arith.sub"(%R2.2432, %R0, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B145 = "arith.cast"(%R13, %D17) {round_mode = 1} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B146 = "arith.min"(%R2.2432, %C127, %D17) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B147 = "arith.max"(%R13, %C-127, %D17) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B148 = "arith.adds"(%R2.2432, %C127, %C23, %D17) {round_mode = 1} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x32x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B149 = "sfu.taylor_4x"(%R0, %R4.4864, %D17) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B150 = "arith.mul"(%R2.2432, %R13, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B151 = "arith.add"(%R13, %C1.0, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B152 = "arith.div"(%C1.0, %R0, %D17) {iter = 3} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B153 = "arith.mul"(%R13, %R10, %D17) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B154 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D17) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], res_add = False} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D18 = "dma.tensor"(%G42439920, %B153) {decompress = False} : (memref<1x3x114x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, none), %R12.896, %B155 = "tsbc.s_bc"(%S1024, %D18) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B156 = "arith.sub"(%C0.0, %R10, %D18) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B157 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B158 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B159 = "arith.cast"(%R11, %D18) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B160 = "arith.mul"(%R9.320, %C0.6931471824645996, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B161 = "arith.sub"(%R11.8640, %R11, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B162 = "arith.cast"(%R9.320, %D18) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B163 = "arith.min"(%R11.8640, %C127, %D18) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B164 = "arith.max"(%R9.320, %C-127, %D18) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B165 = "arith.adds"(%R11.8640, %C127, %C23, %D18) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B166 = "sfu.taylor_4x"(%R11, %R12.896, %D18) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B167 = "arith.mul"(%R11.8640, %R9.320, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B168 = "arith.add"(%R9.320, %C1.0, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B169 = "arith.div"(%C1.0, %R11, %D18) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B170 = "arith.mul"(%R9.320, %R10, %D18) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B171 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D18) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29131648, %D19 = "dma.tensor"(%R9.320, %B170) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B172 = "tsbc.s_bc"(%S1024, %D19) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B173 = "arith.sub"(%C0.0, %R10, %D19) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B174 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B175 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B176 = "arith.cast"(%R11, %D19) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B177 = "arith.mul"(%R9.320, %C0.6931471824645996, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B178 = "arith.sub"(%R11.8640, %R11, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B179 = "arith.cast"(%R9.320, %D19) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B180 = "arith.min"(%R11.8640, %C127, %D19) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B181 = "arith.max"(%R9.320, %C-127, %D19) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B182 = "arith.adds"(%R11.8640, %C127, %C23, %D19) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B183 = "sfu.taylor_4x"(%R11, %R12.896, %D19) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B184 = "arith.mul"(%R11.8640, %R9.320, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B185 = "arith.add"(%R9.320, %C1.0, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B186 = "arith.div"(%C1.0, %R11, %D19) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B187 = "arith.mul"(%R9.320, %R10, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B188 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D19) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B189 = "tsbc.s_bc"(%S1024, %D19) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B190 = "arith.sub"(%C0.0, %R10, %D19) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B191 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B192 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B193 = "arith.cast"(%R11, %D19) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B194 = "arith.mul"(%R13, %C0.6931471824645996, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B195 = "arith.sub"(%R11.8640, %R11, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B196 = "arith.cast"(%R13, %D19) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B197 = "arith.min"(%R11.8640, %C127, %D19) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B198 = "arith.max"(%R13, %C-127, %D19) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B199 = "arith.adds"(%R11.8640, %C127, %C23, %D19) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B200 = "sfu.taylor_4x"(%R11, %R12.896, %D19) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B201 = "arith.mul"(%R11.8640, %R13, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B202 = "arith.add"(%R13, %C1.0, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B203 = "arith.div"(%C1.0, %R11, %D19) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B204 = "arith.mul"(%R13, %R10, %D19) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50381696, %D20 = "dma.tensor"(%R9.320, %B188) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B205 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D20) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], res_add = False} : (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %G47104896, %D21 = "dma.tensor"(%R13, %B204) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.5376, %B206 = "tsbc.s_bc"(%S1024, %D21) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2688, %B207 = "arith.sub"(%C0.0, %R10, %D21) {round_mode = 0} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B208 = "arith.max"(%R2.2688, %C-3.4028198694267105e+35, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B209 = "arith.mul"(%R2.2688, %C1.4426950216293335, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B210 = "arith.cast"(%R0, %D21) {round_mode = 3} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B211 = "arith.mul"(%R13, %C0.6931471824645996, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B212 = "arith.sub"(%R2.2688, %R0, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B213 = "arith.cast"(%R13, %D21) {round_mode = 1} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B214 = "arith.min"(%R2.2688, %C127, %D21) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B215 = "arith.max"(%R13, %C-127, %D21) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B216 = "arith.adds"(%R2.2688, %C127, %C23, %D21) {round_mode = 1} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, ui8, none) -> (memref<1x32x55x161xsi32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B217 = "sfu.taylor_4x"(%R0, %R4.5376, %D21) : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B218 = "arith.mul"(%R2.2688, %R13, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B219 = "arith.add"(%R13, %C1.0, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B220 = "arith.div"(%C1.0, %R0, %D21) {iter = 3} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B221 = "arith.mul"(%R13, %R10, %D21) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R10, %B222 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D21) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], res_add = False} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D22 = "dma.tensor"(%G42715136, %B221) {decompress = False} : (memref<1x3x114x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, none), %R12.896, %B223 = "tsbc.s_bc"(%S1024, %D22) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B224 = "arith.sub"(%C0.0, %R10, %D22) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B225 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B226 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B227 = "arith.cast"(%R11, %D22) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B228 = "arith.mul"(%R9.320, %C0.6931471824645996, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B229 = "arith.sub"(%R11.8640, %R11, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B230 = "arith.cast"(%R9.320, %D22) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B231 = "arith.min"(%R11.8640, %C127, %D22) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B232 = "arith.max"(%R9.320, %C-127, %D22) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B233 = "arith.adds"(%R11.8640, %C127, %C23, %D22) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B234 = "sfu.taylor_4x"(%R11, %R12.896, %D22) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B235 = "arith.mul"(%R11.8640, %R9.320, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B236 = "arith.add"(%R9.320, %C1.0, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B237 = "arith.div"(%C1.0, %R11, %D22) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B238 = "arith.mul"(%R9.320, %R10, %D22) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B239 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D22) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29131968, %D23 = "dma.tensor"(%R9.320, %B238) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B240 = "tsbc.s_bc"(%S1024, %D23) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B241 = "arith.sub"(%C0.0, %R10, %D23) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B242 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B243 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B244 = "arith.cast"(%R11, %D23) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B245 = "arith.mul"(%R9.320, %C0.6931471824645996, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B246 = "arith.sub"(%R11.8640, %R11, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B247 = "arith.cast"(%R9.320, %D23) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B248 = "arith.min"(%R11.8640, %C127, %D23) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B249 = "arith.max"(%R9.320, %C-127, %D23) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B250 = "arith.adds"(%R11.8640, %C127, %C23, %D23) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B251 = "sfu.taylor_4x"(%R11, %R12.896, %D23) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B252 = "arith.mul"(%R11.8640, %R9.320, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B253 = "arith.add"(%R9.320, %C1.0, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B254 = "arith.div"(%C1.0, %R11, %D23) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B255 = "arith.mul"(%R9.320, %R10, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B256 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D23) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B257 = "tsbc.s_bc"(%S1024, %D23) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B258 = "arith.sub"(%C0.0, %R10, %D23) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B259 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B260 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B261 = "arith.cast"(%R11, %D23) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B262 = "arith.mul"(%R13, %C0.6931471824645996, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B263 = "arith.sub"(%R11.8640, %R11, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B264 = "arith.cast"(%R13, %D23) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B265 = "arith.min"(%R11.8640, %C127, %D23) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B266 = "arith.max"(%R13, %C-127, %D23) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B267 = "arith.adds"(%R11.8640, %C127, %C23, %D23) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B268 = "sfu.taylor_4x"(%R11, %R12.896, %D23) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B269 = "arith.mul"(%R11.8640, %R13, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B270 = "arith.add"(%R13, %C1.0, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B271 = "arith.div"(%C1.0, %R11, %D23) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B272 = "arith.mul"(%R13, %R10, %D23) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50382016, %D24 = "dma.tensor"(%R9.320, %B256) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B273 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D24) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], res_add = False} : (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %G47105216, %D25 = "dma.tensor"(%R13, %B272) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.4864, %B274 = "tsbc.s_bc"(%S1024, %D25) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2432, %B275 = "arith.sub"(%C0.0, %R10, %D25) {round_mode = 0} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B276 = "arith.max"(%R2.2432, %C-3.4028198694267105e+35, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B277 = "arith.mul"(%R2.2432, %C1.4426950216293335, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B278 = "arith.cast"(%R0, %D25) {round_mode = 3} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B279 = "arith.mul"(%R13, %C0.6931471824645996, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B280 = "arith.sub"(%R2.2432, %R0, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B281 = "arith.cast"(%R13, %D25) {round_mode = 1} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B282 = "arith.min"(%R2.2432, %C127, %D25) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B283 = "arith.max"(%R13, %C-127, %D25) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B284 = "arith.adds"(%R2.2432, %C127, %C23, %D25) {round_mode = 1} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x32x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B285 = "sfu.taylor_4x"(%R0, %R4.4864, %D25) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B286 = "arith.mul"(%R2.2432, %R13, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B287 = "arith.add"(%R13, %C1.0, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B288 = "arith.div"(%C1.0, %R0, %D25) {iter = 3} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B289 = "arith.mul"(%R13, %R10, %D25) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B290 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D25) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], res_add = False} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D26 = "dma.tensor"(%G42716400, %B289) {decompress = False} : (memref<1x3x114x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, none), %R12.896, %B291 = "tsbc.s_bc"(%S1024, %D26) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B292 = "arith.sub"(%C0.0, %R10, %D26) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B293 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B294 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B295 = "arith.cast"(%R11, %D26) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B296 = "arith.mul"(%R9.320, %C0.6931471824645996, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B297 = "arith.sub"(%R11.8640, %R11, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B298 = "arith.cast"(%R9.320, %D26) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B299 = "arith.min"(%R11.8640, %C127, %D26) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B300 = "arith.max"(%R9.320, %C-127, %D26) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B301 = "arith.adds"(%R11.8640, %C127, %C23, %D26) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B302 = "sfu.taylor_4x"(%R11, %R12.896, %D26) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B303 = "arith.mul"(%R11.8640, %R9.320, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B304 = "arith.add"(%R9.320, %C1.0, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B305 = "arith.div"(%C1.0, %R11, %D26) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B306 = "arith.mul"(%R9.320, %R10, %D26) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B307 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D26) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29148928, %D27 = "dma.tensor"(%R9.320, %B306) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B308 = "tsbc.s_bc"(%S1024, %D27) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B309 = "arith.sub"(%C0.0, %R10, %D27) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B310 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B311 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B312 = "arith.cast"(%R11, %D27) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B313 = "arith.mul"(%R9.320, %C0.6931471824645996, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B314 = "arith.sub"(%R11.8640, %R11, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B315 = "arith.cast"(%R9.320, %D27) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B316 = "arith.min"(%R11.8640, %C127, %D27) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B317 = "arith.max"(%R9.320, %C-127, %D27) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B318 = "arith.adds"(%R11.8640, %C127, %C23, %D27) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B319 = "sfu.taylor_4x"(%R11, %R12.896, %D27) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B320 = "arith.mul"(%R11.8640, %R9.320, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B321 = "arith.add"(%R9.320, %C1.0, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B322 = "arith.div"(%C1.0, %R11, %D27) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B323 = "arith.mul"(%R9.320, %R10, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B324 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D27) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B325 = "tsbc.s_bc"(%S1024, %D27) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B326 = "arith.sub"(%C0.0, %R10, %D27) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B327 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B328 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B329 = "arith.cast"(%R11, %D27) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B330 = "arith.mul"(%R13, %C0.6931471824645996, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B331 = "arith.sub"(%R11.8640, %R11, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B332 = "arith.cast"(%R13, %D27) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B333 = "arith.min"(%R11.8640, %C127, %D27) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B334 = "arith.max"(%R13, %C-127, %D27) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B335 = "arith.adds"(%R11.8640, %C127, %C23, %D27) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B336 = "sfu.taylor_4x"(%R11, %R12.896, %D27) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B337 = "arith.mul"(%R11.8640, %R13, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B338 = "arith.add"(%R13, %C1.0, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B339 = "arith.div"(%C1.0, %R11, %D27) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B340 = "arith.mul"(%R13, %R10, %D27) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50398976, %D28 = "dma.tensor"(%R9.320, %B324) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B341 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D28) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], res_add = False} : (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %G47122176, %D29 = "dma.tensor"(%R13, %B340) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.5376, %B342 = "tsbc.s_bc"(%S1024, %D29) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2688, %B343 = "arith.sub"(%C0.0, %R10, %D29) {round_mode = 0} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B344 = "arith.max"(%R2.2688, %C-3.4028198694267105e+35, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B345 = "arith.mul"(%R2.2688, %C1.4426950216293335, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B346 = "arith.cast"(%R0, %D29) {round_mode = 3} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B347 = "arith.mul"(%R13, %C0.6931471824645996, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B348 = "arith.sub"(%R2.2688, %R0, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B349 = "arith.cast"(%R13, %D29) {round_mode = 1} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B350 = "arith.min"(%R2.2688, %C127, %D29) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B351 = "arith.max"(%R13, %C-127, %D29) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B352 = "arith.adds"(%R2.2688, %C127, %C23, %D29) {round_mode = 1} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, ui8, none) -> (memref<1x32x55x161xsi32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B353 = "sfu.taylor_4x"(%R0, %R4.5376, %D29) : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B354 = "arith.mul"(%R2.2688, %R13, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B355 = "arith.add"(%R13, %C1.0, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B356 = "arith.div"(%C1.0, %R0, %D29) {iter = 3} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B357 = "arith.mul"(%R13, %R10, %D29) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R10, %B358 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D29) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], res_add = False} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D30 = "dma.tensor"(%G42991616, %B357) {decompress = False} : (memref<1x3x114x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, none), %R12.896, %B359 = "tsbc.s_bc"(%S1024, %D30) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B360 = "arith.sub"(%C0.0, %R10, %D30) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B361 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B362 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B363 = "arith.cast"(%R11, %D30) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B364 = "arith.mul"(%R9.320, %C0.6931471824645996, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B365 = "arith.sub"(%R11.8640, %R11, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B366 = "arith.cast"(%R9.320, %D30) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B367 = "arith.min"(%R11.8640, %C127, %D30) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B368 = "arith.max"(%R9.320, %C-127, %D30) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B369 = "arith.adds"(%R11.8640, %C127, %C23, %D30) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B370 = "sfu.taylor_4x"(%R11, %R12.896, %D30) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B371 = "arith.mul"(%R11.8640, %R9.320, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B372 = "arith.add"(%R9.320, %C1.0, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B373 = "arith.div"(%C1.0, %R11, %D30) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B374 = "arith.mul"(%R9.320, %R10, %D30) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B375 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D30) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29149248, %D31 = "dma.tensor"(%R9.320, %B374) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B376 = "tsbc.s_bc"(%S1024, %D31) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B377 = "arith.sub"(%C0.0, %R10, %D31) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B378 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B379 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B380 = "arith.cast"(%R11, %D31) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B381 = "arith.mul"(%R9.320, %C0.6931471824645996, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B382 = "arith.sub"(%R11.8640, %R11, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B383 = "arith.cast"(%R9.320, %D31) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B384 = "arith.min"(%R11.8640, %C127, %D31) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B385 = "arith.max"(%R9.320, %C-127, %D31) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B386 = "arith.adds"(%R11.8640, %C127, %C23, %D31) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B387 = "sfu.taylor_4x"(%R11, %R12.896, %D31) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B388 = "arith.mul"(%R11.8640, %R9.320, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B389 = "arith.add"(%R9.320, %C1.0, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B390 = "arith.div"(%C1.0, %R11, %D31) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B391 = "arith.mul"(%R9.320, %R10, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B392 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D31) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B393 = "tsbc.s_bc"(%S1024, %D31) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B394 = "arith.sub"(%C0.0, %R10, %D31) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B395 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B396 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B397 = "arith.cast"(%R11, %D31) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B398 = "arith.mul"(%R13, %C0.6931471824645996, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B399 = "arith.sub"(%R11.8640, %R11, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B400 = "arith.cast"(%R13, %D31) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B401 = "arith.min"(%R11.8640, %C127, %D31) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B402 = "arith.max"(%R13, %C-127, %D31) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B403 = "arith.adds"(%R11.8640, %C127, %C23, %D31) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B404 = "sfu.taylor_4x"(%R11, %R12.896, %D31) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B405 = "arith.mul"(%R11.8640, %R13, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B406 = "arith.add"(%R13, %C1.0, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B407 = "arith.div"(%C1.0, %R11, %D31) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B408 = "arith.mul"(%R13, %R10, %D31) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50399296, %D32 = "dma.tensor"(%R9.320, %B392) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B409 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D32) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], res_add = False} : (memref<1x3x114x322xf32, strides: [36720, 36720, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %G47122496, %D33 = "dma.tensor"(%R13, %B408) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.4864, %B410 = "tsbc.s_bc"(%S1024, %D33) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2432, %B411 = "arith.sub"(%C0.0, %R10, %D33) {round_mode = 0} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B412 = "arith.max"(%R2.2432, %C-3.4028198694267105e+35, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B413 = "arith.mul"(%R2.2432, %C1.4426950216293335, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B414 = "arith.cast"(%R0, %D33) {round_mode = 3} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B415 = "arith.mul"(%R13, %C0.6931471824645996, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B416 = "arith.sub"(%R2.2432, %R0, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B417 = "arith.cast"(%R13, %D33) {round_mode = 1} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B418 = "arith.min"(%R2.2432, %C127, %D33) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B419 = "arith.max"(%R13, %C-127, %D33) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B420 = "arith.adds"(%R2.2432, %C127, %C23, %D33) {round_mode = 1} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x32x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B421 = "sfu.taylor_4x"(%R0, %R4.4864, %D33) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B422 = "arith.mul"(%R2.2432, %R13, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B423 = "arith.add"(%R13, %C1.0, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B424 = "arith.div"(%C1.0, %R0, %D33) {iter = 3} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B425 = "arith.mul"(%R13, %R10, %D33) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B426 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D33) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], res_add = False} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D34 = "dma.tensor"(%G42992880, %B425) {decompress = False} : (memref<1x3x114x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, none), %R12.896, %B427 = "tsbc.s_bc"(%S1024, %D34) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B428 = "arith.sub"(%C0.0, %R10, %D34) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B429 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B430 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B431 = "arith.cast"(%R11, %D34) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B432 = "arith.mul"(%R9.320, %C0.6931471824645996, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B433 = "arith.sub"(%R11.8640, %R11, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B434 = "arith.cast"(%R9.320, %D34) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B435 = "arith.min"(%R11.8640, %C127, %D34) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B436 = "arith.max"(%R9.320, %C-127, %D34) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B437 = "arith.adds"(%R11.8640, %C127, %C23, %D34) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B438 = "sfu.taylor_4x"(%R11, %R12.896, %D34) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B439 = "arith.mul"(%R11.8640, %R9.320, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B440 = "arith.add"(%R9.320, %C1.0, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B441 = "arith.div"(%C1.0, %R11, %D34) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B442 = "arith.mul"(%R9.320, %R10, %D34) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B443 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D34) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29166208, %D35 = "dma.tensor"(%R9.320, %B442) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B444 = "tsbc.s_bc"(%S1024, %D35) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B445 = "arith.sub"(%C0.0, %R10, %D35) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B446 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B447 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B448 = "arith.cast"(%R11, %D35) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B449 = "arith.mul"(%R9.320, %C0.6931471824645996, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B450 = "arith.sub"(%R11.8640, %R11, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B451 = "arith.cast"(%R9.320, %D35) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B452 = "arith.min"(%R11.8640, %C127, %D35) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B453 = "arith.max"(%R9.320, %C-127, %D35) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B454 = "arith.adds"(%R11.8640, %C127, %C23, %D35) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B455 = "sfu.taylor_4x"(%R11, %R12.896, %D35) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B456 = "arith.mul"(%R11.8640, %R9.320, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B457 = "arith.add"(%R9.320, %C1.0, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B458 = "arith.div"(%C1.0, %R11, %D35) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B459 = "arith.mul"(%R9.320, %R10, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B460 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D35) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B461 = "tsbc.s_bc"(%S1024, %D35) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B462 = "arith.sub"(%C0.0, %R10, %D35) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B463 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B464 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B465 = "arith.cast"(%R11, %D35) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B466 = "arith.mul"(%R13, %C0.6931471824645996, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B467 = "arith.sub"(%R11.8640, %R11, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B468 = "arith.cast"(%R13, %D35) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B469 = "arith.min"(%R11.8640, %C127, %D35) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B470 = "arith.max"(%R13, %C-127, %D35) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B471 = "arith.adds"(%R11.8640, %C127, %C23, %D35) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B472 = "sfu.taylor_4x"(%R11, %R12.896, %D35) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B473 = "arith.mul"(%R11.8640, %R13, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B474 = "arith.add"(%R13, %C1.0, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B475 = "arith.div"(%C1.0, %R11, %D35) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B476 = "arith.mul"(%R13, %R10, %D35) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50416256, %D36 = "dma.tensor"(%R9.320, %B460) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B477 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D36) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], res_add = False} : (memref<1x3x114x324xf32, strides: [36944, 36944, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %G47139456, %D37 = "dma.tensor"(%R13, %B476) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.5376, %B478 = "tsbc.s_bc"(%S1024, %D37) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2688, %B479 = "arith.sub"(%C0.0, %R10, %D37) {round_mode = 0} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B480 = "arith.max"(%R2.2688, %C-3.4028198694267105e+35, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B481 = "arith.mul"(%R2.2688, %C1.4426950216293335, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B482 = "arith.cast"(%R0, %D37) {round_mode = 3} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B483 = "arith.mul"(%R13, %C0.6931471824645996, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B484 = "arith.sub"(%R2.2688, %R0, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B485 = "arith.cast"(%R13, %D37) {round_mode = 1} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B486 = "arith.min"(%R2.2688, %C127, %D37) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B487 = "arith.max"(%R13, %C-127, %D37) {round_mode = 0} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, none) -> (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, none), %R13, %B488 = "arith.adds"(%R2.2688, %C127, %C23, %D37) {round_mode = 1} : (memref<1x32x55x161xsi16, strides: [8864, 8864, 161, 1]>, si16, ui8, none) -> (memref<1x32x55x161xsi32, strides: [8864, 8864, 161, 1]>, none), %R2.2688, %B489 = "sfu.taylor_4x"(%R0, %R4.5376, %D37) : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B490 = "arith.mul"(%R2.2688, %R13, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R0, %B491 = "arith.add"(%R13, %C1.0, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, f32, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B492 = "arith.div"(%C1.0, %R0, %D37) {iter = 3} : (f32, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R13, %B493 = "arith.mul"(%R13, %R10, %D37) {round_mode = 0} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none) -> (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, none), %R10, %B494 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D37) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], res_add = False} : (memref<1x32x55x161xf32, strides: [8864, 8864, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R0, %D38 = "dma.tensor"(%G43268096, %B493) {decompress = False} : (memref<1x3x110x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x110x322xf32, strides: [35424, 35424, 322, 1]>, none), %R12.896, %B495 = "tsbc.s_bc"(%S1024, %D38) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B496 = "arith.sub"(%C0.0, %R10, %D38) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B497 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B498 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B499 = "arith.cast"(%R11, %D38) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B500 = "arith.mul"(%R9.320, %C0.6931471824645996, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B501 = "arith.sub"(%R11.8640, %R11, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B502 = "arith.cast"(%R9.320, %D38) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B503 = "arith.min"(%R11.8640, %C127, %D38) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B504 = "arith.max"(%R9.320, %C-127, %D38) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B505 = "arith.adds"(%R11.8640, %C127, %C23, %D38) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B506 = "sfu.taylor_4x"(%R11, %R12.896, %D38) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B507 = "arith.mul"(%R11.8640, %R9.320, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B508 = "arith.add"(%R9.320, %C1.0, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B509 = "arith.div"(%C1.0, %R11, %D38) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B510 = "arith.mul"(%R9.320, %R10, %D38) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B511 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D38) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G29166528, %D39 = "dma.tensor"(%R9.320, %B510) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.896, %B512 = "tsbc.s_bc"(%S1024, %D39) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B513 = "arith.sub"(%C0.0, %R10, %D39) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B514 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B515 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B516 = "arith.cast"(%R11, %D39) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B517 = "arith.mul"(%R9.320, %C0.6931471824645996, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B518 = "arith.sub"(%R11.8640, %R11, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B519 = "arith.cast"(%R9.320, %D39) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B520 = "arith.min"(%R11.8640, %C127, %D39) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B521 = "arith.max"(%R9.320, %C-127, %D39) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R9.320, %B522 = "arith.adds"(%R11.8640, %C127, %C23, %D39) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B523 = "sfu.taylor_4x"(%R11, %R12.896, %D39) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B524 = "arith.mul"(%R11.8640, %R9.320, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B525 = "arith.add"(%R9.320, %C1.0, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B526 = "arith.div"(%C1.0, %R11, %D39) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R9.320, %B527 = "arith.mul"(%R9.320, %R10, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B528 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D39) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R12.896, %B529 = "tsbc.s_bc"(%S1024, %D39) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8640, %B530 = "arith.sub"(%C0.0, %R10, %D39) {round_mode = 0} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B531 = "arith.max"(%R11.8640, %C-3.4028198694267105e+35, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B532 = "arith.mul"(%R11.8640, %C1.4426950216293335, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B533 = "arith.cast"(%R11, %D39) {round_mode = 3} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B534 = "arith.mul"(%R13, %C0.6931471824645996, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B535 = "arith.sub"(%R11.8640, %R11, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B536 = "arith.cast"(%R13, %D39) {round_mode = 1} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B537 = "arith.min"(%R11.8640, %C127, %D39) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R11.8640, %B538 = "arith.max"(%R13, %C-127, %D39) {round_mode = 0} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R13, %B539 = "arith.adds"(%R11.8640, %C127, %C23, %D39) {round_mode = 1} : (memref<1x32x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x32x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R11.8640, %B540 = "sfu.taylor_4x"(%R11, %R12.896, %D39) : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B541 = "arith.mul"(%R11.8640, %R13, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R11, %B542 = "arith.add"(%R13, %C1.0, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B543 = "arith.div"(%C1.0, %R11, %D39) {iter = 3} : (f32, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R13, %B544 = "arith.mul"(%R13, %R10, %D39) {round_mode = 0} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G50416576, %D40 = "dma.tensor"(%R9.320, %B528) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B545 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D40) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], res_add = False} : (memref<1x3x110x322xf32, strides: [35424, 35424, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %G47139776, %D41 = "dma.tensor"(%R13, %B544) {decompress = False} : (memref<1x32x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x32x27x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.2304, %B546 = "tsbc.s_bc"(%S1024, %D41) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1152, %B547 = "arith.sub"(%C0.0, %R10, %D41) {round_mode = 0} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B548 = "arith.max"(%R2.1152, %C-3.4028198694267105e+35, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B549 = "arith.mul"(%R2.1152, %C1.4426950216293335, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B550 = "arith.cast"(%R0, %D41) {round_mode = 3} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B551 = "arith.mul"(%R13, %C0.6931471824645996, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B552 = "arith.sub"(%R2.1152, %R0, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B553 = "arith.cast"(%R13, %D41) {round_mode = 1} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B554 = "arith.min"(%R2.1152, %C127, %D41) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B555 = "arith.max"(%R13, %C-127, %D41) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B556 = "arith.adds"(%R2.1152, %C127, %C23, %D41) {round_mode = 1} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, ui8, none) -> (memref<1x32x53x160xsi32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B557 = "sfu.taylor_4x"(%R0, %R4.2304, %D41) : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B558 = "arith.mul"(%R2.1152, %R13, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B559 = "arith.add"(%R13, %C1.0, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B560 = "arith.div"(%C1.0, %R0, %D41) {iter = 3} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B561 = "arith.mul"(%R13, %R10, %D41) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B562 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D41) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], res_add = False} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R0, %D42 = "dma.tensor"(%G43269360, %B561) {decompress = False} : (memref<1x3x110x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x110x324xf32, strides: [35648, 35648, 324, 1]>, none), %R12.256, %B563 = "tsbc.s_bc"(%S1024, %D42) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B564 = "arith.sub"(%C0.0, %R10, %D42) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B565 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B566 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B567 = "arith.cast"(%R11, %D42) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B568 = "arith.mul"(%R9.320, %C0.6931471824645996, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B569 = "arith.sub"(%R11.8320, %R11, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B570 = "arith.cast"(%R9.320, %D42) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B571 = "arith.min"(%R11.8320, %C127, %D42) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B572 = "arith.max"(%R9.320, %C-127, %D42) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B573 = "arith.adds"(%R11.8320, %C127, %C23, %D42) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B574 = "sfu.taylor_4x"(%R11, %R12.256, %D42) : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B575 = "arith.mul"(%R11.8320, %R9.320, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B576 = "arith.add"(%R9.320, %C1.0, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B577 = "arith.div"(%C1.0, %R11, %D42) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B578 = "arith.mul"(%R9.320, %R10, %D42) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B579 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D42) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G29183488, %D43 = "dma.tensor"(%R9.320, %B578) {decompress = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.256, %B580 = "tsbc.s_bc"(%S1024, %D43) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B581 = "arith.sub"(%C0.0, %R10, %D43) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B582 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B583 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B584 = "arith.cast"(%R11, %D43) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B585 = "arith.mul"(%R9.320, %C0.6931471824645996, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B586 = "arith.sub"(%R11.8320, %R11, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B587 = "arith.cast"(%R9.320, %D43) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B588 = "arith.min"(%R11.8320, %C127, %D43) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B589 = "arith.max"(%R9.320, %C-127, %D43) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B590 = "arith.adds"(%R11.8320, %C127, %C23, %D43) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B591 = "sfu.taylor_4x"(%R11, %R12.256, %D43) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B592 = "arith.mul"(%R11.8320, %R9.320, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B593 = "arith.add"(%R9.320, %C1.0, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B594 = "arith.div"(%C1.0, %R11, %D43) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B595 = "arith.mul"(%R9.320, %R10, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B596 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D43) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R12.256, %B597 = "tsbc.s_bc"(%S1024, %D43) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B598 = "arith.sub"(%C0.0, %R10, %D43) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B599 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B600 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B601 = "arith.cast"(%R11, %D43) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B602 = "arith.mul"(%R13, %C0.6931471824645996, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B603 = "arith.sub"(%R11.8320, %R11, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B604 = "arith.cast"(%R13, %D43) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B605 = "arith.min"(%R11.8320, %C127, %D43) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B606 = "arith.max"(%R13, %C-127, %D43) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B607 = "arith.adds"(%R11.8320, %C127, %C23, %D43) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B608 = "sfu.taylor_4x"(%R11, %R12.256, %D43) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B609 = "arith.mul"(%R11.8320, %R13, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B610 = "arith.add"(%R13, %C1.0, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B611 = "arith.div"(%C1.0, %R11, %D43) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B612 = "arith.mul"(%R13, %R10, %D43) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G50433536, %D44 = "dma.tensor"(%R9.320, %B596) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B613 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D44) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], res_add = False} : (memref<1x3x110x324xf32, strides: [35648, 35648, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %G47156736, %D45 = "dma.tensor"(%R13, %B612) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.2816, %B614 = "tsbc.s_bc"(%S1024, %D45) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1408, %B615 = "arith.sub"(%C0.0, %R10, %D45) {round_mode = 0} : (f32, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B616 = "arith.max"(%R2.1408, %C-3.4028198694267105e+35, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B617 = "arith.mul"(%R2.1408, %C1.4426950216293335, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B618 = "arith.cast"(%R0, %D45) {round_mode = 3} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B619 = "arith.mul"(%R13, %C0.6931471824645996, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B620 = "arith.sub"(%R2.1408, %R0, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B621 = "arith.cast"(%R13, %D45) {round_mode = 1} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R13, %B622 = "arith.min"(%R2.1408, %C127, %D45) {round_mode = 0} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B623 = "arith.max"(%R13, %C-127, %D45) {round_mode = 0} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R13, %B624 = "arith.adds"(%R2.1408, %C127, %C23, %D45) {round_mode = 1} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, ui8, none) -> (memref<1x32x53x161xsi32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B625 = "sfu.taylor_4x"(%R0, %R4.2816, %D45) : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B626 = "arith.mul"(%R2.1408, %R13, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B627 = "arith.add"(%R13, %C1.0, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B628 = "arith.div"(%C1.0, %R0, %D45) {iter = 3} : (f32, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B629 = "arith.mul"(%R13, %R10, %D45) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R10, %B630 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D45) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], res_add = False} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R0, %D46 = "dma.tensor"(%G43534336, %B629) {decompress = False} : (memref<1x3x108x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x108x322xf32, strides: [34784, 34784, 322, 1]>, none), %R12.256, %B631 = "tsbc.s_bc"(%S1024, %D46) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B632 = "arith.sub"(%C0.0, %R10, %D46) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B633 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B634 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B635 = "arith.cast"(%R11, %D46) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B636 = "arith.mul"(%R9.320, %C0.6931471824645996, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B637 = "arith.sub"(%R11.8320, %R11, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B638 = "arith.cast"(%R9.320, %D46) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B639 = "arith.min"(%R11.8320, %C127, %D46) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B640 = "arith.max"(%R9.320, %C-127, %D46) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B641 = "arith.adds"(%R11.8320, %C127, %C23, %D46) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B642 = "sfu.taylor_4x"(%R11, %R12.256, %D46) : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B643 = "arith.mul"(%R11.8320, %R9.320, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B644 = "arith.add"(%R9.320, %C1.0, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B645 = "arith.div"(%C1.0, %R11, %D46) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B646 = "arith.mul"(%R9.320, %R10, %D46) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B647 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D46) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G29183808, %D47 = "dma.tensor"(%R9.320, %B646) {decompress = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.256, %B648 = "tsbc.s_bc"(%S1024, %D47) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B649 = "arith.sub"(%C0.0, %R10, %D47) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B650 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B651 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B652 = "arith.cast"(%R11, %D47) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B653 = "arith.mul"(%R9.320, %C0.6931471824645996, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B654 = "arith.sub"(%R11.8320, %R11, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B655 = "arith.cast"(%R9.320, %D47) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B656 = "arith.min"(%R11.8320, %C127, %D47) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B657 = "arith.max"(%R9.320, %C-127, %D47) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B658 = "arith.adds"(%R11.8320, %C127, %C23, %D47) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B659 = "sfu.taylor_4x"(%R11, %R12.256, %D47) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B660 = "arith.mul"(%R11.8320, %R9.320, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B661 = "arith.add"(%R9.320, %C1.0, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B662 = "arith.div"(%C1.0, %R11, %D47) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B663 = "arith.mul"(%R9.320, %R10, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B664 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D47) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R12.256, %B665 = "tsbc.s_bc"(%S1024, %D47) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B666 = "arith.sub"(%C0.0, %R10, %D47) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B667 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B668 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B669 = "arith.cast"(%R11, %D47) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B670 = "arith.mul"(%R13, %C0.6931471824645996, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B671 = "arith.sub"(%R11.8320, %R11, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B672 = "arith.cast"(%R13, %D47) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B673 = "arith.min"(%R11.8320, %C127, %D47) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B674 = "arith.max"(%R13, %C-127, %D47) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B675 = "arith.adds"(%R11.8320, %C127, %C23, %D47) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B676 = "sfu.taylor_4x"(%R11, %R12.256, %D47) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B677 = "arith.mul"(%R11.8320, %R13, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B678 = "arith.add"(%R13, %C1.0, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B679 = "arith.div"(%C1.0, %R11, %D47) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B680 = "arith.mul"(%R13, %R10, %D47) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G50433856, %D48 = "dma.tensor"(%R9.320, %B664) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B681 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D48) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 2, 2, 0], res_add = False} : (memref<1x3x108x322xf32, strides: [34784, 34784, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %G47157056, %D49 = "dma.tensor"(%R13, %B680) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.2304, %B682 = "tsbc.s_bc"(%S1024, %D49) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1152, %B683 = "arith.sub"(%C0.0, %R10, %D49) {round_mode = 0} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B684 = "arith.max"(%R2.1152, %C-3.4028198694267105e+35, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B685 = "arith.mul"(%R2.1152, %C1.4426950216293335, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B686 = "arith.cast"(%R0, %D49) {round_mode = 3} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B687 = "arith.mul"(%R13, %C0.6931471824645996, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B688 = "arith.sub"(%R2.1152, %R0, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B689 = "arith.cast"(%R13, %D49) {round_mode = 1} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B690 = "arith.min"(%R2.1152, %C127, %D49) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B691 = "arith.max"(%R13, %C-127, %D49) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B692 = "arith.adds"(%R2.1152, %C127, %C23, %D49) {round_mode = 1} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, ui8, none) -> (memref<1x32x53x160xsi32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B693 = "sfu.taylor_4x"(%R0, %R4.2304, %D49) : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B694 = "arith.mul"(%R2.1152, %R13, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B695 = "arith.add"(%R13, %C1.0, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B696 = "arith.div"(%C1.0, %R0, %D49) {iter = 3} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B697 = "arith.mul"(%R13, %R10, %D49) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B698 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D49) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 0], res_add = False} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R0, %D50 = "dma.tensor"(%G43535600, %B697) {decompress = False} : (memref<1x3x108x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x108x324xf32, strides: [34992, 34992, 324, 1]>, none), %R12.256, %B699 = "tsbc.s_bc"(%S1024, %D50) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B700 = "arith.sub"(%C0.0, %R10, %D50) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B701 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B702 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B703 = "arith.cast"(%R11, %D50) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B704 = "arith.mul"(%R9.320, %C0.6931471824645996, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B705 = "arith.sub"(%R11.8320, %R11, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B706 = "arith.cast"(%R9.320, %D50) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B707 = "arith.min"(%R11.8320, %C127, %D50) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B708 = "arith.max"(%R9.320, %C-127, %D50) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B709 = "arith.adds"(%R11.8320, %C127, %C23, %D50) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B710 = "sfu.taylor_4x"(%R11, %R12.256, %D50) : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B711 = "arith.mul"(%R11.8320, %R9.320, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B712 = "arith.add"(%R9.320, %C1.0, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B713 = "arith.div"(%C1.0, %R11, %D50) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B714 = "arith.mul"(%R9.320, %R10, %D50) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B715 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D50) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G29200128, %D51 = "dma.tensor"(%R9.320, %B714) {decompress = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.256, %B716 = "tsbc.s_bc"(%S1024, %D51) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B717 = "arith.sub"(%C0.0, %R10, %D51) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B718 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B719 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B720 = "arith.cast"(%R11, %D51) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B721 = "arith.mul"(%R9.320, %C0.6931471824645996, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B722 = "arith.sub"(%R11.8320, %R11, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B723 = "arith.cast"(%R9.320, %D51) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B724 = "arith.min"(%R11.8320, %C127, %D51) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B725 = "arith.max"(%R9.320, %C-127, %D51) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B726 = "arith.adds"(%R11.8320, %C127, %C23, %D51) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B727 = "sfu.taylor_4x"(%R11, %R12.256, %D51) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B728 = "arith.mul"(%R11.8320, %R9.320, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B729 = "arith.add"(%R9.320, %C1.0, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B730 = "arith.div"(%C1.0, %R11, %D51) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B731 = "arith.mul"(%R9.320, %R10, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B732 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D51) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R12.256, %B733 = "tsbc.s_bc"(%S1024, %D51) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B734 = "arith.sub"(%C0.0, %R10, %D51) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B735 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B736 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B737 = "arith.cast"(%R11, %D51) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B738 = "arith.mul"(%R13, %C0.6931471824645996, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B739 = "arith.sub"(%R11.8320, %R11, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B740 = "arith.cast"(%R13, %D51) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B741 = "arith.min"(%R11.8320, %C127, %D51) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B742 = "arith.max"(%R13, %C-127, %D51) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B743 = "arith.adds"(%R11.8320, %C127, %C23, %D51) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B744 = "sfu.taylor_4x"(%R11, %R12.256, %D51) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B745 = "arith.mul"(%R11.8320, %R13, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B746 = "arith.add"(%R13, %C1.0, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B747 = "arith.div"(%C1.0, %R11, %D51) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B748 = "arith.mul"(%R13, %R10, %D51) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G50450176, %D52 = "dma.tensor"(%R9.320, %B732) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R10, %B749 = "conv.normal"(%R0, %R15.2688, %R9.10176, %C0.0, %D52) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 2, 0, 2], res_add = False} : (memref<1x3x108x324xf32, strides: [34992, 34992, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %G47173376, %D53 = "dma.tensor"(%R13, %B748) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.2816, %B750 = "tsbc.s_bc"(%S1024, %D53) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1408, %B751 = "arith.sub"(%C0.0, %R10, %D53) {round_mode = 0} : (f32, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B752 = "arith.max"(%R2.1408, %C-3.4028198694267105e+35, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B753 = "arith.mul"(%R2.1408, %C1.4426950216293335, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B754 = "arith.cast"(%R0, %D53) {round_mode = 3} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B755 = "arith.mul"(%R13, %C0.6931471824645996, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B756 = "arith.sub"(%R2.1408, %R0, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B757 = "arith.cast"(%R13, %D53) {round_mode = 1} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R13, %B758 = "arith.min"(%R2.1408, %C127, %D53) {round_mode = 0} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B759 = "arith.max"(%R13, %C-127, %D53) {round_mode = 0} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, none) -> (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, none), %R13, %B760 = "arith.adds"(%R2.1408, %C127, %C23, %D53) {round_mode = 1} : (memref<1x32x53x161xsi16, strides: [8544, 8544, 161, 1]>, si16, ui8, none) -> (memref<1x32x53x161xsi32, strides: [8544, 8544, 161, 1]>, none), %R2.1408, %B761 = "sfu.taylor_4x"(%R0, %R4.2816, %D53) : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B762 = "arith.mul"(%R2.1408, %R13, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R0, %B763 = "arith.add"(%R13, %C1.0, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, f32, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B764 = "arith.div"(%C1.0, %R0, %D53) {iter = 3} : (f32, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R13, %B765 = "arith.mul"(%R13, %R10, %D53) {round_mode = 0} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none) -> (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, none), %R10, %B766 = "conv.normal"(%R13, %R9.8960, %R9.10112, %C0.0, %D53) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 1, 0, 1], res_add = False} : (memref<1x32x53x161xf32, strides: [8544, 8544, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R0, %D54 = "dma.tensor"(%G47087616, %B765) {decompress = False} : (memref<1x32x55x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R12.256, %B767 = "tsbc.s_bc"(%S1024, %D54) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B768 = "arith.sub"(%C0.0, %R10, %D54) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B769 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B770 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B771 = "arith.cast"(%R11, %D54) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B772 = "arith.mul"(%R9.320, %C0.6931471824645996, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B773 = "arith.sub"(%R11.8320, %R11, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B774 = "arith.cast"(%R9.320, %D54) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B775 = "arith.min"(%R11.8320, %C127, %D54) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B776 = "arith.max"(%R9.320, %C-127, %D54) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B777 = "arith.adds"(%R11.8320, %C127, %C23, %D54) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B778 = "sfu.taylor_4x"(%R11, %R12.256, %D54) : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B779 = "arith.mul"(%R11.8320, %R9.320, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B780 = "arith.add"(%R9.320, %C1.0, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B781 = "arith.div"(%C1.0, %R11, %D54) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B782 = "arith.mul"(%R9.320, %R10, %D54) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B783 = "conv.normal"(%R9.320, %R15.3136, %R9.10240, %C0.0, %D54) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G29200448, %D55 = "dma.tensor"(%R9.320, %B782) {decompress = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R12.256, %B784 = "tsbc.s_bc"(%S1024, %D55) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B785 = "arith.sub"(%C0.0, %R10, %D55) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B786 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B787 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B788 = "arith.cast"(%R11, %D55) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B789 = "arith.mul"(%R9.320, %C0.6931471824645996, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B790 = "arith.sub"(%R11.8320, %R11, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B791 = "arith.cast"(%R9.320, %D55) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B792 = "arith.min"(%R11.8320, %C127, %D55) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B793 = "arith.max"(%R9.320, %C-127, %D55) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B794 = "arith.adds"(%R11.8320, %C127, %C23, %D55) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B795 = "sfu.taylor_4x"(%R11, %R12.256, %D55) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B796 = "arith.mul"(%R11.8320, %R9.320, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B797 = "arith.add"(%R9.320, %C1.0, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B798 = "arith.div"(%C1.0, %R11, %D55) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R9.320, %B799 = "arith.mul"(%R9.320, %R10, %D55) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %B800 = "conv.normal"(%R9.320, %R15.3392, %R9.10304, %C0.0, %D55) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R12.7872, %D56 = "dma.tensor"(%G155648, %B799) {decompress = False} : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R12.256, %B801 = "tsbc.s_bc"(%S1024, %D56) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.8320, %B802 = "arith.sub"(%C0.0, %R10, %D56) {round_mode = 0} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B803 = "arith.max"(%R11.8320, %C-3.4028198694267105e+35, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B804 = "arith.mul"(%R11.8320, %C1.4426950216293335, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B805 = "arith.cast"(%R11, %D56) {round_mode = 3} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B806 = "arith.mul"(%R13, %C0.6931471824645996, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B807 = "arith.sub"(%R11.8320, %R11, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B808 = "arith.cast"(%R13, %D56) {round_mode = 1} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B809 = "arith.min"(%R11.8320, %C127, %D56) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B810 = "arith.max"(%R13, %C-127, %D56) {round_mode = 0} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R13, %B811 = "arith.adds"(%R11.8320, %C127, %C23, %D56) {round_mode = 1} : (memref<1x32x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x32x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R11.8320, %B812 = "sfu.taylor_4x"(%R11, %R12.256, %D56) : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B813 = "arith.mul"(%R11.8320, %R13, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R11, %B814 = "arith.add"(%R13, %C1.0, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B815 = "arith.div"(%C1.0, %R11, %D56) {iter = 3} : (f32, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R13, %B816 = "arith.mul"(%R13, %R10, %D56) {round_mode = 0} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G50450496, %D57 = "dma.tensor"(%R9.320, %B800) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.2432, %D58 = "dma.tensor"(%G118784, %B800) {decompress = False} : (memref<1x32x32x9xf32, strides: [9216, 288, 9, 1]>, none) -> (memref<1x32x32x9xf32, strides: [288, 288, 9, 1]>, none), %R10, %B817 = "conv.normal"(%R0, %R15.2432, %R12.7872, %C0.0, %D58) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], res_add = False} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R5, %D59 = "dma.tensor"(%G29114368, %B816) {decompress = False} : (memref<1x64x54x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %G47173696, %D60 = "dma.tensor"(%R13, %B816) {decompress = False} : (memref<1x32x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x32x26x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.3584, %B818 = "tsbc.s_bc"(%S1024, %D60) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1792, %B819 = "arith.sub"(%C0.0, %R10, %D60) {round_mode = 0} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B820 = "arith.max"(%R2.1792, %C-3.4028198694267105e+35, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B821 = "arith.mul"(%R2.1792, %C1.4426950216293335, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B822 = "arith.cast"(%R0, %D60) {round_mode = 3} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B823 = "arith.mul"(%R13, %C0.6931471824645996, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B824 = "arith.sub"(%R2.1792, %R0, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B825 = "arith.cast"(%R13, %D60) {round_mode = 1} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R13, %B826 = "arith.min"(%R2.1792, %C127, %D60) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B827 = "arith.max"(%R13, %C-127, %D60) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R13, %B828 = "arith.adds"(%R2.1792, %C127, %C23, %D60) {round_mode = 1} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, ui8, none) -> (memref<1x32x54x160xsi32, strides: [8640, 8640, 160, 1]>, none), %R2.1792, %B829 = "sfu.taylor_4x"(%R0, %R4.3584, %D60) : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B830 = "arith.mul"(%R2.1792, %R13, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B831 = "arith.add"(%R13, %C1.0, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B832 = "arith.div"(%C1.0, %R0, %D60) {iter = 3} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R13, %B833 = "arith.mul"(%R13, %R10, %D60) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R7.2432, %D61 = "dma.tensor"(%G50364416, %B817) {decompress = False} : (memref<1x32x54x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R0, %B834 = "arith.add"(%R7.2432, %R13, %D61) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [0, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [0, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R12.7552, %D62 = "dma.tensor"(%G159744, %B833) {decompress = False} : (memref<1x32x64x1xf32, strides: [2048, 64, 1, 1]>, none) -> (memref<1x32x64x1xf32, strides: [64, 64, 1, 1]>, none), %R12.7808, %D63 = "dma.tensor"(%G167936, %B833) {decompress = False} : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R2.2432, %B835 = "conv.normal"(%R5, %R12.7552, %R12.7808, %C0.0, %D63) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R9.3584, %B836 = "tsbc.s_bc"(%S1024, %D63) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.1792, %B837 = "arith.sub"(%C0.0, %R2.2432, %D63) {round_mode = 0} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R7.1792, %B838 = "arith.max"(%R7.1792, %C-3.4028198694267105e+35, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R5, %B839 = "arith.mul"(%R7.1792, %C1.4426950216293335, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R10, %B840 = "arith.cast"(%R5, %D63) {round_mode = 3} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R5, %B841 = "arith.mul"(%R10, %C0.6931471824645996, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R5, %B842 = "arith.sub"(%R7.1792, %R5, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R7.1792, %B843 = "arith.cast"(%R10, %D63) {round_mode = 1} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R10, %B844 = "arith.min"(%R7.1792, %C127, %D63) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R7.1792, %B845 = "arith.max"(%R10, %C-127, %D63) {round_mode = 0} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R10, %B846 = "arith.adds"(%R7.1792, %C127, %C23, %D63) {round_mode = 1} : (memref<1x32x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, ui8, none) -> (memref<1x32x54x160xsi32, strides: [8640, 8640, 160, 1]>, none), %R7.1792, %B847 = "sfu.taylor_4x"(%R5, %R9.3584, %D63) : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R10, %B848 = "arith.mul"(%R7.1792, %R10, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R5, %B849 = "arith.add"(%R10, %C1.0, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R10, %B850 = "arith.div"(%C1.0, %R5, %D63) {iter = 3} : (f32, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R10, %B851 = "arith.mul"(%R10, %R2.2432, %D63) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R12.7936, %D64 = "dma.tensor"(%G188416, %B835) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R3, %B852 = "arith.copy"(%R0, %D64) {round_mode = 0} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R3.L32, %B853 = "tsbc.l_copy"(%R10, %D64) : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R15.3584, %D65 = "dma.tensor"(%G172032, %B851) {decompress = False} : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [64, 64, 1, 1]>, none), %R0, %B854 = "conv.normal"(%R3, %R15.3584, %R12.7936, %C0.0, %D65) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R10.3584, %B855 = "tsbc.s_bc"(%S1024, %D65) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.1792, %B856 = "arith.sub"(%C0.0, %R0, %D65) {round_mode = 0} : (f32, memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R8.1792, %B857 = "arith.max"(%R8.1792, %C-3.4028198694267105e+35, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R6, %B858 = "arith.mul"(%R8.1792, %C1.4426950216293335, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R3, %B859 = "arith.cast"(%R6, %D65) {round_mode = 3} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R6, %B860 = "arith.mul"(%R3, %C0.6931471824645996, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R6, %B861 = "arith.sub"(%R8.1792, %R6, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R8.1792, %B862 = "arith.cast"(%R3, %D65) {round_mode = 1} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R3, %B863 = "arith.min"(%R8.1792, %C127, %D65) {round_mode = 0} : (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R8.1792, %B864 = "arith.max"(%R3, %C-127, %D65) {round_mode = 0} : (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, none) -> (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, none), %R3, %B865 = "arith.adds"(%R8.1792, %C127, %C23, %D65) {round_mode = 1} : (memref<1x64x54x160xsi16, strides: [8640, 8640, 160, 1]>, si16, ui8, none) -> (memref<1x64x54x160xsi32, strides: [8640, 8640, 160, 1]>, none), %R8.1792, %B866 = "sfu.taylor_4x"(%R6, %R10.3584, %D65) : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R3, %B867 = "arith.mul"(%R8.1792, %R3, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R6, %B868 = "arith.add"(%R3, %C1.0, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, f32, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R3, %B869 = "arith.div"(%C1.0, %R6, %D65) {iter = 3} : (f32, memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R3, %B870 = "arith.mul"(%R3, %R0, %D65) {round_mode = 0} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none) -> (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R12.2432, %D66 = "dma.tensor"(%G192512, %B854) {decompress = False} : (memref<1x128x64x9xf32, strides: [73728, 576, 9, 1]>, none) -> (memref<1x128x64x9xf32, strides: [1152, 576, 9, 1]>, none), %R15.3840, %D67 = "dma.tensor"(%G487424, %B854) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R7, %B871 = "conv.normal"(%R3, %R12.2432, %R15.3840, %C0.0, %D67) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], res_add = False} : (memref<1x64x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %D68 = "dma.tensor"(%G47120896, %B870) {decompress = False} : (memref<1x32x57x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x57x160xf32, strides: [9120, 9120, 160, 1]>, none), %R6.1792, %B872 = "tsbc.s_bc"(%S1024, %D68) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.896, %B873 = "arith.sub"(%C0.0, %R7, %D68) {round_mode = 0} : (f32, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B874 = "arith.max"(%R5.896, %C-3.4028198694267105e+35, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B875 = "arith.mul"(%R5.896, %C1.4426950216293335, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B876 = "arith.cast"(%R4, %D68) {round_mode = 3} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B877 = "arith.mul"(%R2.3712, %C0.6931471824645996, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B878 = "arith.sub"(%R5.896, %R4, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B879 = "arith.cast"(%R2.3712, %D68) {round_mode = 1} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R2.3712, %B880 = "arith.min"(%R5.896, %C127, %D68) {round_mode = 0} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R5.896, %B881 = "arith.max"(%R2.3712, %C-127, %D68) {round_mode = 0} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R2.3712, %B882 = "arith.adds"(%R5.896, %C127, %C23, %D68) {round_mode = 1} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x128x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B883 = "sfu.taylor_4x"(%R4, %R6.1792, %D68) : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B884 = "arith.mul"(%R5.896, %R2.3712, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B885 = "arith.add"(%R2.3712, %C1.0, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B886 = "arith.div"(%C1.0, %R4, %D68) {iter = 3} : (f32, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B887 = "arith.mul"(%R2.3712, %R7, %D68) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R12.7040, %D69 = "dma.tensor"(%G491520, %B871) {decompress = False} : (memref<1x64x128x1xf32, strides: [8192, 128, 1, 1]>, none) -> (memref<1x64x128x1xf32, strides: [128, 128, 1, 1]>, none), %R15.3904, %D70 = "dma.tensor"(%G524288, %B871) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R4, %B888 = "conv.normal"(%R2.3712, %R12.7040, %R15.3904, %C0.0, %D70) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G35667968, %D71 = "dma.tensor"(%R2.3712, %B887) {decompress = False} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [819200, 6400, 80, 1]>, none), %R3.4608, %B889 = "tsbc.s_bc"(%S1024, %D71) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.12352, %B890 = "arith.sub"(%C0.0, %R4, %D71) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B891 = "arith.max"(%R2.12352, %C-3.4028198694267105e+35, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B892 = "arith.mul"(%R2.12352, %C1.4426950216293335, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B893 = "arith.cast"(%R2.3712, %D71) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B894 = "arith.mul"(%R7.2432, %C0.6931471824645996, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B895 = "arith.sub"(%R2.12352, %R2.3712, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B896 = "arith.cast"(%R7.2432, %D71) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R7.2432, %B897 = "arith.min"(%R2.12352, %C127, %D71) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R2.12352, %B898 = "arith.max"(%R7.2432, %C-127, %D71) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R7.2432, %B899 = "arith.adds"(%R2.12352, %C127, %C23, %D71) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B900 = "sfu.taylor_4x"(%R2.3712, %R3.4608, %D71) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B901 = "arith.mul"(%R2.12352, %R7.2432, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B902 = "arith.add"(%R7.2432, %C1.0, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B903 = "arith.div"(%C1.0, %R2.3712, %D71) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B904 = "arith.mul"(%R7.2432, %R4, %D71) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B905 = "conv.normal"(%R0, %R15.2432, %R12.7872, %C0.0, %D71) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], res_add = False} : (memref<1x32x57x160xf32, strides: [9120, 9120, 160, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %G38944768, %D72 = "dma.tensor"(%R7.2432, %B904) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R5, %D73 = "dma.tensor"(%G29148288, %B904) {decompress = False} : (memref<1x64x55x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R4.4864, %B906 = "tsbc.s_bc"(%S1024, %D73) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.2432, %B907 = "arith.sub"(%C0.0, %R10, %D73) {round_mode = 0} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B908 = "arith.max"(%R2.2432, %C-3.4028198694267105e+35, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B909 = "arith.mul"(%R2.2432, %C1.4426950216293335, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B910 = "arith.cast"(%R0, %D73) {round_mode = 3} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B911 = "arith.mul"(%R13, %C0.6931471824645996, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B912 = "arith.sub"(%R2.2432, %R0, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B913 = "arith.cast"(%R13, %D73) {round_mode = 1} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B914 = "arith.min"(%R2.2432, %C127, %D73) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B915 = "arith.max"(%R13, %C-127, %D73) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R13, %B916 = "arith.adds"(%R2.2432, %C127, %C23, %D73) {round_mode = 1} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x32x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B917 = "sfu.taylor_4x"(%R0, %R4.4864, %D73) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B918 = "arith.mul"(%R2.2432, %R13, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B919 = "arith.add"(%R13, %C1.0, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B920 = "arith.div"(%C1.0, %R0, %D73) {iter = 3} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R13, %B921 = "arith.mul"(%R13, %R10, %D73) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R7.2432, %D74 = "dma.tensor"(%G50398336, %B905) {decompress = False} : (memref<1x32x55x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B922 = "arith.add"(%R7.2432, %R13, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [0, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [0, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R2.2432, %B923 = "conv.normal"(%R5, %R12.7552, %R12.7808, %C0.0, %D74) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R9.4864, %B924 = "tsbc.s_bc"(%S1024, %D74) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.2432, %B925 = "arith.sub"(%C0.0, %R2.2432, %D74) {round_mode = 0} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R7.2432, %B926 = "arith.max"(%R7.2432, %C-3.4028198694267105e+35, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R5, %B927 = "arith.mul"(%R7.2432, %C1.4426950216293335, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B928 = "arith.cast"(%R5, %D74) {round_mode = 3} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R5, %B929 = "arith.mul"(%R10, %C0.6931471824645996, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R5, %B930 = "arith.sub"(%R7.2432, %R5, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R7.2432, %B931 = "arith.cast"(%R10, %D74) {round_mode = 1} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R10, %B932 = "arith.min"(%R7.2432, %C127, %D74) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R7.2432, %B933 = "arith.max"(%R10, %C-127, %D74) {round_mode = 0} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R10, %B934 = "arith.adds"(%R7.2432, %C127, %C23, %D74) {round_mode = 1} : (memref<1x32x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x32x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R7.2432, %B935 = "sfu.taylor_4x"(%R5, %R9.4864, %D74) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B936 = "arith.mul"(%R7.2432, %R10, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R5, %B937 = "arith.add"(%R10, %C1.0, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B938 = "arith.div"(%C1.0, %R5, %D74) {iter = 3} : (f32, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10, %B939 = "arith.mul"(%R10, %R2.2432, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3, %B940 = "arith.copy"(%R0, %D74) {round_mode = 0} : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3.L32, %B941 = "tsbc.l_copy"(%R10, %D74) : (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x32x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R0, %B942 = "conv.normal"(%R3, %R15.3584, %R12.7936, %C0.0, %D74) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R10.4864, %B943 = "tsbc.s_bc"(%S1024, %D74) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.2432, %B944 = "arith.sub"(%C0.0, %R0, %D74) {round_mode = 0} : (f32, memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R8.2432, %B945 = "arith.max"(%R8.2432, %C-3.4028198694267105e+35, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R6, %B946 = "arith.mul"(%R8.2432, %C1.4426950216293335, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3, %B947 = "arith.cast"(%R6, %D74) {round_mode = 3} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R6, %B948 = "arith.mul"(%R3, %C0.6931471824645996, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R6, %B949 = "arith.sub"(%R8.2432, %R6, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R8.2432, %B950 = "arith.cast"(%R3, %D74) {round_mode = 1} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R3, %B951 = "arith.min"(%R8.2432, %C127, %D74) {round_mode = 0} : (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R8.2432, %B952 = "arith.max"(%R3, %C-127, %D74) {round_mode = 0} : (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, none) -> (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, none), %R3, %B953 = "arith.adds"(%R8.2432, %C127, %C23, %D74) {round_mode = 1} : (memref<1x64x55x160xsi16, strides: [8800, 8800, 160, 1]>, si16, ui8, none) -> (memref<1x64x55x160xsi32, strides: [8800, 8800, 160, 1]>, none), %R8.2432, %B954 = "sfu.taylor_4x"(%R6, %R10.4864, %D74) : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3, %B955 = "arith.mul"(%R8.2432, %R3, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R6, %B956 = "arith.add"(%R3, %C1.0, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, f32, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3, %B957 = "arith.div"(%C1.0, %R6, %D74) {iter = 3} : (f32, memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R3, %B958 = "arith.mul"(%R3, %R0, %D74) {round_mode = 0} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none) -> (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, none), %R7, %B959 = "conv.normal"(%R3, %R12.2432, %R15.3840, %C0.0, %D74) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], res_add = False} : (memref<1x64x55x160xf32, strides: [8800, 8800, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %D75 = "dma.tensor"(%G47155456, %B958) {decompress = False} : (memref<1x32x54x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, none), %R6.1792, %B960 = "tsbc.s_bc"(%S1024, %D75) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.896, %B961 = "arith.sub"(%C0.0, %R7, %D75) {round_mode = 0} : (f32, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B962 = "arith.max"(%R5.896, %C-3.4028198694267105e+35, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B963 = "arith.mul"(%R5.896, %C1.4426950216293335, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B964 = "arith.cast"(%R4, %D75) {round_mode = 3} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B965 = "arith.mul"(%R2.3712, %C0.6931471824645996, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B966 = "arith.sub"(%R5.896, %R4, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B967 = "arith.cast"(%R2.3712, %D75) {round_mode = 1} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R2.3712, %B968 = "arith.min"(%R5.896, %C127, %D75) {round_mode = 0} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R5.896, %B969 = "arith.max"(%R2.3712, %C-127, %D75) {round_mode = 0} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, none) -> (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, none), %R2.3712, %B970 = "arith.adds"(%R5.896, %C127, %C23, %D75) {round_mode = 1} : (memref<1x128x27x80xsi16, strides: [4352, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x128x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B971 = "sfu.taylor_4x"(%R4, %R6.1792, %D75) : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B972 = "arith.mul"(%R5.896, %R2.3712, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B973 = "arith.add"(%R2.3712, %C1.0, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B974 = "arith.div"(%C1.0, %R4, %D75) {iter = 3} : (f32, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R2.3712, %B975 = "arith.mul"(%R2.3712, %R7, %D75) {round_mode = 0} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R4, %B976 = "conv.normal"(%R2.3712, %R12.7040, %R15.3904, %C0.0, %D75) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %G35676608, %D76 = "dma.tensor"(%R2.3712, %B975) {decompress = False} : (memref<1x128x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x128x27x80xf32, strides: [819200, 6400, 80, 1]>, none), %R3.4608, %B977 = "tsbc.s_bc"(%S1024, %D76) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.12352, %B978 = "arith.sub"(%C0.0, %R4, %D76) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B979 = "arith.max"(%R2.12352, %C-3.4028198694267105e+35, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B980 = "arith.mul"(%R2.12352, %C1.4426950216293335, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B981 = "arith.cast"(%R2.3712, %D76) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B982 = "arith.mul"(%R7.2432, %C0.6931471824645996, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B983 = "arith.sub"(%R2.12352, %R2.3712, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B984 = "arith.cast"(%R7.2432, %D76) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R7.2432, %B985 = "arith.min"(%R2.12352, %C127, %D76) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R2.12352, %B986 = "arith.max"(%R7.2432, %C-127, %D76) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, none), %R7.2432, %B987 = "arith.adds"(%R2.12352, %C127, %C23, %D76) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [2176, 2176, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [2160, 2160, 80, 1]>, none), %R2.12352, %B988 = "sfu.taylor_4x"(%R2.3712, %R3.4608, %D76) : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B989 = "arith.mul"(%R2.12352, %R7.2432, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R2.3712, %B990 = "arith.add"(%R7.2432, %C1.0, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B991 = "arith.div"(%C1.0, %R2.3712, %D76) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R7.2432, %B992 = "arith.mul"(%R7.2432, %R4, %D76) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none), %R10, %B993 = "conv.normal"(%R0, %R15.2432, %R12.7872, %C0.0, %D76) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], res_add = False} : (memref<1x32x54x160xf32, strides: [8640, 8640, 160, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %G38953408, %D77 = "dma.tensor"(%R7.2432, %B992) {decompress = False} : (memref<1x64x27x80xf32, strides: [2160, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R5, %D78 = "dma.tensor"(%G29182848, %B992) {decompress = False} : (memref<1x64x53x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R4.2304, %B994 = "tsbc.s_bc"(%S1024, %D78) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.1152, %B995 = "arith.sub"(%C0.0, %R10, %D78) {round_mode = 0} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B996 = "arith.max"(%R2.1152, %C-3.4028198694267105e+35, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B997 = "arith.mul"(%R2.1152, %C1.4426950216293335, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B998 = "arith.cast"(%R0, %D78) {round_mode = 3} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B999 = "arith.mul"(%R13, %C0.6931471824645996, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B1000 = "arith.sub"(%R2.1152, %R0, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B1001 = "arith.cast"(%R13, %D78) {round_mode = 1} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B1002 = "arith.min"(%R2.1152, %C127, %D78) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B1003 = "arith.max"(%R13, %C-127, %D78) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R13, %B1004 = "arith.adds"(%R2.1152, %C127, %C23, %D78) {round_mode = 1} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, ui8, none) -> (memref<1x32x53x160xsi32, strides: [8480, 8480, 160, 1]>, none), %R2.1152, %B1005 = "sfu.taylor_4x"(%R0, %R4.2304, %D78) : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B1006 = "arith.mul"(%R2.1152, %R13, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B1007 = "arith.add"(%R13, %C1.0, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B1008 = "arith.div"(%C1.0, %R0, %D78) {iter = 3} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R13, %B1009 = "arith.mul"(%R13, %R10, %D78) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R7.2432, %D79 = "dma.tensor"(%G50432896, %B993) {decompress = False} : (memref<1x32x53x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B1010 = "arith.add"(%R7.2432, %R13, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [0, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [0, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R2.2432, %B1011 = "conv.normal"(%R5, %R12.7552, %R12.7808, %C0.0, %D79) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R9.2304, %B1012 = "tsbc.s_bc"(%S1024, %D79) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.1152, %B1013 = "arith.sub"(%C0.0, %R2.2432, %D79) {round_mode = 0} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R7.1152, %B1014 = "arith.max"(%R7.1152, %C-3.4028198694267105e+35, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R5, %B1015 = "arith.mul"(%R7.1152, %C1.4426950216293335, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B1016 = "arith.cast"(%R5, %D79) {round_mode = 3} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R5, %B1017 = "arith.mul"(%R10, %C0.6931471824645996, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R5, %B1018 = "arith.sub"(%R7.1152, %R5, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R7.1152, %B1019 = "arith.cast"(%R10, %D79) {round_mode = 1} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R10, %B1020 = "arith.min"(%R7.1152, %C127, %D79) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R7.1152, %B1021 = "arith.max"(%R10, %C-127, %D79) {round_mode = 0} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R10, %B1022 = "arith.adds"(%R7.1152, %C127, %C23, %D79) {round_mode = 1} : (memref<1x32x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, ui8, none) -> (memref<1x32x53x160xsi32, strides: [8480, 8480, 160, 1]>, none), %R7.1152, %B1023 = "sfu.taylor_4x"(%R5, %R9.2304, %D79) : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B1024 = "arith.mul"(%R7.1152, %R10, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R5, %B1025 = "arith.add"(%R10, %C1.0, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B1026 = "arith.div"(%C1.0, %R5, %D79) {iter = 3} : (f32, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10, %B1027 = "arith.mul"(%R10, %R2.2432, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3, %B1028 = "arith.copy"(%R0, %D79) {round_mode = 0} : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3.L32, %B1029 = "tsbc.l_copy"(%R10, %D79) : (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x32x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R0, %B1030 = "conv.normal"(%R3, %R15.3584, %R12.7936, %C0.0, %D79) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R10.2304, %B1031 = "tsbc.s_bc"(%S1024, %D79) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.1152, %B1032 = "arith.sub"(%C0.0, %R0, %D79) {round_mode = 0} : (f32, memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R8.1152, %B1033 = "arith.max"(%R8.1152, %C-3.4028198694267105e+35, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R6, %B1034 = "arith.mul"(%R8.1152, %C1.4426950216293335, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3, %B1035 = "arith.cast"(%R6, %D79) {round_mode = 3} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R6, %B1036 = "arith.mul"(%R3, %C0.6931471824645996, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R6, %B1037 = "arith.sub"(%R8.1152, %R6, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R8.1152, %B1038 = "arith.cast"(%R3, %D79) {round_mode = 1} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R3, %B1039 = "arith.min"(%R8.1152, %C127, %D79) {round_mode = 0} : (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R8.1152, %B1040 = "arith.max"(%R3, %C-127, %D79) {round_mode = 0} : (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, none) -> (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, none), %R3, %B1041 = "arith.adds"(%R8.1152, %C127, %C23, %D79) {round_mode = 1} : (memref<1x64x53x160xsi16, strides: [8480, 8480, 160, 1]>, si16, ui8, none) -> (memref<1x64x53x160xsi32, strides: [8480, 8480, 160, 1]>, none), %R8.1152, %B1042 = "sfu.taylor_4x"(%R6, %R10.2304, %D79) : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3, %B1043 = "arith.mul"(%R8.1152, %R3, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R6, %B1044 = "arith.add"(%R3, %C1.0, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, f32, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3, %B1045 = "arith.div"(%C1.0, %R6, %D79) {iter = 3} : (f32, memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R3, %B1046 = "arith.mul"(%R3, %R0, %D79) {round_mode = 0} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none) -> (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, none), %R7, %B1047 = "conv.normal"(%R3, %R12.2432, %R15.3840, %C0.0, %D79) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], res_add = False} : (memref<1x64x53x160xf32, strides: [8480, 8480, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R6.512, %B1048 = "tsbc.s_bc"(%S1024, %D79) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.256, %B1049 = "arith.sub"(%C0.0, %R7, %D79) {round_mode = 0} : (f32, memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1050 = "arith.max"(%R5.256, %C-3.4028198694267105e+35, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R4, %B1051 = "arith.mul"(%R5.256, %C1.4426950216293335, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1052 = "arith.cast"(%R4, %D79) {round_mode = 3} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R4, %B1053 = "arith.mul"(%R2.3712, %C0.6931471824645996, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R4, %B1054 = "arith.sub"(%R5.256, %R4, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1055 = "arith.cast"(%R2.3712, %D79) {round_mode = 1} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1056 = "arith.min"(%R5.256, %C127, %D79) {round_mode = 0} : (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1057 = "arith.max"(%R2.3712, %C-127, %D79) {round_mode = 0} : (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1058 = "arith.adds"(%R5.256, %C127, %C23, %D79) {round_mode = 1} : (memref<1x128x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x128x26x80xsi32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1059 = "sfu.taylor_4x"(%R4, %R6.512, %D79) : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1060 = "arith.mul"(%R5.256, %R2.3712, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R4, %B1061 = "arith.add"(%R2.3712, %C1.0, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1062 = "arith.div"(%C1.0, %R4, %D79) {iter = 3} : (f32, memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R2.3712, %B1063 = "arith.mul"(%R2.3712, %R7, %D79) {round_mode = 0} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R4, %B1064 = "conv.normal"(%R2.3712, %R12.7040, %R15.3904, %C0.0, %D79) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %G35685248, %D80 = "dma.tensor"(%R2.3712, %B1063) {decompress = False} : (memref<1x128x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x128x26x80xf32, strides: [819200, 6400, 80, 1]>, none), %R3.3968, %B1065 = "tsbc.s_bc"(%S1024, %D80) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.12032, %B1066 = "arith.sub"(%C0.0, %R4, %D80) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.12032, %B1067 = "arith.max"(%R2.12032, %C-3.4028198694267105e+35, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.3712, %B1068 = "arith.mul"(%R2.12032, %C1.4426950216293335, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1069 = "arith.cast"(%R2.3712, %D80) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.3712, %B1070 = "arith.mul"(%R7.2432, %C0.6931471824645996, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.3712, %B1071 = "arith.sub"(%R2.12032, %R2.3712, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.12032, %B1072 = "arith.cast"(%R7.2432, %D80) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1073 = "arith.min"(%R2.12032, %C127, %D80) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R2.12032, %B1074 = "arith.max"(%R7.2432, %C-127, %D80) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1075 = "arith.adds"(%R2.12032, %C127, %C23, %D80) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [2080, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [2080, 2080, 80, 1]>, none), %R2.12032, %B1076 = "sfu.taylor_4x"(%R2.3712, %R3.3968, %D80) : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1077 = "arith.mul"(%R2.12032, %R7.2432, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R2.3712, %B1078 = "arith.add"(%R7.2432, %C1.0, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1079 = "arith.div"(%C1.0, %R2.3712, %D80) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R7.2432, %B1080 = "arith.mul"(%R7.2432, %R4, %D80) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none), %R10, %D81 = "dma.tensor"(%G544768, %B1064) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %G38962048, %D82 = "dma.tensor"(%R7.2432, %B1080) {decompress = False} : (memref<1x64x26x80xf32, strides: [2080, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [409600, 6400, 80, 1]>, none), %R0, %D83 = "dma.tensor"(%G38944768, %B1080) {decompress = False} : (memref<1x64x80x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3, %D84 = "dma.tensor"(%G528384, %B1080) {decompress = False} : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [64, 64, 1, 1]>, none), %R4, %B1081 = "conv.normal"(%R0, %R3, %R10, %C0.0, %D84) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R9.2048, %B1082 = "tsbc.s_bc"(%S1024, %D84) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.9216, %B1083 = "arith.sub"(%C0.0, %R4, %D84) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1084 = "arith.max"(%R7.9216, %C-3.4028198694267105e+35, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1085 = "arith.mul"(%R7.9216, %C1.4426950216293335, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1086 = "arith.cast"(%R6, %D84) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1087 = "arith.mul"(%R1.9216, %C0.6931471824645996, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1088 = "arith.sub"(%R7.9216, %R6, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1089 = "arith.cast"(%R1.9216, %D84) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1090 = "arith.min"(%R7.9216, %C127, %D84) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1091 = "arith.max"(%R1.9216, %C-127, %D84) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1092 = "arith.adds"(%R7.9216, %C127, %C23, %D84) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1093 = "sfu.taylor_4x"(%R6, %R9.2048, %D84) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1094 = "arith.mul"(%R7.9216, %R1.9216, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1095 = "arith.add"(%R1.9216, %C1.0, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1096 = "arith.div"(%C1.0, %R6, %D84) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1097 = "arith.mul"(%R1.9216, %R4, %D84) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R10, %D85 = "dma.tensor"(%G548864, %B1081) {decompress = False} : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [576, 576, 9, 1]>, none), %R11, %D86 = "dma.tensor"(%G696320, %B1081) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R5, %B1098 = "conv.normal"(%R1.9216, %R10, %R11, %C0.0, %D86) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4.11264, %B1099 = "tsbc.s_bc"(%S1024, %D86) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.2048, %B1100 = "arith.sub"(%C0.0, %R5, %D86) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1101 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1102 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1103 = "arith.cast"(%R1.9216, %D86) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1104 = "arith.mul"(%R7, %C0.6931471824645996, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1105 = "arith.sub"(%R3.2048, %R1.9216, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1106 = "arith.cast"(%R7, %D86) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7, %B1107 = "arith.min"(%R3.2048, %C127, %D86) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1108 = "arith.max"(%R7, %C-127, %D86) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7, %B1109 = "arith.adds"(%R3.2048, %C127, %C23, %D86) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1110 = "sfu.taylor_4x"(%R1.9216, %R4.11264, %D86) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1111 = "arith.mul"(%R3.2048, %R7, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1112 = "arith.add"(%R7, %C1.0, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1113 = "arith.div"(%C1.0, %R1.9216, %D86) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1114 = "arith.mul"(%R7, %R5, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R10.2176, %B1115 = "arith.add"(%R0, %R7, %D86) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [0, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [0, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %D87 = "dma.tensor"(%G700416, %B1114) {decompress = False} : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [64, 64, 1, 1]>, none), %R3, %D88 = "dma.tensor"(%G716800, %B1114) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R0, %B1116 = "conv.normal"(%R10.2176, %R2, %R3, %C0.0, %D88) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5.2048, %B1117 = "tsbc.s_bc"(%S1024, %D88) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.9216, %B1118 = "arith.sub"(%C0.0, %R0, %D88) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1119 = "arith.max"(%R3.9216, %C-3.4028198694267105e+35, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1120 = "arith.mul"(%R3.9216, %C1.4426950216293335, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1121 = "arith.cast"(%R2, %D88) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1122 = "arith.mul"(%R7, %C0.6931471824645996, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1123 = "arith.sub"(%R3.9216, %R2, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1124 = "arith.cast"(%R7, %D88) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7, %B1125 = "arith.min"(%R3.9216, %C127, %D88) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1126 = "arith.max"(%R7, %C-127, %D88) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7, %B1127 = "arith.adds"(%R3.9216, %C127, %C23, %D88) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1128 = "sfu.taylor_4x"(%R2, %R5.2048, %D88) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1129 = "arith.mul"(%R3.9216, %R7, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1130 = "arith.add"(%R7, %C1.0, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1131 = "arith.div"(%C1.0, %R2, %D88) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1132 = "arith.mul"(%R7, %R0, %D88) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R9, %D89 = "dma.tensor"(%G720896, %B1116) {decompress = False} : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [576, 576, 9, 1]>, none), %R10, %D90 = "dma.tensor"(%G868352, %B1116) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R5, %B1133 = "conv.normal"(%R7, %R9, %R10, %C0.0, %D90) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %D91 = "dma.tensor"(%G35667968, %B1132) {decompress = False} : (memref<1x128x80x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R10.2048, %B1134 = "tsbc.s_bc"(%S1024, %D91) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.9216, %B1135 = "arith.sub"(%C0.0, %R5, %D91) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8.9216, %B1136 = "arith.max"(%R8.9216, %C-3.4028198694267105e+35, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1137 = "arith.mul"(%R8.9216, %C1.4426950216293335, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1138 = "arith.cast"(%R7, %D91) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1139 = "arith.mul"(%R3.2048, %C0.6931471824645996, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1140 = "arith.sub"(%R8.9216, %R7, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8.9216, %B1141 = "arith.cast"(%R3.2048, %D91) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1142 = "arith.min"(%R8.9216, %C127, %D91) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R8.9216, %B1143 = "arith.max"(%R3.2048, %C-127, %D91) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1144 = "arith.adds"(%R8.9216, %C127, %C23, %D91) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R8.9216, %B1145 = "sfu.taylor_4x"(%R7, %R10.2048, %D91) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1146 = "arith.mul"(%R8.9216, %R3.2048, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7, %B1147 = "arith.add"(%R3.2048, %C1.0, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1148 = "arith.div"(%C1.0, %R7, %D91) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1149 = "arith.mul"(%R3.2048, %R5, %D91) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R11.11392, %D92 = "dma.tensor"(%G905216, %B1133) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R5.9216, %B1150 = "arith.add"(%R10.2176, %R3.2048, %D92) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [0, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [0, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %D93 = "dma.tensor"(%G872448, %B1149) {decompress = False} : (memref<1x64x128x1xf32, strides: [8192, 128, 1, 1]>, none) -> (memref<1x64x128x1xf32, strides: [128, 128, 1, 1]>, none), %R4, %B1151 = "conv.normal"(%R0, %R8, %R11.11392, %C0.0, %D93) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.2048, %B1152 = "tsbc.s_bc"(%S1024, %D93) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R1.9216, %B1153 = "arith.sub"(%C0.0, %R4, %D93) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1154 = "arith.max"(%R1.9216, %C-3.4028198694267105e+35, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1155 = "arith.mul"(%R1.9216, %C1.4426950216293335, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1156 = "arith.cast"(%R0, %D93) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1157 = "arith.mul"(%R8, %C0.6931471824645996, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1158 = "arith.sub"(%R1.9216, %R0, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1159 = "arith.cast"(%R8, %D93) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R8, %B1160 = "arith.min"(%R1.9216, %C127, %D93) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1161 = "arith.max"(%R8, %C-127, %D93) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R8, %B1162 = "arith.adds"(%R1.9216, %C127, %C23, %D93) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R1.9216, %B1163 = "sfu.taylor_4x"(%R0, %R3.2048, %D93) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1164 = "arith.mul"(%R1.9216, %R8, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1165 = "arith.add"(%R8, %C1.0, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1166 = "arith.div"(%C1.0, %R0, %D93) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1167 = "arith.mul"(%R8, %R4, %D93) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R11, %D94 = "dma.tensor"(%G974848, %B1151) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10, %D95 = "dma.tensor"(%G909312, %B1151) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R0, %B1168 = "arith.copy"(%R5.9216, %D95) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R1.9216, %B1169 = "arith.copy"(%R8, %D95) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R4, %B1170 = "conv.normal"(%R0, %R10, %R11, %C0.0, %D95) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R15.6144, %B1171 = "tsbc.s_bc"(%S1024, %D95) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R12.4096, %B1172 = "arith.sub"(%C0.0, %R4, %D95) {round_mode = 0} : (f32, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R12.4096, %B1173 = "arith.max"(%R12.4096, %C-3.4028198694267105e+35, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.2048, %B1174 = "arith.mul"(%R12.4096, %C1.4426950216293335, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R0, %B1175 = "arith.cast"(%R9.2048, %D95) {round_mode = 3} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.2048, %B1176 = "arith.mul"(%R0, %C0.6931471824645996, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.2048, %B1177 = "arith.sub"(%R12.4096, %R9.2048, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R12.4096, %B1178 = "arith.cast"(%R0, %D95) {round_mode = 1} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R0, %B1179 = "arith.min"(%R12.4096, %C127, %D95) {round_mode = 0} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R12.4096, %B1180 = "arith.max"(%R0, %C-127, %D95) {round_mode = 0} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R0, %B1181 = "arith.adds"(%R12.4096, %C127, %C23, %D95) {round_mode = 1} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x128x80x80xsi32, strides: [12800, 6400, 80, 1]>, none), %R12.4096, %B1182 = "sfu.taylor_4x"(%R9.2048, %R15.6144, %D95) : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R0, %B1183 = "arith.mul"(%R12.4096, %R0, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.2048, %B1184 = "arith.add"(%R0, %C1.0, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R0, %B1185 = "arith.div"(%C1.0, %R9.2048, %D95) {iter = 3} : (f32, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R0, %B1186 = "arith.mul"(%R0, %R4, %D95) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R8, %D96 = "dma.tensor"(%G978944, %B1170) {decompress = False} : (memref<1x256x128x9xf32, strides: [294912, 1152, 9, 1]>, none) -> (memref<1x256x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R3.2048, %D97 = "dma.tensor"(%G2158592, %B1170) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R5, %B1187 = "conv.normal"(%R0, %R8, %R3.2048, %C0.0, %D97) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<256x128x3x3xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R11.2048, %B1188 = "tsbc.s_bc"(%S1024, %D97) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.9216, %B1189 = "arith.sub"(%C0.0, %R5, %D97) {round_mode = 0} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1190 = "arith.max"(%R9.9216, %C-3.4028198694267105e+35, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1191 = "arith.mul"(%R9.9216, %C1.4426950216293335, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1192 = "arith.cast"(%R8, %D97) {round_mode = 3} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1193 = "arith.mul"(%R3.2048, %C0.6931471824645996, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1194 = "arith.sub"(%R9.9216, %R8, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1195 = "arith.cast"(%R3.2048, %D97) {round_mode = 1} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1196 = "arith.min"(%R9.9216, %C127, %D97) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1197 = "arith.max"(%R3.2048, %C-127, %D97) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1198 = "arith.adds"(%R9.9216, %C127, %C23, %D97) {round_mode = 1} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x256x40x40xsi32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1199 = "sfu.taylor_4x"(%R8, %R11.2048, %D97) : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1200 = "arith.mul"(%R9.9216, %R3.2048, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1201 = "arith.add"(%R3.2048, %C1.0, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1202 = "arith.div"(%C1.0, %R8, %D97) {iter = 3} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1203 = "arith.mul"(%R3.2048, %R5, %D97) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R7, %D98 = "dma.tensor"(%G2162688, %B1187) {decompress = False} : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [512, 256, 1, 1]>, none), %R12, %D99 = "dma.tensor"(%G2293760, %B1187) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R6, %B1204 = "conv.normal"(%R3.2048, %R7, %R12, %C0.0, %D99) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8.9216, %B1205 = "tsbc.s_bc"(%S1024, %D99) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.12800, %B1206 = "arith.sub"(%C0.0, %R6, %D99) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7.12800, %B1207 = "arith.max"(%R7.12800, %C-3.4028198694267105e+35, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1208 = "arith.mul"(%R7.12800, %C1.4426950216293335, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1209 = "arith.cast"(%R7, %D99) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1210 = "arith.mul"(%R4.11264, %C0.6931471824645996, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1211 = "arith.sub"(%R7.12800, %R7, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7.12800, %B1212 = "arith.cast"(%R4.11264, %D99) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1213 = "arith.min"(%R7.12800, %C127, %D99) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7.12800, %B1214 = "arith.max"(%R4.11264, %C-127, %D99) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1215 = "arith.adds"(%R7.12800, %C127, %C23, %D99) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R7.12800, %B1216 = "sfu.taylor_4x"(%R7, %R8.9216, %D99) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1217 = "arith.mul"(%R7.12800, %R4.11264, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1218 = "arith.add"(%R4.11264, %C1.0, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1219 = "arith.div"(%C1.0, %R7, %D99) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1220 = "arith.mul"(%R4.11264, %R6, %D99) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %D100 = "dma.tensor"(%G2297856, %B1204) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R10, %D101 = "dma.tensor"(%G2363392, %B1204) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R7, %B1221 = "conv.normal"(%R4.11264, %R9, %R10, %C0.0, %D101) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R11, %D102 = "dma.tensor"(%G2957312, %B1220) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R9.9216, %B1222 = "tsbc.s_bc"(%S1024, %D102) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.12800, %B1223 = "arith.sub"(%C0.0, %R7, %D102) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1224 = "arith.max"(%R8.12800, %C-3.4028198694267105e+35, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1225 = "arith.mul"(%R8.12800, %C1.4426950216293335, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1226 = "arith.cast"(%R8, %D102) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1227 = "arith.mul"(%R5.7680, %C0.6931471824645996, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1228 = "arith.sub"(%R8.12800, %R8, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1229 = "arith.cast"(%R5.7680, %D102) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1230 = "arith.min"(%R8.12800, %C127, %D102) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1231 = "arith.max"(%R5.7680, %C-127, %D102) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1232 = "arith.adds"(%R8.12800, %C127, %C23, %D102) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1233 = "sfu.taylor_4x"(%R8, %R9.9216, %D102) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1234 = "arith.mul"(%R8.12800, %R5.7680, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1235 = "arith.add"(%R5.7680, %C1.0, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1236 = "arith.div"(%C1.0, %R8, %D102) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1237 = "arith.mul"(%R5.7680, %R7, %D102) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %D103 = "dma.tensor"(%G2367488, %B1221) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R8, %B1238 = "conv.normal"(%R5.7680, %R10, %R11, %C0.0, %D103) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7.512, %B1239 = "tsbc.s_bc"(%S1024, %D103) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R6.4096, %B1240 = "arith.sub"(%C0.0, %R8, %D103) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1241 = "arith.max"(%R6.4096, %C-3.4028198694267105e+35, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1242 = "arith.mul"(%R6.4096, %C1.4426950216293335, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1243 = "arith.cast"(%R5.7680, %D103) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1244 = "arith.mul"(%R9, %C0.6931471824645996, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1245 = "arith.sub"(%R6.4096, %R5.7680, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1246 = "arith.cast"(%R9, %D103) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9, %B1247 = "arith.min"(%R6.4096, %C127, %D103) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1248 = "arith.max"(%R9, %C-127, %D103) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9, %B1249 = "arith.adds"(%R6.4096, %C127, %C23, %D103) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1250 = "sfu.taylor_4x"(%R5.7680, %R7.512, %D103) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1251 = "arith.mul"(%R6.4096, %R9, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1252 = "arith.add"(%R9, %C1.0, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1253 = "arith.div"(%C1.0, %R5.7680, %D103) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1254 = "arith.mul"(%R9, %R8, %D103) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R11, %D104 = "dma.tensor"(%G3026944, %B1238) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10, %D105 = "dma.tensor"(%G2961408, %B1238) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R6, %B1255 = "arith.add"(%R4.11264, %R9, %D105) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1256 = "conv.normal"(%R6, %R10, %R11, %C0.0, %D105) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %D106 = "dma.tensor"(%G3620864, %B1255) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10.9280, %B1257 = "tsbc.s_bc"(%S1024, %D106) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.12864, %B1258 = "arith.sub"(%C0.0, %R4.11264, %D106) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.12864, %B1259 = "arith.max"(%R9.12864, %C-3.4028198694267105e+35, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.64, %B1260 = "arith.mul"(%R9.12864, %C1.4426950216293335, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1261 = "arith.cast"(%R9.64, %D106) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.64, %B1262 = "arith.mul"(%R6.12800, %C0.6931471824645996, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.64, %B1263 = "arith.sub"(%R9.12864, %R9.64, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.12864, %B1264 = "arith.cast"(%R6.12800, %D106) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1265 = "arith.min"(%R9.12864, %C127, %D106) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9.12864, %B1266 = "arith.max"(%R6.12800, %C-127, %D106) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1267 = "arith.adds"(%R9.12864, %C127, %C23, %D106) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R9.12864, %B1268 = "sfu.taylor_4x"(%R9.64, %R10.9280, %D106) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1269 = "arith.mul"(%R9.12864, %R6.12800, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.64, %B1270 = "arith.add"(%R6.12800, %C1.0, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1271 = "arith.div"(%C1.0, %R9.64, %D106) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.12800, %B1272 = "arith.mul"(%R6.12800, %R4.11264, %D106) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %D107 = "dma.tensor"(%G3031040, %B1256) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R4.11264, %B1273 = "conv.normal"(%R6.12800, %R8, %R9, %C0.0, %D107) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10.9216, %B1274 = "tsbc.s_bc"(%S1024, %D107) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.12800, %B1275 = "arith.sub"(%C0.0, %R4.11264, %D107) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.12800, %B1276 = "arith.max"(%R9.12800, %C-3.4028198694267105e+35, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1277 = "arith.mul"(%R9.12800, %C1.4426950216293335, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1278 = "arith.cast"(%R9, %D107) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1279 = "arith.mul"(%R7, %C0.6931471824645996, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1280 = "arith.sub"(%R9.12800, %R9, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.12800, %B1281 = "arith.cast"(%R7, %D107) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1282 = "arith.min"(%R9.12800, %C127, %D107) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9.12800, %B1283 = "arith.max"(%R7, %C-127, %D107) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1284 = "arith.adds"(%R9.12800, %C127, %C23, %D107) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R9.12800, %B1285 = "sfu.taylor_4x"(%R9, %R10.9216, %D107) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1286 = "arith.mul"(%R9.12800, %R7, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1287 = "arith.add"(%R7, %C1.0, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1288 = "arith.div"(%C1.0, %R9, %D107) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1289 = "arith.mul"(%R7, %R4.11264, %D107) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %D108 = "dma.tensor"(%G3690496, %B1273) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R6.12800, %D109 = "dma.tensor"(%G3624960, %B1273) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R4.11264, %B1290 = "arith.add"(%R6, %R7, %D109) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1291 = "conv.normal"(%R4.11264, %R6.12800, %R8, %C0.0, %D109) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.9344, %D110 = "dma.tensor"(%G4284416, %B1290) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R9.9216, %B1292 = "tsbc.s_bc"(%S1024, %D110) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.12800, %B1293 = "arith.sub"(%C0.0, %R7, %D110) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1294 = "arith.max"(%R8.12800, %C-3.4028198694267105e+35, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1295 = "arith.mul"(%R8.12800, %C1.4426950216293335, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1296 = "arith.cast"(%R8, %D110) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1297 = "arith.mul"(%R5.7680, %C0.6931471824645996, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1298 = "arith.sub"(%R8.12800, %R8, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1299 = "arith.cast"(%R5.7680, %D110) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1300 = "arith.min"(%R8.12800, %C127, %D110) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1301 = "arith.max"(%R5.7680, %C-127, %D110) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1302 = "arith.adds"(%R8.12800, %C127, %C23, %D110) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R8.12800, %B1303 = "sfu.taylor_4x"(%R8, %R9.9216, %D110) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1304 = "arith.mul"(%R8.12800, %R5.7680, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R8, %B1305 = "arith.add"(%R5.7680, %C1.0, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1306 = "arith.div"(%C1.0, %R8, %D110) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1307 = "arith.mul"(%R5.7680, %R7, %D110) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %D111 = "dma.tensor"(%G3694592, %B1291) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R8, %B1308 = "conv.normal"(%R5.7680, %R10, %R9.9344, %C0.0, %D111) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7.512, %B1309 = "tsbc.s_bc"(%S1024, %D111) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R6.4096, %B1310 = "arith.sub"(%C0.0, %R8, %D111) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1311 = "arith.max"(%R6.4096, %C-3.4028198694267105e+35, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1312 = "arith.mul"(%R6.4096, %C1.4426950216293335, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1313 = "arith.cast"(%R5.7680, %D111) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1314 = "arith.mul"(%R9, %C0.6931471824645996, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1315 = "arith.sub"(%R6.4096, %R5.7680, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1316 = "arith.cast"(%R9, %D111) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9, %B1317 = "arith.min"(%R6.4096, %C127, %D111) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1318 = "arith.max"(%R9, %C-127, %D111) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9, %B1319 = "arith.adds"(%R6.4096, %C127, %C23, %D111) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R6.4096, %B1320 = "sfu.taylor_4x"(%R5.7680, %R7.512, %D111) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1321 = "arith.mul"(%R6.4096, %R9, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.7680, %B1322 = "arith.add"(%R9, %C1.0, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1323 = "arith.div"(%C1.0, %R5.7680, %D111) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %B1324 = "arith.mul"(%R9, %R8, %D111) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R11, %D112 = "dma.tensor"(%G4419584, %B1308) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10, %D113 = "dma.tensor"(%G4288512, %B1308) {decompress = False} : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [512, 256, 1, 1]>, none), %R6, %B1325 = "arith.add"(%R4.11264, %R9, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [0, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1326 = "conv.normal"(%R3.2048, %R10, %R11, %C0.0, %D113) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.11264, %B1327 = "tsbc.s_bc"(%S1024, %D113) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.14848, %B1328 = "arith.sub"(%C0.0, %R5, %D113) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.14848, %B1329 = "arith.max"(%R3.14848, %C-3.4028198694267105e+35, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1330 = "arith.mul"(%R3.14848, %C1.4426950216293335, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1331 = "arith.cast"(%R3.2048, %D113) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1332 = "arith.mul"(%R7, %C0.6931471824645996, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1333 = "arith.sub"(%R3.14848, %R3.2048, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.14848, %B1334 = "arith.cast"(%R7, %D113) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1335 = "arith.min"(%R3.14848, %C127, %D113) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.14848, %B1336 = "arith.max"(%R7, %C-127, %D113) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1337 = "arith.adds"(%R3.14848, %C127, %C23, %D113) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R3.14848, %B1338 = "sfu.taylor_4x"(%R3.2048, %R4.11264, %D113) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1339 = "arith.mul"(%R3.14848, %R7, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1340 = "arith.add"(%R7, %C1.0, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1341 = "arith.div"(%C1.0, %R3.2048, %D113) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1342 = "arith.mul"(%R7, %R5, %D113) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R11, %D114 = "dma.tensor"(%G4685824, %B1326) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R10, %D115 = "dma.tensor"(%G4423680, %B1326) {decompress = False} : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R3.2048, %B1343 = "arith.copy"(%R6, %D115) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.14848, %B1344 = "arith.copy"(%R7, %D115) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R13, %B1345 = "conv.normal"(%R3.2048, %R10, %R11, %C0.0, %D115) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %D116 = "dma.tensor"(%G4689920, %B1344) {decompress = False} : (memref<1x512x256x9xf32, strides: [1179648, 2304, 9, 1]>, none) -> (memref<1x512x256x9xf32, strides: [18432, 2304, 9, 1]>, none), %R12.10240, %B1346 = "tsbc.s_bc"(%S1024, %D116) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.1024, %B1347 = "arith.sub"(%C0.0, %R13, %D116) {round_mode = 0} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R11.1024, %B1348 = "arith.max"(%R11.1024, %C-3.4028198694267105e+35, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.8192, %B1349 = "arith.mul"(%R11.1024, %C1.4426950216293335, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1350 = "arith.cast"(%R9.8192, %D116) {round_mode = 3} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.8192, %B1351 = "arith.mul"(%R3.2048, %C0.6931471824645996, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.8192, %B1352 = "arith.sub"(%R11.1024, %R9.8192, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R11.1024, %B1353 = "arith.cast"(%R3.2048, %D116) {round_mode = 1} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1354 = "arith.min"(%R11.1024, %C127, %D116) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R11.1024, %B1355 = "arith.max"(%R3.2048, %C-127, %D116) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1356 = "arith.adds"(%R11.1024, %C127, %C23, %D116) {round_mode = 1} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x256x40x40xsi32, strides: [6400, 1600, 40, 1]>, none), %R11.1024, %B1357 = "sfu.taylor_4x"(%R9.8192, %R12.10240, %D116) : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1358 = "arith.mul"(%R11.1024, %R3.2048, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.8192, %B1359 = "arith.add"(%R3.2048, %C1.0, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1360 = "arith.div"(%C1.0, %R9.8192, %D116) {iter = 3} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1361 = "arith.mul"(%R3.2048, %R13, %D116) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R15, %D117 = "dma.tensor"(%G9408512, %B1345) {decompress = False} : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10, %B1362 = "conv.normal"(%R3.2048, %R5, %R15, %C0.0, %D117) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<512x256x3x3xf32>, memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R11, %D118 = "dma.tensor"(%G9936896, %B1361) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R6.4096, %B1363 = "tsbc.s_bc"(%S1024, %D118) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.7680, %B1364 = "arith.sub"(%C0.0, %R10, %D118) {round_mode = 0} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1365 = "arith.max"(%R5.7680, %C-3.4028198694267105e+35, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1366 = "arith.mul"(%R5.7680, %C1.4426950216293335, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1367 = "arith.cast"(%R4.11264, %D118) {round_mode = 3} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1368 = "arith.mul"(%R9, %C0.6931471824645996, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1369 = "arith.sub"(%R5.7680, %R4.11264, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1370 = "arith.cast"(%R9, %D118) {round_mode = 1} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R9, %B1371 = "arith.min"(%R5.7680, %C127, %D118) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R5.7680, %B1372 = "arith.max"(%R9, %C-127, %D118) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R9, %B1373 = "arith.adds"(%R5.7680, %C127, %C23, %D118) {round_mode = 1} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, ui8, none) -> (memref<1x512x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1374 = "sfu.taylor_4x"(%R4.11264, %R6.4096, %D118) : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1375 = "arith.mul"(%R5.7680, %R9, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1376 = "arith.add"(%R9, %C1.0, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1377 = "arith.div"(%C1.0, %R4.11264, %D118) {iter = 3} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1378 = "arith.mul"(%R9, %R10, %D118) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7, %D119 = "dma.tensor"(%G9412608, %B1362) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R8, %B1379 = "conv.normal"(%R9, %R7, %R11, %C0.0, %D119) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R12, %D120 = "dma.tensor"(%G10203136, %B1378) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R4.11264, %D121 = "dma.tensor"(%G10207232, %B1378) {decompress = False} : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [9216, 2304, 9, 1]>, none), %R11.12800, %B1380 = "tsbc.s_bc"(%S1024, %D121) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R11.6400, %B1381 = "arith.sub"(%C0.0, %R8, %D121) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11.6400, %B1382 = "arith.max"(%R11.6400, %C-3.4028198694267105e+35, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %B1383 = "arith.mul"(%R11.6400, %C1.4426950216293335, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.15360, %B1384 = "arith.cast"(%R11, %D121) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %B1385 = "arith.mul"(%R6.15360, %C0.6931471824645996, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %B1386 = "arith.sub"(%R11.6400, %R11, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11.6400, %B1387 = "arith.cast"(%R6.15360, %D121) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6.15360, %B1388 = "arith.min"(%R11.6400, %C127, %D121) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R11.6400, %B1389 = "arith.max"(%R6.15360, %C-127, %D121) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6.15360, %B1390 = "arith.adds"(%R11.6400, %C127, %C23, %D121) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R11.6400, %B1391 = "sfu.taylor_4x"(%R11, %R11.12800, %D121) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.15360, %B1392 = "arith.mul"(%R11.6400, %R6.15360, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %B1393 = "arith.add"(%R6.15360, %C1.0, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.15360, %B1394 = "arith.div"(%C1.0, %R11, %D121) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.15360, %B1395 = "arith.mul"(%R6.15360, %R8, %D121) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.12800, %D122 = "dma.tensor"(%G9940992, %B1379) {decompress = False} : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R8, %B1396 = "conv.normal"(%R6.15360, %R9.12800, %R12, %C0.0, %D122) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R10.9216, %B1397 = "tsbc.s_bc"(%S1024, %D122) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R10.2816, %B1398 = "arith.sub"(%C0.0, %R8, %D122) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R10.2816, %B1399 = "arith.max"(%R10.2816, %C-3.4028198694267105e+35, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.12800, %B1400 = "arith.mul"(%R10.2816, %C1.4426950216293335, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1401 = "arith.cast"(%R9.12800, %D122) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.12800, %B1402 = "arith.mul"(%R7.5376, %C0.6931471824645996, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.12800, %B1403 = "arith.sub"(%R10.2816, %R9.12800, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R10.2816, %B1404 = "arith.cast"(%R7.5376, %D122) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R7.5376, %B1405 = "arith.min"(%R10.2816, %C127, %D122) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R10.2816, %B1406 = "arith.max"(%R7.5376, %C-127, %D122) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R7.5376, %B1407 = "arith.adds"(%R10.2816, %C127, %C23, %D122) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R10.2816, %B1408 = "sfu.taylor_4x"(%R9.12800, %R10.9216, %D122) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1409 = "arith.mul"(%R10.2816, %R7.5376, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.12800, %B1410 = "arith.add"(%R7.5376, %C1.0, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1411 = "arith.div"(%C1.0, %R9.12800, %D122) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1412 = "arith.mul"(%R7.5376, %R8, %D122) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %D123 = "dma.tensor"(%G12566528, %B1396) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R9.12800, %B1413 = "conv.normal"(%R7.5376, %R4.11264, %R11, %C0.0, %D123) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.6400, %D124 = "dma.tensor"(%G12570624, %B1412) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R8.1792, %B1414 = "tsbc.s_bc"(%S1024, %D124) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.11776, %B1415 = "arith.sub"(%C0.0, %R9.12800, %D124) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.11776, %B1416 = "arith.max"(%R7.11776, %C-3.4028198694267105e+35, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1417 = "arith.mul"(%R7.11776, %C1.4426950216293335, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1418 = "arith.cast"(%R7.5376, %D124) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1419 = "arith.mul"(%R4.11264, %C0.6931471824645996, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1420 = "arith.sub"(%R7.11776, %R7.5376, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.11776, %B1421 = "arith.cast"(%R4.11264, %D124) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R4.11264, %B1422 = "arith.min"(%R7.11776, %C127, %D124) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R7.11776, %B1423 = "arith.max"(%R4.11264, %C-127, %D124) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R4.11264, %B1424 = "arith.adds"(%R7.11776, %C127, %C23, %D124) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R7.11776, %B1425 = "sfu.taylor_4x"(%R7.5376, %R8.1792, %D124) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1426 = "arith.mul"(%R7.11776, %R4.11264, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7.5376, %B1427 = "arith.add"(%R4.11264, %C1.0, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1428 = "arith.div"(%C1.0, %R7.5376, %D124) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1429 = "arith.mul"(%R4.11264, %R9.12800, %D124) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %D125 = "dma.tensor"(%G13094912, %B1413) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R8, %B1430 = "arith.add"(%R6.15360, %R4.11264, %D125) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [0, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [0, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7, %B1431 = "conv.normal"(%R9, %R8.6400, %R6, %C0.0, %D125) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %D126 = "dma.tensor"(%G13099008, %B1430) {decompress = False} : (memref<1x512x512x1xf32, strides: [262144, 512, 1, 1]>, none) -> (memref<1x512x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R10, %D127 = "dma.tensor"(%G14147584, %B1430) {decompress = False} : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, none), %R9.2816, %B1432 = "tsbc.s_bc"(%S1024, %D127) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.12800, %B1433 = "arith.sub"(%C0.0, %R7, %D127) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.12800, %B1434 = "arith.max"(%R8.12800, %C-3.4028198694267105e+35, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.6400, %B1435 = "arith.mul"(%R8.12800, %C1.4426950216293335, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.11264, %B1436 = "arith.cast"(%R8.6400, %D127) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.6400, %B1437 = "arith.mul"(%R5.11264, %C0.6931471824645996, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.6400, %B1438 = "arith.sub"(%R8.12800, %R8.6400, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.12800, %B1439 = "arith.cast"(%R5.11264, %D127) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R5.11264, %B1440 = "arith.min"(%R8.12800, %C127, %D127) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R8.12800, %B1441 = "arith.max"(%R5.11264, %C-127, %D127) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R5.11264, %B1442 = "arith.adds"(%R8.12800, %C127, %C23, %D127) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R8.12800, %B1443 = "sfu.taylor_4x"(%R8.6400, %R9.2816, %D127) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.11264, %B1444 = "arith.mul"(%R8.12800, %R5.11264, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8.6400, %B1445 = "arith.add"(%R5.11264, %C1.0, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.11264, %B1446 = "arith.div"(%C1.0, %R8.6400, %D127) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.11264, %B1447 = "arith.mul"(%R5.11264, %R7, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7, %B1448 = "arith.copy"(%R8, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7.6400, %B1449 = "arith.copy"(%R5.11264, %D127) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1450 = "conv.normal"(%R7, %R4.11264, %R10, %C0.0, %D127) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R11, %D128 = "dma.tensor"(%G14675968, %B1449) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R6.4096, %B1451 = "tsbc.s_bc"(%S1024, %D128) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.7680, %B1452 = "arith.sub"(%C0.0, %R8, %D128) {round_mode = 0} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1453 = "arith.max"(%R5.7680, %C-3.4028198694267105e+35, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1454 = "arith.mul"(%R5.7680, %C1.4426950216293335, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7, %B1455 = "arith.cast"(%R4.11264, %D128) {round_mode = 3} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1456 = "arith.mul"(%R7, %C0.6931471824645996, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1457 = "arith.sub"(%R5.7680, %R4.11264, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1458 = "arith.cast"(%R7, %D128) {round_mode = 1} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R7, %B1459 = "arith.min"(%R5.7680, %C127, %D128) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R5.7680, %B1460 = "arith.max"(%R7, %C-127, %D128) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R7, %B1461 = "arith.adds"(%R5.7680, %C127, %C23, %D128) {round_mode = 1} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, ui8, none) -> (memref<1x512x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R5.7680, %B1462 = "sfu.taylor_4x"(%R4.11264, %R6.4096, %D128) : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7, %B1463 = "arith.mul"(%R5.7680, %R7, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1464 = "arith.add"(%R7, %C1.0, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7, %B1465 = "arith.div"(%C1.0, %R4.11264, %D128) {iter = 3} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7, %B1466 = "arith.mul"(%R7, %R8, %D128) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %D129 = "dma.tensor"(%G14151680, %B1450) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R8, %B1467 = "conv.normal"(%R7, %R9, %R11, %C0.0, %D129) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %D130 = "dma.tensor"(%G14680064, %B1466) {decompress = False} : (memref<1x512x1024x1xf32, strides: [524288, 1024, 1, 1]>, none) -> (memref<1x512x1024x1xf32, strides: [8192, 1024, 1, 1]>, none), %R12.6400, %D131 = "dma.tensor"(%G16777216, %B1466) {decompress = False} : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, none), %R9.12800, %B1468 = "tsbc.s_bc"(%S1024, %D131) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.6400, %B1469 = "arith.sub"(%C0.0, %R8, %D131) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.6400, %B1470 = "arith.max"(%R9.6400, %C-3.4028198694267105e+35, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9, %B1471 = "arith.mul"(%R9.6400, %C1.4426950216293335, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.11264, %B1472 = "arith.cast"(%R9, %D131) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9, %B1473 = "arith.mul"(%R6.11264, %C0.6931471824645996, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9, %B1474 = "arith.sub"(%R9.6400, %R9, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9.6400, %B1475 = "arith.cast"(%R6.11264, %D131) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6.11264, %B1476 = "arith.min"(%R9.6400, %C127, %D131) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R9.6400, %B1477 = "arith.max"(%R6.11264, %C-127, %D131) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6.11264, %B1478 = "arith.adds"(%R9.6400, %C127, %C23, %D131) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R9.6400, %B1479 = "sfu.taylor_4x"(%R9, %R9.12800, %D131) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.11264, %B1480 = "arith.mul"(%R9.6400, %R6.11264, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R9, %B1481 = "arith.add"(%R6.11264, %C1.0, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.11264, %B1482 = "arith.div"(%C1.0, %R9, %D131) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.11264, %B1483 = "arith.mul"(%R6.11264, %R8, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R10, %B1484 = "pord.maxpooling"(%R6.11264, %C-3.4028234663852886e+38, %D131) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11, %B1485 = "pord.maxpooling"(%R10, %C-3.4028234663852886e+38, %D131) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R12, %B1486 = "pord.maxpooling"(%R11, %C-3.4028234663852886e+38, %D131) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R8, %B1487 = "arith.copy"(%R6.11264, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 400, 20, 1]>, none), %R8.6400, %B1488 = "arith.copy"(%R10, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 400, 20, 1]>, none), %R8.12800, %B1489 = "arith.copy"(%R11, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 400, 20, 1]>, none), %R9.2816, %B1490 = "arith.copy"(%R12, %D131) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 400, 20, 1]>, none), %R7, %B1491 = "conv.normal"(%R8, %R4.11264, %R12.6400, %C0.0, %D131) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x1024x20x20xf32, strides: [6400, 400, 20, 1]>, memref<512x1024x1x1xf32>, memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R10, %D132 = "dma.tensor"(%G17305600, %B1490) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R9.9216, %B1492 = "tsbc.s_bc"(%S1024, %D132) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R8.12800, %B1493 = "arith.sub"(%C0.0, %R7, %D132) {round_mode = 0} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8.12800, %B1494 = "arith.max"(%R8.12800, %C-3.4028198694267105e+35, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1495 = "arith.mul"(%R8.12800, %C1.4426950216293335, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1496 = "arith.cast"(%R8, %D132) {round_mode = 3} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1497 = "arith.mul"(%R4.11264, %C0.6931471824645996, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1498 = "arith.sub"(%R8.12800, %R8, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8.12800, %B1499 = "arith.cast"(%R4.11264, %D132) {round_mode = 1} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R4.11264, %B1500 = "arith.min"(%R8.12800, %C127, %D132) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R8.12800, %B1501 = "arith.max"(%R4.11264, %C-127, %D132) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R4.11264, %B1502 = "arith.adds"(%R8.12800, %C127, %C23, %D132) {round_mode = 1} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, ui8, none) -> (memref<1x512x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R8.12800, %B1503 = "sfu.taylor_4x"(%R8, %R9.9216, %D132) : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1504 = "arith.mul"(%R8.12800, %R4.11264, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1505 = "arith.add"(%R4.11264, %C1.0, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1506 = "arith.div"(%C1.0, %R8, %D132) {iter = 3} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4.11264, %B1507 = "arith.mul"(%R4.11264, %R7, %D132) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R6, %D133 = "dma.tensor"(%G16781312, %B1491) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R7, %B1508 = "conv.normal"(%R4.11264, %R6, %R10, %C0.0, %D133) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R11.13312, %D134 = "dma.tensor"(%G17571840, %B1507) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R6.12800, %B1509 = "tsbc.s_bc"(%S1024, %D134) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R6.6400, %B1510 = "arith.sub"(%C0.0, %R7, %D134) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.6400, %B1511 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1512 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1513 = "arith.cast"(%R6, %D134) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1514 = "arith.mul"(%R4.11264, %C0.6931471824645996, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1515 = "arith.sub"(%R6.6400, %R6, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6.6400, %B1516 = "arith.cast"(%R4.11264, %D134) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R4.11264, %B1517 = "arith.min"(%R6.6400, %C127, %D134) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6.6400, %B1518 = "arith.max"(%R4.11264, %C-127, %D134) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R4.11264, %B1519 = "arith.adds"(%R6.6400, %C127, %C23, %D134) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R6.6400, %B1520 = "sfu.taylor_4x"(%R6, %R6.12800, %D134) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1521 = "arith.mul"(%R6.6400, %R4.11264, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1522 = "arith.add"(%R4.11264, %C1.0, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1523 = "arith.div"(%C1.0, %R6, %D134) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.11264, %B1524 = "arith.mul"(%R4.11264, %R7, %D134) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R10, %D135 = "dma.tensor"(%G17309696, %B1508) {decompress = False} : (memref<1x128x512x1xf32, strides: [65536, 512, 1, 1]>, none) -> (memref<1x128x512x1xf32, strides: [1024, 512, 1, 1]>, none), %R10.4096, %B1525 = "arith.copy"(%R4.11264, %D135) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 1600, 80, 2]>, none), %R10.4100, %B1526 = "arith.copy"(%R4.11264, %D135) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [6400, 1600, 80, 2]>, none), %R10.4256, %B1527 = "arith.copy"(%R10.4096, %D135) {round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 1600, 80, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 1600, 80, 1]>, none), %R6, %B1528 = "arith.copy"(%R10.4096, %D135) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [12800, 1600, 40, 1]>, none), %R7.9216, %B1529 = "arith.copy"(%R3.2048, %D135) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [12800, 1600, 40, 1]>, none), %R3.2048, %B1530 = "conv.normal"(%R6, %R10, %R11.13312, %C0.0, %D135) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x40x40xf32, strides: [12800, 1600, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %G49135616, %D136 = "dma.tensor"(%R4.11264, %B1529) {decompress = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R11.9216, %B1531 = "tsbc.s_bc"(%S1024, %D136) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R10.12800, %B1532 = "arith.sub"(%C0.0, %R3.2048, %D136) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10.12800, %B1533 = "arith.max"(%R10.12800, %C-3.4028198694267105e+35, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %B1534 = "arith.mul"(%R10.12800, %C1.4426950216293335, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1535 = "arith.cast"(%R10, %D136) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %B1536 = "arith.mul"(%R4, %C0.6931471824645996, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %B1537 = "arith.sub"(%R10.12800, %R10, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10.12800, %B1538 = "arith.cast"(%R4, %D136) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4, %B1539 = "arith.min"(%R10.12800, %C127, %D136) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R10.12800, %B1540 = "arith.max"(%R4, %C-127, %D136) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4, %B1541 = "arith.adds"(%R10.12800, %C127, %C23, %D136) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R10.12800, %B1542 = "sfu.taylor_4x"(%R10, %R11.9216, %D136) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1543 = "arith.mul"(%R10.12800, %R4, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %B1544 = "arith.add"(%R4, %C1.0, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1545 = "arith.div"(%C1.0, %R10, %D136) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1546 = "arith.mul"(%R4, %R3.2048, %D136) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %D137 = "dma.tensor"(%G17575936, %B1530) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R9.2048, %D138 = "dma.tensor"(%G17641472, %B1530) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3.2048, %B1547 = "conv.normal"(%R4, %R5, %R9.2048, %C0.0, %D138) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10.11392, %D139 = "dma.tensor"(%G18235392, %B1546) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10.11264, %B1548 = "tsbc.s_bc"(%S1024, %D139) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.14848, %B1549 = "arith.sub"(%C0.0, %R3.2048, %D139) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.14848, %B1550 = "arith.max"(%R9.14848, %C-3.4028198694267105e+35, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.2048, %B1551 = "arith.mul"(%R9.14848, %C1.4426950216293335, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1552 = "arith.cast"(%R9.2048, %D139) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.2048, %B1553 = "arith.mul"(%R4, %C0.6931471824645996, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.2048, %B1554 = "arith.sub"(%R9.14848, %R9.2048, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.14848, %B1555 = "arith.cast"(%R4, %D139) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4, %B1556 = "arith.min"(%R9.14848, %C127, %D139) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R9.14848, %B1557 = "arith.max"(%R4, %C-127, %D139) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4, %B1558 = "arith.adds"(%R9.14848, %C127, %C23, %D139) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R9.14848, %B1559 = "sfu.taylor_4x"(%R9.2048, %R10.11264, %D139) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1560 = "arith.mul"(%R9.14848, %R4, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9.2048, %B1561 = "arith.add"(%R4, %C1.0, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1562 = "arith.div"(%C1.0, %R9.2048, %D139) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1563 = "arith.mul"(%R4, %R3.2048, %D139) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %D140 = "dma.tensor"(%G17645568, %B1547) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R9.2048, %B1564 = "conv.normal"(%R4, %R5, %R10.11392, %C0.0, %D140) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.9216, %B1565 = "tsbc.s_bc"(%S1024, %D140) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R4.12800, %B1566 = "arith.sub"(%C0.0, %R9.2048, %D140) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1567 = "arith.max"(%R4.12800, %C-3.4028198694267105e+35, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1568 = "arith.mul"(%R4.12800, %C1.4426950216293335, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1569 = "arith.cast"(%R4, %D140) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1570 = "arith.mul"(%R3.2048, %C0.6931471824645996, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1571 = "arith.sub"(%R4.12800, %R4, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1572 = "arith.cast"(%R3.2048, %D140) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1573 = "arith.min"(%R4.12800, %C127, %D140) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1574 = "arith.max"(%R3.2048, %C-127, %D140) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1575 = "arith.adds"(%R4.12800, %C127, %C23, %D140) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1576 = "sfu.taylor_4x"(%R4, %R5.9216, %D140) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1577 = "arith.mul"(%R4.12800, %R3.2048, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1578 = "arith.add"(%R3.2048, %C1.0, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1579 = "arith.div"(%C1.0, %R4, %D140) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1580 = "arith.mul"(%R3.2048, %R9.2048, %D140) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R10, %D141 = "dma.tensor"(%G18239488, %B1564) {decompress = False} : (memref<1x128x512x1xf32, strides: [65536, 512, 1, 1]>, none) -> (memref<1x128x512x1xf32, strides: [1024, 512, 1, 1]>, none), %R11, %D142 = "dma.tensor"(%G18501632, %B1564) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3.14848, %B1581 = "conv.normal"(%R6, %R10, %R11, %C0.0, %D142) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x40x40xf32, strides: [12800, 1600, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.9216, %B1582 = "tsbc.s_bc"(%S1024, %D142) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.12800, %B1583 = "arith.sub"(%C0.0, %R3.14848, %D142) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1584 = "arith.max"(%R5.12800, %C-3.4028198694267105e+35, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1585 = "arith.mul"(%R5.12800, %C1.4426950216293335, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1586 = "arith.cast"(%R5, %D142) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1587 = "arith.mul"(%R7, %C0.6931471824645996, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1588 = "arith.sub"(%R5.12800, %R5, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1589 = "arith.cast"(%R7, %D142) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1590 = "arith.min"(%R5.12800, %C127, %D142) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1591 = "arith.max"(%R7, %C-127, %D142) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R7, %B1592 = "arith.adds"(%R5.12800, %C127, %C23, %D142) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1593 = "sfu.taylor_4x"(%R5, %R6.9216, %D142) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1594 = "arith.mul"(%R5.12800, %R7, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1595 = "arith.add"(%R7, %C1.0, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1596 = "arith.div"(%C1.0, %R5, %D142) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %B1597 = "arith.mul"(%R7, %R3.14848, %D142) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R9, %D143 = "dma.tensor"(%G18767872, %B1581) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R8, %D144 = "dma.tensor"(%G18505728, %B1581) {decompress = False} : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R5, %B1598 = "arith.copy"(%R3.2048, %D144) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5.12800, %B1599 = "arith.copy"(%R7, %D144) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.2048, %B1600 = "conv.normal"(%R5, %R8, %R9, %C0.0, %D144) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R11.2048, %B1601 = "tsbc.s_bc"(%S1024, %D144) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R9.9216, %B1602 = "arith.sub"(%C0.0, %R3.2048, %D144) {round_mode = 0} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1603 = "arith.max"(%R9.9216, %C-3.4028198694267105e+35, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1604 = "arith.mul"(%R9.9216, %C1.4426950216293335, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %B1605 = "arith.cast"(%R8, %D144) {round_mode = 3} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1606 = "arith.mul"(%R5, %C0.6931471824645996, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1607 = "arith.sub"(%R9.9216, %R8, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1608 = "arith.cast"(%R5, %D144) {round_mode = 1} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R5, %B1609 = "arith.min"(%R9.9216, %C127, %D144) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1610 = "arith.max"(%R5, %C-127, %D144) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R5, %B1611 = "arith.adds"(%R9.9216, %C127, %C23, %D144) {round_mode = 1} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x256x40x40xsi32, strides: [6400, 1600, 40, 1]>, none), %R9.9216, %B1612 = "sfu.taylor_4x"(%R8, %R11.2048, %D144) : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %B1613 = "arith.mul"(%R9.9216, %R5, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1614 = "arith.add"(%R5, %C1.0, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %B1615 = "arith.div"(%C1.0, %R8, %D144) {iter = 3} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %B1616 = "arith.mul"(%R5, %R3.2048, %D144) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R7, %D145 = "dma.tensor"(%G18771968, %B1600) {decompress = False} : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [512, 256, 1, 1]>, none), %R12, %D146 = "dma.tensor"(%G18903040, %B1600) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R4, %B1617 = "conv.normal"(%R5, %R7, %R12, %C0.0, %D146) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6.9216, %B1618 = "tsbc.s_bc"(%S1024, %D146) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.12800, %B1619 = "arith.sub"(%C0.0, %R4, %D146) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1620 = "arith.max"(%R5.12800, %C-3.4028198694267105e+35, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1621 = "arith.mul"(%R5.12800, %C1.4426950216293335, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1622 = "arith.cast"(%R5, %D146) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1623 = "arith.mul"(%R3.2048, %C0.6931471824645996, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1624 = "arith.sub"(%R5.12800, %R5, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1625 = "arith.cast"(%R3.2048, %D146) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1626 = "arith.min"(%R5.12800, %C127, %D146) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1627 = "arith.max"(%R3.2048, %C-127, %D146) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1628 = "arith.adds"(%R5.12800, %C127, %C23, %D146) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R5.12800, %B1629 = "sfu.taylor_4x"(%R5, %R6.9216, %D146) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1630 = "arith.mul"(%R5.12800, %R3.2048, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1631 = "arith.add"(%R3.2048, %C1.0, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1632 = "arith.div"(%C1.0, %R5, %D146) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1633 = "arith.mul"(%R3.2048, %R4, %D146) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7.2048, %D147 = "dma.tensor"(%G18972672, %B1617) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R4, %B1634 = "arith.copy"(%R3.2048, %D147) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [12800, 6400, 160, 2]>, none), %R4.4, %B1635 = "arith.copy"(%R3.2048, %D147) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [12800, 6400, 160, 2]>, none), %R4.320, %B1636 = "arith.copy"(%R4, %D147) {round_mode = 0} : (memref<1x128x40x80xf32, strides: [12800, 6400, 160, 1]>, none) -> (memref<1x128x40x80xf32, strides: [12800, 6400, 160, 1]>, none), %R9.2176, %B1637 = "arith.copy"(%R4, %D147) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [25600, 6400, 80, 1]>, none), %R12.4224, %B1638 = "arith.copy"(%R0, %D147) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [25600, 6400, 80, 1]>, none), %R8, %D148 = "dma.tensor"(%G18907136, %B1636) {decompress = False} : (memref<1x64x256x1xf32, strides: [16384, 256, 1, 1]>, none) -> (memref<1x64x256x1xf32, strides: [256, 256, 1, 1]>, none), %R0, %B1639 = "conv.normal"(%R9.2176, %R8, %R7.2048, %C0.0, %D148) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x80x80xf32, strides: [25600, 6400, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %G32391168, %D149 = "dma.tensor"(%R3.2048, %B1638) {decompress = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [204800, 1600, 40, 1]>, none), %R8.2048, %B1640 = "tsbc.s_bc"(%S1024, %D149) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R6.9216, %B1641 = "arith.sub"(%C0.0, %R0, %D149) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6.9216, %B1642 = "arith.max"(%R6.9216, %C-3.4028198694267105e+35, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5, %B1643 = "arith.mul"(%R6.9216, %C1.4426950216293335, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1644 = "arith.cast"(%R5, %D149) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5, %B1645 = "arith.mul"(%R2, %C0.6931471824645996, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5, %B1646 = "arith.sub"(%R6.9216, %R5, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6.9216, %B1647 = "arith.cast"(%R2, %D149) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R2, %B1648 = "arith.min"(%R6.9216, %C127, %D149) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R6.9216, %B1649 = "arith.max"(%R2, %C-127, %D149) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R2, %B1650 = "arith.adds"(%R6.9216, %C127, %C23, %D149) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R6.9216, %B1651 = "sfu.taylor_4x"(%R5, %R8.2048, %D149) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1652 = "arith.mul"(%R6.9216, %R2, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5, %B1653 = "arith.add"(%R2, %C1.0, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1654 = "arith.div"(%C1.0, %R5, %D149) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1655 = "arith.mul"(%R2, %R0, %D149) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %D150 = "dma.tensor"(%G18976768, %B1639) {decompress = False} : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [64, 64, 1, 1]>, none), %R9, %D151 = "dma.tensor"(%G18993152, %B1639) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R0, %B1656 = "conv.normal"(%R2, %R4, %R9, %C0.0, %D151) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R9.2048, %B1657 = "tsbc.s_bc"(%S1024, %D151) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R7.9216, %B1658 = "arith.sub"(%C0.0, %R0, %D151) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1659 = "arith.max"(%R7.9216, %C-3.4028198694267105e+35, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1660 = "arith.mul"(%R7.9216, %C1.4426950216293335, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1661 = "arith.cast"(%R6, %D151) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1662 = "arith.mul"(%R2, %C0.6931471824645996, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1663 = "arith.sub"(%R7.9216, %R6, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1664 = "arith.cast"(%R2, %D151) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R2, %B1665 = "arith.min"(%R7.9216, %C127, %D151) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1666 = "arith.max"(%R2, %C-127, %D151) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R2, %B1667 = "arith.adds"(%R7.9216, %C127, %C23, %D151) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R7.9216, %B1668 = "sfu.taylor_4x"(%R6, %R9.2048, %D151) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1669 = "arith.mul"(%R7.9216, %R2, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R6, %B1670 = "arith.add"(%R2, %C1.0, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1671 = "arith.div"(%C1.0, %R6, %D151) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1672 = "arith.mul"(%R2, %R0, %D151) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %D152 = "dma.tensor"(%G18997248, %B1656) {decompress = False} : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [576, 576, 9, 1]>, none), %R5, %D153 = "dma.tensor"(%G19144704, %B1656) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R6, %B1673 = "conv.normal"(%R2, %R4, %R5, %C0.0, %D153) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5.2048, %B1674 = "tsbc.s_bc"(%S1024, %D153) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.9216, %B1675 = "arith.sub"(%C0.0, %R6, %D153) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1676 = "arith.max"(%R3.9216, %C-3.4028198694267105e+35, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1677 = "arith.mul"(%R3.9216, %C1.4426950216293335, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1678 = "arith.cast"(%R2, %D153) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1679 = "arith.mul"(%R0, %C0.6931471824645996, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1680 = "arith.sub"(%R3.9216, %R2, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1681 = "arith.cast"(%R0, %D153) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R0, %B1682 = "arith.min"(%R3.9216, %C127, %D153) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1683 = "arith.max"(%R0, %C-127, %D153) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R0, %B1684 = "arith.adds"(%R3.9216, %C127, %C23, %D153) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R3.9216, %B1685 = "sfu.taylor_4x"(%R2, %R5.2048, %D153) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1686 = "arith.mul"(%R3.9216, %R0, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R2, %B1687 = "arith.add"(%R0, %C1.0, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1688 = "arith.div"(%C1.0, %R2, %D153) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R0, %B1689 = "arith.mul"(%R0, %R6, %D153) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %D154 = "dma.tensor"(%G19148800, %B1673) {decompress = False} : (memref<1x64x256x1xf32, strides: [16384, 256, 1, 1]>, none) -> (memref<1x64x256x1xf32, strides: [256, 256, 1, 1]>, none), %R9, %D155 = "dma.tensor"(%G19214336, %B1673) {decompress = False} : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, none), %R1.9216, %B1690 = "conv.normal"(%R9.2176, %R8, %R9, %C0.0, %D155) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x80x80xf32, strides: [25600, 6400, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xf32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R7.2048, %B1691 = "tsbc.s_bc"(%S1024, %D155) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.9216, %B1692 = "arith.sub"(%C0.0, %R1.9216, %D155) {round_mode = 0} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5.9216, %B1693 = "arith.max"(%R5.9216, %C-3.4028198694267105e+35, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %B1694 = "arith.mul"(%R5.9216, %C1.4426950216293335, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1695 = "arith.cast"(%R4, %D155) {round_mode = 3} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %B1696 = "arith.mul"(%R8, %C0.6931471824645996, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %B1697 = "arith.sub"(%R5.9216, %R4, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R5.9216, %B1698 = "arith.cast"(%R8, %D155) {round_mode = 1} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R8, %B1699 = "arith.min"(%R5.9216, %C127, %D155) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R5.9216, %B1700 = "arith.max"(%R8, %C-127, %D155) {round_mode = 0} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, none) -> (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, none), %R8, %B1701 = "arith.adds"(%R5.9216, %C127, %C23, %D155) {round_mode = 1} : (memref<1x64x80x80xsi16, strides: [6400, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x64x80x80xsi32, strides: [6400, 6400, 80, 1]>, none), %R5.9216, %B1702 = "sfu.taylor_4x"(%R4, %R7.2048, %D155) : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1703 = "arith.mul"(%R5.9216, %R8, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R4, %B1704 = "arith.add"(%R8, %C1.0, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, f32, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1705 = "arith.div"(%C1.0, %R4, %D155) {iter = 3} : (f32, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R8, %B1706 = "arith.mul"(%R8, %R1.9216, %D155) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none), %R11, %D156 = "dma.tensor"(%G19283968, %B1690) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R10, %D157 = "dma.tensor"(%G19218432, %B1690) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R4, %B1707 = "arith.copy"(%R0, %D157) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R5.9216, %B1708 = "arith.copy"(%R8, %D157) {round_mode = 0} : (memref<1x64x80x80xf32, strides: [6400, 6400, 80, 1]>, none) -> (memref<1x64x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R0, %B1709 = "conv.normal"(%R4, %R10, %R11, %C0.0, %D157) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R15.4160, %B1710 = "tsbc.s_bc"(%S1024, %D157) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R12.2112, %B1711 = "arith.sub"(%C0.0, %R0, %D157) {round_mode = 0} : (f32, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R12.2112, %B1712 = "arith.max"(%R12.2112, %C-3.4028198694267105e+35, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.64, %B1713 = "arith.mul"(%R12.2112, %C1.4426950216293335, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R4, %B1714 = "arith.cast"(%R9.64, %D157) {round_mode = 3} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.64, %B1715 = "arith.mul"(%R4, %C0.6931471824645996, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.64, %B1716 = "arith.sub"(%R12.2112, %R9.64, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R12.2112, %B1717 = "arith.cast"(%R4, %D157) {round_mode = 1} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R4, %B1718 = "arith.min"(%R12.2112, %C127, %D157) {round_mode = 0} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R12.2112, %B1719 = "arith.max"(%R4, %C-127, %D157) {round_mode = 0} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, none) -> (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, none), %R4, %B1720 = "arith.adds"(%R12.2112, %C127, %C23, %D157) {round_mode = 1} : (memref<1x128x80x80xsi16, strides: [12800, 6400, 80, 1]>, si16, ui8, none) -> (memref<1x128x80x80xsi32, strides: [12800, 6400, 80, 1]>, none), %R12.2112, %B1721 = "sfu.taylor_4x"(%R9.64, %R15.4160, %D157) : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R4, %B1722 = "arith.mul"(%R12.2112, %R4, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R9.64, %B1723 = "arith.add"(%R4, %C1.0, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, f32, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R4, %B1724 = "arith.div"(%C1.0, %R9.64, %D157) {iter = 3} : (f32, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R4, %B1725 = "arith.mul"(%R4, %R0, %D157) {round_mode = 0} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none), %R8, %D158 = "dma.tensor"(%G19288064, %B1709) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R9, %D159 = "dma.tensor"(%G19877888, %B1709) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R2, %B1726 = "conv.normal"(%R4, %R8, %R9, %C0.0, %D159) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %G29114368, %D160 = "dma.tensor"(%R4, %B1725) {decompress = False} : (memref<1x128x80x80xf32, strides: [12800, 6400, 80, 1]>, none) -> (memref<1x128x80x80xf32, strides: [819200, 6400, 80, 1]>, none), %G40583168, %D161 = "dma.tensor"(%R2, %B1726) {decompress = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [204800, 1600, 40, 1]>, none), %R3.2816, %D162 = "dma.tensor"(%G40583168, %B1726) {decompress = False} : (memref<1x128x40x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1727 = "tsbc.s_bc"(%S1024, %D162) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R0.12800, %B1728 = "arith.sub"(%C0.0, %R3.2816, %D162) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0.12800, %B1729 = "arith.max"(%R0.12800, %C-3.4028198694267105e+35, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1730 = "arith.mul"(%R0.12800, %C1.4426950216293335, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2, %B1731 = "arith.cast"(%R0, %D162) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1732 = "arith.mul"(%R2, %C0.6931471824645996, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1733 = "arith.sub"(%R0.12800, %R0, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0.12800, %B1734 = "arith.cast"(%R2, %D162) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R2, %B1735 = "arith.min"(%R0.12800, %C127, %D162) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R0.12800, %B1736 = "arith.max"(%R2, %C-127, %D162) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R2, %B1737 = "arith.adds"(%R0.12800, %C127, %C23, %D162) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R0.12800, %B1738 = "sfu.taylor_4x"(%R0, %R1.9216, %D162) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2, %B1739 = "arith.mul"(%R0.12800, %R2, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1740 = "arith.add"(%R2, %C1.0, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2, %B1741 = "arith.div"(%C1.0, %R0, %D162) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2, %B1742 = "arith.mul"(%R2, %R3.2816, %D162) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %D163 = "dma.tensor"(%G32391168, %B1726) {decompress = False} : (memref<1x128x40x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1743 = "arith.copy"(%R2, %D163) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R0.12800, %B1744 = "arith.copy"(%R4, %D163) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R6, %D164 = "dma.tensor"(%G19881984, %B1742) {decompress = False} : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [512, 256, 1, 1]>, none), %R7, %D165 = "dma.tensor"(%G20013056, %B1742) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R5, %B1745 = "conv.normal"(%R0, %R6, %R7, %C0.0, %D165) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.9216, %B1746 = "tsbc.s_bc"(%S1024, %D165) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.12800, %B1747 = "arith.sub"(%C0.0, %R5, %D165) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.12800, %B1748 = "arith.max"(%R3.12800, %C-3.4028198694267105e+35, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3, %B1749 = "arith.mul"(%R3.12800, %C1.4426950216293335, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1750 = "arith.cast"(%R3, %D165) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3, %B1751 = "arith.mul"(%R1.9216, %C0.6931471824645996, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3, %B1752 = "arith.sub"(%R3.12800, %R3, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.12800, %B1753 = "arith.cast"(%R1.9216, %D165) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1754 = "arith.min"(%R3.12800, %C127, %D165) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R3.12800, %B1755 = "arith.max"(%R1.9216, %C-127, %D165) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1756 = "arith.adds"(%R3.12800, %C127, %C23, %D165) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R3.12800, %B1757 = "sfu.taylor_4x"(%R3, %R4.9216, %D165) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1758 = "arith.mul"(%R3.12800, %R1.9216, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3, %B1759 = "arith.add"(%R1.9216, %C1.0, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1760 = "arith.div"(%C1.0, %R3, %D165) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1761 = "arith.mul"(%R1.9216, %R5, %D165) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6, %D166 = "dma.tensor"(%G20017152, %B1745) {decompress = False} : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [256, 128, 1, 1]>, none), %R7, %D167 = "dma.tensor"(%G20082688, %B1745) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3, %B1762 = "conv.normal"(%R1.9216, %R6, %R7, %C0.0, %D167) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5.9344, %D168 = "dma.tensor"(%G20676608, %B1761) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R5.9216, %B1763 = "tsbc.s_bc"(%S1024, %D168) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R4.12800, %B1764 = "arith.sub"(%C0.0, %R3, %D168) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1765 = "arith.max"(%R4.12800, %C-3.4028198694267105e+35, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1766 = "arith.mul"(%R4.12800, %C1.4426950216293335, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1767 = "arith.cast"(%R4, %D168) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1768 = "arith.mul"(%R1.9216, %C0.6931471824645996, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1769 = "arith.sub"(%R4.12800, %R4, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1770 = "arith.cast"(%R1.9216, %D168) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1771 = "arith.min"(%R4.12800, %C127, %D168) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1772 = "arith.max"(%R1.9216, %C-127, %D168) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1773 = "arith.adds"(%R4.12800, %C127, %C23, %D168) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R4.12800, %B1774 = "sfu.taylor_4x"(%R4, %R5.9216, %D168) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1775 = "arith.mul"(%R4.12800, %R1.9216, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R4, %B1776 = "arith.add"(%R1.9216, %C1.0, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1777 = "arith.div"(%C1.0, %R4, %D168) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1778 = "arith.mul"(%R1.9216, %R3, %D168) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6, %D169 = "dma.tensor"(%G20086784, %B1762) {decompress = False} : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [2304, 1152, 9, 1]>, none), %R4, %B1779 = "conv.normal"(%R1.9216, %R6, %R5.9344, %C0.0, %D169) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R3.2048, %B1780 = "tsbc.s_bc"(%S1024, %D169) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.5632, %B1781 = "arith.sub"(%C0.0, %R4, %D169) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2.5632, %B1782 = "arith.max"(%R2.5632, %C-3.4028198694267105e+35, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1783 = "arith.mul"(%R2.5632, %C1.4426950216293335, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1784 = "arith.cast"(%R1.9216, %D169) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1785 = "arith.mul"(%R5, %C0.6931471824645996, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1786 = "arith.sub"(%R2.5632, %R1.9216, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2.5632, %B1787 = "arith.cast"(%R5, %D169) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5, %B1788 = "arith.min"(%R2.5632, %C127, %D169) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R2.5632, %B1789 = "arith.max"(%R5, %C-127, %D169) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R5, %B1790 = "arith.adds"(%R2.5632, %C127, %C23, %D169) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R2.5632, %B1791 = "sfu.taylor_4x"(%R1.9216, %R3.2048, %D169) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1792 = "arith.mul"(%R2.5632, %R5, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.9216, %B1793 = "arith.add"(%R5, %C1.0, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1794 = "arith.div"(%C1.0, %R1.9216, %D169) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R5, %B1795 = "arith.mul"(%R5, %R4, %D169) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R6, %D170 = "dma.tensor"(%G20680704, %B1779) {decompress = False} : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [512, 256, 1, 1]>, none), %R7, %D171 = "dma.tensor"(%G20811776, %B1779) {decompress = False} : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3, %B1796 = "conv.normal"(%R0, %R6, %R7, %C0.0, %D171) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xf32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R2.9216, %B1797 = "tsbc.s_bc"(%S1024, %D171) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R1.12800, %B1798 = "arith.sub"(%C0.0, %R3, %D171) {round_mode = 0} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.12800, %B1799 = "arith.max"(%R1.12800, %C-3.4028198694267105e+35, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1, %B1800 = "arith.mul"(%R1.12800, %C1.4426950216293335, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1801 = "arith.cast"(%R1, %D171) {round_mode = 3} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1, %B1802 = "arith.mul"(%R0, %C0.6931471824645996, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1, %B1803 = "arith.sub"(%R1.12800, %R1, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1.12800, %B1804 = "arith.cast"(%R0, %D171) {round_mode = 1} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R0, %B1805 = "arith.min"(%R1.12800, %C127, %D171) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R1.12800, %B1806 = "arith.max"(%R0, %C-127, %D171) {round_mode = 0} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, none) -> (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, none), %R0, %B1807 = "arith.adds"(%R1.12800, %C127, %C23, %D171) {round_mode = 1} : (memref<1x128x40x40xsi16, strides: [3200, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x128x40x40xsi32, strides: [3200, 1600, 40, 1]>, none), %R1.12800, %B1808 = "sfu.taylor_4x"(%R1, %R2.9216, %D171) : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1809 = "arith.mul"(%R1.12800, %R0, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R1, %B1810 = "arith.add"(%R0, %C1.0, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, f32, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1811 = "arith.div"(%C1.0, %R1, %D171) {iter = 3} : (f32, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R0, %B1812 = "arith.mul"(%R0, %R3, %D171) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none), %R7, %D172 = "dma.tensor"(%G21078016, %B1796) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R5.12800, %D173 = "dma.tensor"(%G20815872, %B1796) {decompress = False} : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R3, %B1813 = "arith.copy"(%R5, %D173) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.12800, %B1814 = "arith.copy"(%R0, %D173) {round_mode = 0} : (memref<1x128x40x40xf32, strides: [3200, 1600, 40, 1]>, none) -> (memref<1x128x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R8, %B1815 = "conv.normal"(%R3, %R5.12800, %R7, %C0.0, %D173) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R0, %D174 = "dma.tensor"(%G21082112, %B1814) {decompress = False} : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [9216, 2304, 9, 1]>, none), %R5.6144, %B1816 = "tsbc.s_bc"(%S1024, %D174) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.13312, %B1817 = "arith.sub"(%C0.0, %R8, %D174) {round_mode = 0} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.13312, %B1818 = "arith.max"(%R3.13312, %C-3.4028198694267105e+35, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R2.4096, %B1819 = "arith.mul"(%R3.13312, %C1.4426950216293335, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R6, %B1820 = "arith.cast"(%R2.4096, %D174) {round_mode = 3} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R2.4096, %B1821 = "arith.mul"(%R6, %C0.6931471824645996, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R2.4096, %B1822 = "arith.sub"(%R3.13312, %R2.4096, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R3.13312, %B1823 = "arith.cast"(%R6, %D174) {round_mode = 1} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R6, %B1824 = "arith.min"(%R3.13312, %C127, %D174) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R3.13312, %B1825 = "arith.max"(%R6, %C-127, %D174) {round_mode = 0} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, none) -> (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, none), %R6, %B1826 = "arith.adds"(%R3.13312, %C127, %C23, %D174) {round_mode = 1} : (memref<1x256x40x40xsi16, strides: [6400, 1600, 40, 1]>, si16, ui8, none) -> (memref<1x256x40x40xsi32, strides: [6400, 1600, 40, 1]>, none), %R3.13312, %B1827 = "sfu.taylor_4x"(%R2.4096, %R5.6144, %D174) : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R6, %B1828 = "arith.mul"(%R3.13312, %R6, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R2.4096, %B1829 = "arith.add"(%R6, %C1.0, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, f32, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R6, %B1830 = "arith.div"(%C1.0, %R2.4096, %D174) {iter = 3} : (f32, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R6, %B1831 = "arith.mul"(%R6, %R8, %D174) {round_mode = 0} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R10, %D175 = "dma.tensor"(%G23441408, %B1815) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R4, %B1832 = "conv.normal"(%R6, %R0, %R10, %C0.0, %D175) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %G47087616, %D176 = "dma.tensor"(%R6, %B1831) {decompress = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [409600, 1600, 40, 1]>, none), %R0.12800, %B1833 = "tsbc.s_bc"(%S1024, %D176) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R0.6400, %B1834 = "arith.sub"(%C0.0, %R4, %D176) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1835 = "arith.max"(%R0.6400, %C-3.4028198694267105e+35, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1836 = "arith.mul"(%R0.6400, %C1.4426950216293335, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.12800, %B1837 = "arith.cast"(%R0, %D176) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1838 = "arith.mul"(%R2.12800, %C0.6931471824645996, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1839 = "arith.sub"(%R0.6400, %R0, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1840 = "arith.cast"(%R2.12800, %D176) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.12800, %B1841 = "arith.min"(%R0.6400, %C127, %D176) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R0.6400, %B1842 = "arith.max"(%R2.12800, %C-127, %D176) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.12800, %B1843 = "arith.adds"(%R0.6400, %C127, %C23, %D176) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1844 = "sfu.taylor_4x"(%R0, %R0.12800, %D176) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.12800, %B1845 = "arith.mul"(%R0.6400, %R2.12800, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1846 = "arith.add"(%R2.12800, %C1.0, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.12800, %B1847 = "arith.div"(%C1.0, %R0, %D176) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.12800, %B1848 = "arith.mul"(%R2.12800, %R4, %D176) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %G48726016, %D177 = "dma.tensor"(%R2.12800, %B1848) {decompress = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R0, %D178 = "dma.tensor"(%G23445504, %B1848) {decompress = False} : (memref<1x255x128x1xf32, strides: [32640, 128, 1, 1]>, none) -> (memref<1x255x128x1xf32, strides: [512, 128, 1, 1]>, none), %R0.2048, %D179 = "dma.tensor"(%G23576576, %B1848) {decompress = False} : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, none), %R1, %D180 = "dma.tensor"(%G29114368, %B1848) {decompress = False} : (memref<1x128x40x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x40x80xf32, strides: [6400, 3200, 80, 1]>, none), %R5, %B1849 = "conv.normal"(%R1, %R0, %R0.2048, %C0.0, %D180) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x80xf32, strides: [6400, 3200, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x255x40x80xf32, strides: [12800, 3200, 80, 1]>, none), %R3, %D181 = "dma.tensor"(%G29127168, %B1848) {decompress = False} : (memref<1x128x40x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x40x80xf32, strides: [6400, 3200, 80, 1]>, none), %R9, %B1850 = "conv.normal"(%R3, %R0, %R0.2048, %C0.0, %D181) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x128x40x80xf32, strides: [6400, 3200, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x255x40x80xf32, strides: [12800, 3200, 80, 1]>, none), %G35643392, %D182 = "dma.tensor"(%R5, %B1849) {decompress = False} : (memref<1x255x40x80xf32, strides: [12800, 3200, 80, 1]>, none) -> (memref<1x255x40x80xf32, strides: [1632000, 6400, 80, 1]>, none), %G35656192, %D183 = "dma.tensor"(%R9, %B1850) {decompress = False} : (memref<1x255x40x80xf32, strides: [12800, 3200, 80, 1]>, none) -> (memref<1x255x40x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R0, %D184 = "dma.tensor"(%G35643392, %B1850) {decompress = False} : (memref<85x64x100x1xf32, strides: [6400, 100, 1, 1]>, none) -> (memref<85x64x100x1xf32, strides: [100, 100, 1, 1]>, none), %R8, %B1851 = "arith.copy"(%R0, %D184) {round_mode = 0} : (memref<1x64x100x85xf32, strides: [8500, 100, 1, 100]>, none) -> (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none), %R4, %D185 = "dma.tensor"(%G37819392, %B1850) {decompress = False} : (memref<85x64x100x1xf32, strides: [6400, 100, 1, 1]>, none) -> (memref<85x64x100x1xf32, strides: [100, 100, 1, 1]>, none), %R12, %B1852 = "arith.copy"(%R4, %D185) {round_mode = 0} : (memref<1x64x100x85xf32, strides: [8500, 100, 1, 100]>, none) -> (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none), %G29114368, %D186 = "dma.tensor"(%R8, %B1851) {decompress = False} : (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none) -> (memref<1x64x100x85xf32, strides: [544000, 8500, 85, 1]>, none), %R0, %D187 = "dma.tensor"(%G39995392, %B1851) {decompress = False} : (memref<85x64x100x1xf32, strides: [6400, 100, 1, 1]>, none) -> (memref<85x64x100x1xf32, strides: [100, 100, 1, 1]>, none), %R8, %B1853 = "arith.copy"(%R0, %D187) {round_mode = 0} : (memref<1x64x100x85xf32, strides: [8500, 100, 1, 100]>, none) -> (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none), %G31290368, %D188 = "dma.tensor"(%R12, %B1852) {decompress = False} : (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none) -> (memref<1x64x100x85xf32, strides: [544000, 8500, 85, 1]>, none), %G33466368, %D189 = "dma.tensor"(%R8, %B1853) {decompress = False} : (memref<1x64x100x85xf32, strides: [8500, 8500, 85, 1]>, none) -> (memref<1x64x100x85xf32, strides: [544000, 8500, 85, 1]>, none), %R0, %D190 = "dma.tensor"(%G23580672, %B1853) {decompress = False} : (memref<1x255x256x1xf32, strides: [65280, 256, 1, 1]>, none) -> (memref<1x255x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R0.4096, %D191 = "dma.tensor"(%G23842816, %B1853) {decompress = False} : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, none), %R1, %D192 = "dma.tensor"(%G47087616, %B1853) {decompress = False} : (memref<1x256x40x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %R5, %B1854 = "conv.normal"(%R1, %R0, %R0.4096, %C0.0, %D192) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x40x40xf32, strides: [6400, 1600, 40, 1]>, memref<255x256x1x1xf32>, memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x255x40x40xf32, strides: [6400, 1600, 40, 1]>, none), %G37277696, %D193 = "dma.tensor"(%R5, %B1854) {decompress = False} : (memref<1x255x40x40xf32, strides: [6400, 1600, 40, 1]>, none) -> (memref<1x255x40x40xf32, strides: [408000, 1600, 40, 1]>, none), %R0, %D194 = "dma.tensor"(%G37277696, %B1854) {decompress = False} : (memref<255x64x25x1xf32, strides: [1600, 25, 1, 1]>, none) -> (memref<255x64x25x1xf32, strides: [25, 25, 1, 1]>, none), %R8, %B1855 = "arith.copy"(%R0, %D194) {round_mode = 0} : (memref<3x64x25x85xf32, strides: [2125, 25, 1, 25]>, none) -> (memref<3x64x25x85xf32, strides: [2125, 2125, 85, 1]>, none), %G35643392, %D195 = "dma.tensor"(%R8, %B1855) {decompress = False} : (memref<3x64x25x85xf32, strides: [2125, 2125, 85, 1]>, none) -> (memref<3x64x25x85xf32, strides: [136000, 2125, 85, 1]>, none), %R4, %D196 = "dma.tensor"(%G48726016, %B1855) {decompress = False} : (memref<1x512x20x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5, %D197 = "dma.tensor"(%G23846912, %B1855) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R8, %D198 = "dma.tensor"(%G24371200, %B1855) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R3.6400, %B1856 = "conv.normal"(%R4, %R5, %R8, %C0.0, %D198) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7, %D199 = "dma.tensor"(%G24637440, %B1855) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R0, %D200 = "dma.tensor"(%G24641536, %B1855) {decompress = False} : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [9216, 2304, 9, 1]>, none), %R5.9216, %B1857 = "tsbc.s_bc"(%S1024, %D200) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R5.2816, %B1858 = "arith.sub"(%C0.0, %R3.6400, %D200) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.2816, %B1859 = "arith.max"(%R5.2816, %C-3.4028198694267105e+35, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.12800, %B1860 = "arith.mul"(%R5.2816, %C1.4426950216293335, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1861 = "arith.cast"(%R4.12800, %D200) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.12800, %B1862 = "arith.mul"(%R2.4096, %C0.6931471824645996, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.12800, %B1863 = "arith.sub"(%R5.2816, %R4.12800, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R5.2816, %B1864 = "arith.cast"(%R2.4096, %D200) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.4096, %B1865 = "arith.min"(%R5.2816, %C127, %D200) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R5.2816, %B1866 = "arith.max"(%R2.4096, %C-127, %D200) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.4096, %B1867 = "arith.adds"(%R5.2816, %C127, %C23, %D200) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R5.2816, %B1868 = "sfu.taylor_4x"(%R4.12800, %R5.9216, %D200) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1869 = "arith.mul"(%R5.2816, %R2.4096, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R4.12800, %B1870 = "arith.add"(%R2.4096, %C1.0, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1871 = "arith.div"(%C1.0, %R4.12800, %D200) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1872 = "arith.mul"(%R2.4096, %R3.6400, %D200) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %D201 = "dma.tensor"(%G24375296, %B1856) {decompress = False} : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R4.12800, %B1873 = "conv.normal"(%R2.4096, %R6, %R7, %C0.0, %D201) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3.512, %B1874 = "tsbc.s_bc"(%S1024, %D201) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.10496, %B1875 = "arith.sub"(%C0.0, %R4.12800, %D201) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.10496, %B1876 = "arith.max"(%R2.10496, %C-3.4028198694267105e+35, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1877 = "arith.mul"(%R2.10496, %C1.4426950216293335, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1878 = "arith.cast"(%R2.4096, %D201) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1879 = "arith.mul"(%R6, %C0.6931471824645996, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1880 = "arith.sub"(%R2.10496, %R2.4096, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.10496, %B1881 = "arith.cast"(%R6, %D201) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6, %B1882 = "arith.min"(%R2.10496, %C127, %D201) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.10496, %B1883 = "arith.max"(%R6, %C-127, %D201) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R6, %B1884 = "arith.adds"(%R2.10496, %C127, %C23, %D201) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R2.10496, %B1885 = "sfu.taylor_4x"(%R2.4096, %R3.512, %D201) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1886 = "arith.mul"(%R2.10496, %R6, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.4096, %B1887 = "arith.add"(%R6, %C1.0, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1888 = "arith.div"(%C1.0, %R2.4096, %D201) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R6, %B1889 = "arith.mul"(%R6, %R4.12800, %D201) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R7, %D202 = "dma.tensor"(%G27000832, %B1873) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R4.12800, %B1890 = "conv.normal"(%R6, %R0, %R7, %C0.0, %D202) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], res_add = False} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3, %D203 = "dma.tensor"(%G27004928, %B1889) {decompress = False} : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R0.12800, %B1891 = "tsbc.s_bc"(%S1024, %D203) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R0.6400, %B1892 = "arith.sub"(%C0.0, %R4.12800, %D203) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1893 = "arith.max"(%R0.6400, %C-3.4028198694267105e+35, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1894 = "arith.mul"(%R0.6400, %C1.4426950216293335, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R1, %B1895 = "arith.cast"(%R0, %D203) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1896 = "arith.mul"(%R1, %C0.6931471824645996, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1897 = "arith.sub"(%R0.6400, %R0, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1898 = "arith.cast"(%R1, %D203) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R1, %B1899 = "arith.min"(%R0.6400, %C127, %D203) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R0.6400, %B1900 = "arith.max"(%R1, %C-127, %D203) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R1, %B1901 = "arith.adds"(%R0.6400, %C127, %C23, %D203) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R0.6400, %B1902 = "sfu.taylor_4x"(%R0, %R0.12800, %D203) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R1, %B1903 = "arith.mul"(%R0.6400, %R1, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %B1904 = "arith.add"(%R1, %C1.0, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R1, %B1905 = "arith.div"(%C1.0, %R0, %D203) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R1, %B1906 = "arith.mul"(%R1, %R4.12800, %D203) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %D204 = "dma.tensor"(%G27529216, %B1890) {decompress = False} : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, none), %R1.6400, %B1907 = "conv.normal"(%R4, %R3, %R2, %C0.0, %D204) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R0, %D205 = "dma.tensor"(%G27533312, %B1906) {decompress = False} : (memref<1x512x512x1xf32, strides: [262144, 512, 1, 1]>, none) -> (memref<1x512x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R5, %D206 = "dma.tensor"(%G28581888, %B1906) {decompress = False} : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, none), %R2.12800, %B1908 = "tsbc.s_bc"(%S1024, %D206) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R2.6400, %B1909 = "arith.sub"(%C0.0, %R1.6400, %D206) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.6400, %B1910 = "arith.max"(%R2.6400, %C-3.4028198694267105e+35, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %B1911 = "arith.mul"(%R2.6400, %C1.4426950216293335, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3, %B1912 = "arith.cast"(%R2, %D206) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %B1913 = "arith.mul"(%R3, %C0.6931471824645996, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %B1914 = "arith.sub"(%R2.6400, %R2, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2.6400, %B1915 = "arith.cast"(%R3, %D206) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R3, %B1916 = "arith.min"(%R2.6400, %C127, %D206) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R2.6400, %B1917 = "arith.max"(%R3, %C-127, %D206) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, none), %R3, %B1918 = "arith.adds"(%R2.6400, %C127, %C23, %D206) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [1664, 416, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [1600, 400, 20, 1]>, none), %R2.6400, %B1919 = "sfu.taylor_4x"(%R2, %R2.12800, %D206) : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3, %B1920 = "arith.mul"(%R2.6400, %R3, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %B1921 = "arith.add"(%R3, %C1.0, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3, %B1922 = "arith.div"(%C1.0, %R2, %D206) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R3, %B1923 = "arith.mul"(%R3, %R1.6400, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none), %R2, %B1924 = "arith.copy"(%R1, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R2.6400, %B1925 = "arith.copy"(%R3, %D206) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R1, %B1926 = "conv.normal"(%R2, %R0, %R5, %C0.0, %D206) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xf32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R6, %D207 = "dma.tensor"(%G29110272, %B1925) {decompress = False} : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, none), %R4.9216, %B1927 = "tsbc.s_bc"(%S1024, %D207) : (memref<1x64x1x10xf32>, none) -> (memref<1x64x1x10xf32, strides: [16, 16, 10, 1]>, none), %R3.12800, %B1928 = "arith.sub"(%C0.0, %R1, %D207) {round_mode = 0} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3.12800, %B1929 = "arith.max"(%R3.12800, %C-3.4028198694267105e+35, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3, %B1930 = "arith.mul"(%R3.12800, %C1.4426950216293335, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1931 = "arith.cast"(%R3, %D207) {round_mode = 3} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3, %B1932 = "arith.mul"(%R0, %C0.6931471824645996, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3, %B1933 = "arith.sub"(%R3.12800, %R3, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3.12800, %B1934 = "arith.cast"(%R0, %D207) {round_mode = 1} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R0, %B1935 = "arith.min"(%R3.12800, %C127, %D207) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R3.12800, %B1936 = "arith.max"(%R0, %C-127, %D207) {round_mode = 0} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, none) -> (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, none), %R0, %B1937 = "arith.adds"(%R3.12800, %C127, %C23, %D207) {round_mode = 1} : (memref<1x512x20x20xsi16, strides: [3328, 416, 20, 1]>, si16, ui8, none) -> (memref<1x512x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R3.12800, %B1938 = "sfu.taylor_4x"(%R3, %R4.9216, %D207) : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1939 = "arith.mul"(%R3.12800, %R0, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3, %B1940 = "arith.add"(%R0, %C1.0, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1941 = "arith.div"(%C1.0, %R3, %D207) {iter = 3} : (f32, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1942 = "arith.mul"(%R0, %R1, %D207) {round_mode = 0} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R2, %D208 = "dma.tensor"(%G28585984, %B1926) {decompress = False} : (memref<1x255x512x1xf32, strides: [130560, 512, 1, 1]>, none) -> (memref<1x255x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R3, %B1943 = "conv.normal"(%R0, %R2, %R6, %C0.0, %D208) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], res_add = False} : (memref<1x512x20x20xf32, strides: [3200, 400, 20, 1]>, memref<255x512x1x1xf32>, memref<1x255x1x1xf32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x255x20x20xf32, strides: [1600, 400, 20, 1]>, none), %G47087616, %D209 = "dma.tensor"(%R3, %B1943) {decompress = False} : (memref<1x255x20x20xf32, strides: [1600, 400, 20, 1]>, none) -> (memref<1x255x20x20xf32, strides: [102000, 400, 20, 1]>, none), %R0, %D210 = "dma.tensor"(%G47087616, %B1943) {decompress = False} : (memref<255x50x8x1xf32, strides: [400, 8, 1, 1]>, none) -> (memref<255x50x8x1xf32, strides: [8, 8, 1, 1]>, none), %R8, %B1944 = "arith.copy"(%R0, %D210) {round_mode = 0} : (memref<3x50x8x85xf32, strides: [680, 8, 1, 8]>, none) -> (memref<3x50x8x85xf32, strides: [680, 680, 85, 1]>, none), %G37277696, %D211 = "dma.tensor"(%R8, %B1944) {decompress = False} : (memref<3x50x8x85xf32, strides: [680, 680, 85, 1]>, none) -> (memref<3x50x8x85xf32, strides: [34000, 680, 85, 1]>, none)]