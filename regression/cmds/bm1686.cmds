[%R0, %D0 = "dma.tensor"(%G13058048, %B0) : (memref<1x3x66x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x66x322xf32, strides: [21252, 21252, 322, 1]>, none), %R11.3296, %D0 = "dma.tensor"(%G0, %B0) : (memref<1x32x3x36xf32, strides: [3456, 108, 36, 1]>, none) -> (memref<1x32x3x36xf32, strides: [108, 108, 36, 1]>, none), %R11.3728, %D0 = "dma.tensor"(%G16384, %B0) : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D3) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [2, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x66x322xf32, strides: [21252, 21252, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D3) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R12, %D3) {round_mode = 0} : (f32, memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, f32, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, f32, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D3) {round_mode = 3} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, f32, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4096, %R3, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R0, %D3) {round_mode = 1} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4096, %C127, %D3) {round_mode = 0} : (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, si16, none) -> (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, none), %R5.4096, %B1 = "arith.max"(%R0, %C-127, %D3) {round_mode = 0} : (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, si16, none) -> (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D3) {round_mode = 1} : (memref<1x32x32x160xsi16, strides: [5120, 5120, 160, 1]>, si16, ui8, none) -> (memref<1x32x32x160xsi32, strides: [5120, 5120, 160, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R3, %R8, %D3) : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4096, %R0, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, f32, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D3) {iter = 3} : (f32, memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D3) {round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none) -> (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, none), %R11.608, %D0 = "dma.tensor"(%G20480, %B1) : (memref<1x64x32x9xf32, strides: [18432, 288, 9, 1]>, none) -> (memref<1x64x32x9xf32, strides: [576, 288, 9, 1]>, none), %R15.5120, %D0 = "dma.tensor"(%G94208, %B1) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D5) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x32x160xf32, strides: [5120, 5120, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D5) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D5) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D5) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D5) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D5) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D5) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D5) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D5) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D5) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D5) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.2912, %D0 = "dma.tensor"(%G98304, %B18) : (memref<1x32x64x1xf32, strides: [2048, 64, 1, 1]>, none) -> (memref<1x32x64x1xf32, strides: [64, 64, 1, 1]>, none), %R14.4880, %D0 = "dma.tensor"(%G106496, %B18) : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D7) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G0, %D0 = "dma.tensor"(%R0, %B34) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D8) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D8) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D8) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D8) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D8) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D8) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D8) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D8) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D8) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D8) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R11.3168, %D0 = "dma.tensor"(%G110592, %B35) : (memref<1x32x32x1xf32, strides: [1024, 32, 1, 1]>, none) -> (memref<1x32x32x1xf32, strides: [32, 32, 1, 1]>, none), %R14.4896, %D0 = "dma.tensor"(%G114688, %B35) : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D10) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21250048, %D0 = "dma.tensor"(%R0, %B51) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D11) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D11) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D11) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D11) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D11) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D11) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D11) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D11) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D11) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D11) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13059312, %B52) : (memref<1x3x66x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x66x324xf32, strides: [21384, 21384, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D12) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [2, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x66x324xf32, strides: [21384, 21384, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %G6553600, %D0 = "dma.tensor"(%R15, %B68) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.256, %B1 = "tsbc.s_bc"(%R0, %D13) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4224, %B1 = "arith.sub"(%C0.0, %R12, %D13) {round_mode = 0} : (f32, memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R5.4224, %B1 = "arith.max"(%R5.4224, %C-3.4028198694267105e+35, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, f32, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4224, %C1.4426950216293335, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, f32, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D13) {round_mode = 3} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, f32, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4224, %R3, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R5.4224, %B1 = "arith.cast"(%R0, %D13) {round_mode = 1} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4224, %C127, %D13) {round_mode = 0} : (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, si16, none) -> (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, none), %R5.4224, %B1 = "arith.max"(%R0, %C-127, %D13) {round_mode = 0} : (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, si16, none) -> (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4224, %C127, %C23, %D13) {round_mode = 1} : (memref<1x32x32x161xsi16, strides: [5152, 5152, 161, 1]>, si16, ui8, none) -> (memref<1x32x32x161xsi32, strides: [5152, 5152, 161, 1]>, none), %R5.4224, %B1 = "sfu.taylor_4x"(%R3, %R8.256, %D13) : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4224, %R0, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, f32, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D13) {iter = 3} : (f32, memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D13) {round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none) -> (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D13) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x32x161xf32, strides: [5152, 5152, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D13) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D13) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D13) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D13) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D13) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D13) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D13) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D13) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D13) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D13) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D13) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G320, %D0 = "dma.tensor"(%R0, %B102) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D14) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D14) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D14) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D14) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D14) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D14) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D14) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D14) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D14) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D14) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D14) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21250368, %D0 = "dma.tensor"(%R0, %B119) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D15) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D15) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D15) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D15) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D15) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D15) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D15) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D15) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D15) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D15) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13211648, %B120) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D16) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6553920, %D0 = "dma.tensor"(%R15, %B136) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D17) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D17) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D17) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D17) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D17) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D17) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D17) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D17) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D17) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D17) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D17) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D17) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D17) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D17) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D17) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D17) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D17) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D17) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D17) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D17) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D17) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D17) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G10240, %D0 = "dma.tensor"(%R0, %B170) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D18) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D18) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D18) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D18) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D18) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D18) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D18) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D18) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D18) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D18) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D18) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21260288, %D0 = "dma.tensor"(%R0, %B187) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D19) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D19) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D19) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D19) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D19) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D19) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D19) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D19) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D19) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D19) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13212912, %B188) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D20) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6563840, %D0 = "dma.tensor"(%R15, %B204) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D21) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D21) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D21) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D21) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D21) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D21) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D21) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D21) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D21) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D21) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D21) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D21) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D21) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D21) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D21) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D21) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D21) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D21) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D21) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D21) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D21) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D21) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G10560, %D0 = "dma.tensor"(%R0, %B238) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D22) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D22) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D22) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D22) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D22) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D22) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D22) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D22) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D22) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D22) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D22) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21260608, %D0 = "dma.tensor"(%R0, %B255) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D23) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D23) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D23) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D23) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D23) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D23) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D23) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D23) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D23) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D23) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13375488, %B256) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D24) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6564160, %D0 = "dma.tensor"(%R15, %B272) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D25) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D25) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D25) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D25) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D25) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D25) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D25) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D25) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D25) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D25) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D25) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D25) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D25) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D25) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D25) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D25) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D25) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D25) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D25) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D25) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D25) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D25) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G20480, %D0 = "dma.tensor"(%R0, %B306) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D26) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D26) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D26) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D26) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D26) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D26) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D26) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D26) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D26) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D26) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D26) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21270528, %D0 = "dma.tensor"(%R0, %B323) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D27) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D27) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D27) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D27) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D27) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D27) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D27) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D27) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D27) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D27) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13376752, %B324) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D28) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6574080, %D0 = "dma.tensor"(%R15, %B340) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D29) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D29) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D29) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D29) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D29) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D29) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D29) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D29) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D29) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D29) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D29) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D29) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D29) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D29) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D29) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D29) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D29) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D29) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D29) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D29) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D29) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D29) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G20800, %D0 = "dma.tensor"(%R0, %B374) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D30) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D30) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D30) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D30) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D30) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D30) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D30) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D30) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D30) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D30) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D30) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21270848, %D0 = "dma.tensor"(%R0, %B391) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D31) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D31) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D31) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D31) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D31) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D31) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D31) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D31) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D31) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D31) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13539328, %B392) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D32) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6574400, %D0 = "dma.tensor"(%R15, %B408) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D33) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D33) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D33) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D33) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D33) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D33) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D33) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D33) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D33) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D33) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D33) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D33) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D33) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D33) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D33) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D33) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D33) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D33) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D33) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D33) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D33) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D33) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G30720, %D0 = "dma.tensor"(%R0, %B442) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D34) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D34) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D34) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D34) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D34) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D34) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D34) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D34) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D34) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D34) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D34) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21280768, %D0 = "dma.tensor"(%R0, %B459) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D35) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D35) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D35) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D35) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D35) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D35) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D35) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D35) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D35) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D35) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13540592, %B460) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D36) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6584320, %D0 = "dma.tensor"(%R15, %B476) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D37) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D37) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D37) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D37) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D37) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D37) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D37) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D37) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D37) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D37) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D37) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D37) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D37) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D37) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D37) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D37) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D37) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D37) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D37) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D37) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D37) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D37) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G31040, %D0 = "dma.tensor"(%R0, %B510) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D38) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D38) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D38) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D38) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D38) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D38) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D38) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D38) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D38) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D38) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D38) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21281088, %D0 = "dma.tensor"(%R0, %B527) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D39) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D39) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D39) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D39) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D39) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D39) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D39) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D39) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D39) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D39) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13703168, %B528) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D40) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6584640, %D0 = "dma.tensor"(%R15, %B544) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D41) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D41) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D41) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D41) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D41) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D41) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D41) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D41) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D41) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D41) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D41) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D41) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D41) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D41) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D41) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D41) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D41) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D41) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D41) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D41) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D41) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D41) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G40960, %D0 = "dma.tensor"(%R0, %B578) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D42) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D42) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D42) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D42) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D42) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D42) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D42) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D42) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D42) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D42) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D42) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21291008, %D0 = "dma.tensor"(%R0, %B595) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D43) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D43) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D43) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D43) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D43) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D43) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D43) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D43) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D43) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D43) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13704432, %B596) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D44) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6594560, %D0 = "dma.tensor"(%R15, %B612) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D45) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D45) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D45) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D45) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D45) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D45) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D45) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D45) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D45) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D45) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D45) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D45) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D45) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D45) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D45) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D45) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D45) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D45) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D45) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D45) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D45) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D45) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G41280, %D0 = "dma.tensor"(%R0, %B646) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D46) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D46) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D46) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D46) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D46) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D46) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D46) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D46) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D46) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D46) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D46) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21291328, %D0 = "dma.tensor"(%R0, %B663) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D47) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D47) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D47) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D47) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D47) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D47) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D47) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D47) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D47) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D47) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13867008, %B664) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D48) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6594880, %D0 = "dma.tensor"(%R15, %B680) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D49) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D49) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D49) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D49) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D49) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D49) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D49) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D49) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D49) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D49) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D49) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D49) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D49) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D49) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D49) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D49) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D49) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D49) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D49) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D49) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D49) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D49) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G51200, %D0 = "dma.tensor"(%R0, %B714) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D50) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D50) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D50) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D50) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D50) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D50) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D50) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D50) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D50) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D50) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D50) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21301248, %D0 = "dma.tensor"(%R0, %B731) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D51) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D51) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D51) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D51) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D51) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D51) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D51) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D51) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D51) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D51) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G13868272, %B732) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D52) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6604800, %D0 = "dma.tensor"(%R15, %B748) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D53) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D53) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D53) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D53) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D53) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D53) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D53) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D53) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D53) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D53) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D53) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D53) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D53) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D53) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D53) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D53) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D53) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D53) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D53) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D53) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D53) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D53) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G51520, %D0 = "dma.tensor"(%R0, %B782) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D54) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D54) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D54) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D54) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D54) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D54) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D54) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D54) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D54) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D54) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D54) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21301568, %D0 = "dma.tensor"(%R0, %B799) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D55) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D55) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D55) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D55) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D55) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D55) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D55) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D55) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D55) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D55) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14030848, %B800) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D56) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6605120, %D0 = "dma.tensor"(%R15, %B816) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D57) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D57) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D57) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D57) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D57) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D57) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D57) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D57) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D57) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D57) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D57) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D57) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D57) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D57) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D57) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D57) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D57) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D57) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D57) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D57) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D57) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D57) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G61440, %D0 = "dma.tensor"(%R0, %B850) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D58) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D58) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D58) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D58) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D58) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D58) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D58) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D58) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D58) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D58) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D58) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21311488, %D0 = "dma.tensor"(%R0, %B867) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D59) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D59) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D59) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D59) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D59) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D59) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D59) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D59) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D59) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D59) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14032112, %B868) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D60) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6615040, %D0 = "dma.tensor"(%R15, %B884) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D61) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D61) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D61) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D61) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D61) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D61) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D61) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D61) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D61) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D61) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D61) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D61) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D61) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D61) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D61) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D61) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D61) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D61) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D61) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D61) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D61) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D61) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G61760, %D0 = "dma.tensor"(%R0, %B918) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D62) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D62) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D62) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D62) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D62) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D62) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D62) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D62) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D62) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D62) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D62) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21311808, %D0 = "dma.tensor"(%R0, %B935) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D63) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D63) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D63) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D63) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D63) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D63) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D63) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D63) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D63) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D63) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14194688, %B936) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D64) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6615360, %D0 = "dma.tensor"(%R15, %B952) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D65) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D65) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D65) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D65) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D65) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D65) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D65) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D65) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D65) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D65) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D65) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D65) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D65) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D65) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D65) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D65) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D65) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D65) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D65) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D65) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D65) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D65) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G71680, %D0 = "dma.tensor"(%R0, %B986) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D66) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D66) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D66) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D66) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D66) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D66) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D66) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D66) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D66) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D66) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D66) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21321728, %D0 = "dma.tensor"(%R0, %B1003) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D67) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D67) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D67) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D67) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D67) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D67) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D67) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D67) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D67) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D67) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14195952, %B1004) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D68) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6625280, %D0 = "dma.tensor"(%R15, %B1020) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D69) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D69) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D69) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D69) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D69) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D69) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D69) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D69) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D69) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D69) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D69) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D69) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D69) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D69) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D69) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D69) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D69) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D69) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D69) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D69) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D69) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D69) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G72000, %D0 = "dma.tensor"(%R0, %B1054) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D70) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D70) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D70) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D70) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D70) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D70) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D70) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D70) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D70) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D70) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D70) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21322048, %D0 = "dma.tensor"(%R0, %B1071) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D71) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D71) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D71) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D71) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D71) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D71) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D71) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D71) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D71) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D71) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14358528, %B1072) : (memref<1x3x70x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D72) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x322xf32, strides: [22540, 22540, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6625600, %D0 = "dma.tensor"(%R15, %B1088) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D73) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D73) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D73) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D73) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D73) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D73) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D73) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D73) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D73) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D73) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D73) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D73) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D73) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D73) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D73) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D73) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D73) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D73) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D73) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D73) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D73) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D73) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G81920, %D0 = "dma.tensor"(%R0, %B1122) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D74) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D74) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D74) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D74) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D74) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D74) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D74) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D74) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D74) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D74) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D74) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21331968, %D0 = "dma.tensor"(%R0, %B1139) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D75) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D75) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D75) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D75) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D75) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D75) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D75) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D75) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D75) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D75) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14359792, %B1140) : (memref<1x3x70x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D76) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x70x324xf32, strides: [22680, 22680, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6635520, %D0 = "dma.tensor"(%R15, %B1156) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D77) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D77) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D77) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D77) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D77) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D77) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D77) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D77) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D77) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D77) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D77) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D77) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D77) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D77) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D77) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D77) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D77) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D77) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D77) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D77) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D77) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D77) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G82240, %D0 = "dma.tensor"(%R0, %B1190) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D78) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D78) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D78) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D78) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D78) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D78) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D78) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D78) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D78) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D78) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D78) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21332288, %D0 = "dma.tensor"(%R0, %B1207) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D79) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D79) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D79) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D79) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D79) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D79) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D79) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D79) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D79) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D79) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14522368, %B1208) : (memref<1x3x68x322xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x68x322xf32, strides: [21896, 21896, 322, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D80) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 2, 2, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x68x322xf32, strides: [21896, 21896, 322, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %G6635840, %D0 = "dma.tensor"(%R15, %B1224) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1280, %B1 = "tsbc.s_bc"(%R0, %D81) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4736, %B1 = "arith.sub"(%C0.0, %R12, %D81) {round_mode = 0} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R5.4736, %C-3.4028198694267105e+35, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4736, %C1.4426950216293335, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D81) {round_mode = 3} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4736, %R3, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.cast"(%R0, %D81) {round_mode = 1} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.min"(%R5.4736, %C127, %D81) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "arith.max"(%R0, %C-127, %D81) {round_mode = 0} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, none) -> (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4736, %C127, %C23, %D81) {round_mode = 1} : (memref<1x32x33x160xsi16, strides: [5280, 5280, 160, 1]>, si16, ui8, none) -> (memref<1x32x33x160xsi32, strides: [5280, 5280, 160, 1]>, none), %R5.4736, %B1 = "sfu.taylor_4x"(%R3, %R8.1280, %D81) : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4736, %R0, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, f32, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D81) {iter = 3} : (f32, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D81) {round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none) -> (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D81) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x160xf32, strides: [5280, 5280, 160, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D81) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D81) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D81) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D81) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D81) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D81) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D81) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D81) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D81) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D81) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D81) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G92160, %D0 = "dma.tensor"(%R0, %B1258) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D82) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D82) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D82) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D82) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D82) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D82) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D82) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D82) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D82) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D82) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D82) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21342208, %D0 = "dma.tensor"(%R0, %B1275) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D83) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D83) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D83) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D83) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D83) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D83) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D83) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D83) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D83) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D83) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G14523632, %B1276) : (memref<1x3x68x324xf32, strides: [1228800, 409600, 640, 1]>, none) -> (memref<1x3x68x324xf32, strides: [22032, 22032, 324, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3296, %R11.3728, %C0.0, %D84) {kernel = [6, 6], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 2, 0, 2], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x3x68x324xf32, strides: [22032, 22032, 324, 1]>, memref<32x3x6x6xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %G6645760, %D0 = "dma.tensor"(%R15, %B1292) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R8.1568, %B1 = "tsbc.s_bc"(%R0, %D85) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4880, %B1 = "arith.sub"(%C0.0, %R12, %D85) {round_mode = 0} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R5.4880, %C-3.4028198694267105e+35, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R5.4880, %C1.4426950216293335, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.cast"(%R3, %D85) {round_mode = 3} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.sub"(%R5.4880, %R3, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "arith.cast"(%R0, %D85) {round_mode = 1} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.min"(%R5.4880, %C127, %D85) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R5.4880, %B1 = "arith.max"(%R0, %C-127, %D85) {round_mode = 0} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, none) -> (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4880, %C127, %C23, %D85) {round_mode = 1} : (memref<1x32x33x161xsi16, strides: [5320, 5320, 161, 1]>, si16, ui8, none) -> (memref<1x32x33x161xsi32, strides: [5316, 5316, 161, 1]>, none), %R5.4880, %B1 = "sfu.taylor_4x"(%R3, %R8.1568, %D85) : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4880, %R0, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R3, %B1 = "arith.add"(%R0, %C1.0, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, f32, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R3, %D85) {iter = 3} : (f32, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R12, %D85) {round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none) -> (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R11.608, %R15.5120, %C0.0, %D85) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x33x161xf32, strides: [5316, 5316, 161, 1]>, memref<64x32x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.4096, %B1 = "tsbc.s_bc"(%R0, %D85) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.2048, %B1 = "arith.sub"(%C0.0, %R5, %D85) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R3.2048, %C-3.4028198694267105e+35, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R3.2048, %C1.4426950216293335, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D85) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.sub"(%R3.2048, %R2, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.cast"(%R0, %D85) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R3.2048, %C127, %D85) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "arith.max"(%R0, %C-127, %D85) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.2048, %C127, %C23, %D85) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R3.2048, %B1 = "sfu.taylor_4x"(%R2, %R4.4096, %D85) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3.2048, %R0, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D85) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R5, %D85) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R11.2912, %R14.4880, %C0.0, %D85) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G92480, %D0 = "dma.tensor"(%R0, %B1326) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [1638400, 25600, 160, 1]>, none), %R2.2048, %B1 = "tsbc.s_bc"(%R0, %D86) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5120, %B1 = "arith.sub"(%C0.0, %R3, %D86) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R1.5120, %C-3.4028198694267105e+35, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R1.5120, %C1.4426950216293335, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R1, %D86) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.sub"(%R1.5120, %R1, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.cast"(%R0, %D86) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R1.5120, %C127, %D86) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "arith.max"(%R0, %C-127, %D86) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R1.5120, %C127, %C23, %D86) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R1.5120, %B1 = "sfu.taylor_4x"(%R1, %R2.2048, %D86) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5120, %R0, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R1, %B1 = "arith.add"(%R0, %C1.0, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R1, %D86) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D86) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R0, %R11.3168, %R14.4896, %C0.0, %D86) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<32x32x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %G21342528, %D0 = "dma.tensor"(%R0, %B1343) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R4.2336, %D0 = "dma.tensor"(%G155648, %B1343) : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R14.2048, %B1 = "tsbc.s_bc"(%R0, %D88) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.5120, %B1 = "arith.sub"(%C0.0, %R12, %D88) {round_mode = 0} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R13.5120, %C-3.4028198694267105e+35, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R13.5120, %C1.4426950216293335, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.cast"(%R13, %D88) {round_mode = 3} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.sub"(%R13.5120, %R13, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.cast"(%R15, %D88) {round_mode = 1} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.min"(%R13.5120, %C127, %D88) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "arith.max"(%R15, %C-127, %D88) {round_mode = 0} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, none) -> (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.add_satu"(%R13.5120, %C127, %C23, %D88) {round_mode = 1} : (memref<1x32x16x80xsi16, strides: [1280, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x32x16x80xsi32, strides: [1280, 1280, 80, 1]>, none), %R13.5120, %B1 = "sfu.taylor_4x"(%R13, %R14.2048, %D88) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R13.5120, %R15, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R13, %B1 = "arith.add"(%R15, %C1.0, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, f32, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R13, %D88) {iter = 3} : (f32, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R12, %D88) {round_mode = 0} : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6553600, %B1344) : (memref<1x32x55x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, none), %G6646080, %D0 = "dma.tensor"(%R15, %B1360) : (memref<1x32x16x80xf32, strides: [1280, 1280, 80, 1]>, none) -> (memref<1x32x16x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1920, %D0 = "dma.tensor"(%G118784, %B1360) : (memref<1x32x32x9xf32, strides: [9216, 288, 9, 1]>, none) -> (memref<1x32x32x9xf32, strides: [288, 288, 9, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D91) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R15.1792, %B1 = "tsbc.s_bc"(%R0, %D91) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.896, %B1 = "arith.sub"(%C0.0, %R5, %D91) {round_mode = 0} : (f32, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.max"(%R13.896, %C-3.4028198694267105e+35, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.896, %C1.4426950216293335, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D91) {round_mode = 3} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.896, %R11, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.cast"(%R8, %D91) {round_mode = 1} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.896, %C127, %D91) {round_mode = 0} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.max"(%R8, %C-127, %D91) {round_mode = 0} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.896, %C127, %C23, %D91) {round_mode = 1} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, ui8, none) -> (memref<1x32x54x80xsi32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "sfu.taylor_4x"(%R11, %R15.1792, %D91) : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.896, %R8, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D91) {iter = 3} : (f32, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D91) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21250048, %B1361) : (memref<1x32x54x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6553916, %B1361) : (memref<1x32x55x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D93) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [0, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [0, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D93) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %G17973248, %D0 = "dma.tensor"(%R11, %B1378) : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1792, %B1 = "tsbc.s_bc"(%R0, %D94) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.896, %B1 = "arith.sub"(%C0.0, %R5, %D94) {round_mode = 0} : (f32, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.max"(%R13.896, %C-3.4028198694267105e+35, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.896, %C1.4426950216293335, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D94) {round_mode = 3} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.896, %R11, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.cast"(%R8, %D94) {round_mode = 1} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.896, %C127, %D94) {round_mode = 0} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "arith.max"(%R8, %C-127, %D94) {round_mode = 0} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, none) -> (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.896, %C127, %C23, %D94) {round_mode = 1} : (memref<1x32x54x80xsi16, strides: [4320, 4320, 80, 1]>, si16, ui8, none) -> (memref<1x32x54x80xsi32, strides: [4320, 4320, 80, 1]>, none), %R13.896, %B1 = "sfu.taylor_4x"(%R11, %R15.1792, %D94) : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.896, %R8, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, f32, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D94) {iter = 3} : (f32, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D94) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21250368, %B1379) : (memref<1x32x54x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6587520, %B1379) : (memref<1x32x55x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D96) {round_mode = 0} : (memref<1x32x54x80xf32, strides: [0, 4320, 80, 1]>, memref<1x32x54x80xf32, strides: [0, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D96) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %G17973568, %D0 = "dma.tensor"(%R11, %B1396) : (memref<1x32x54x80xf32, strides: [4320, 4320, 80, 1]>, none) -> (memref<1x32x54x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1152, %B1 = "tsbc.s_bc"(%R0, %D97) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.576, %B1 = "arith.sub"(%C0.0, %R5, %D97) {round_mode = 0} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R13.576, %C-3.4028198694267105e+35, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.576, %C1.4426950216293335, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D97) {round_mode = 3} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.576, %R11, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.cast"(%R8, %D97) {round_mode = 1} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.576, %C127, %D97) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R8, %C-127, %D97) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.576, %C127, %C23, %D97) {round_mode = 1} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, ui8, none) -> (memref<1x32x53x80xsi32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "sfu.taylor_4x"(%R11, %R15.1152, %D97) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.576, %R8, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D97) {iter = 3} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D97) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21284608, %B1397) : (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6587836, %B1397) : (memref<1x32x55x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D99) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D99) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x55x81xf32, strides: [4456, 4456, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %G18007808, %D0 = "dma.tensor"(%R11, %B1414) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1152, %B1 = "tsbc.s_bc"(%R0, %D100) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.576, %B1 = "arith.sub"(%C0.0, %R5, %D100) {round_mode = 0} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R13.576, %C-3.4028198694267105e+35, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.576, %C1.4426950216293335, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D100) {round_mode = 3} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.576, %R11, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.cast"(%R8, %D100) {round_mode = 1} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.576, %C127, %D100) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R8, %C-127, %D100) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.576, %C127, %C23, %D100) {round_mode = 1} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, ui8, none) -> (memref<1x32x53x80xsi32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "sfu.taylor_4x"(%R11, %R15.1152, %D100) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.576, %R8, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D100) {iter = 3} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D100) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21284928, %B1415) : (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6621440, %B1415) : (memref<1x32x54x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x81xf32, strides: [4376, 4376, 81, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D102) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D102) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x54x81xf32, strides: [4376, 4376, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %G18008128, %D0 = "dma.tensor"(%R11, %B1432) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1152, %B1 = "tsbc.s_bc"(%R0, %D103) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.576, %B1 = "arith.sub"(%C0.0, %R5, %D103) {round_mode = 0} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R13.576, %C-3.4028198694267105e+35, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.576, %C1.4426950216293335, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D103) {round_mode = 3} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.576, %R11, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.cast"(%R8, %D103) {round_mode = 1} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.576, %C127, %D103) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R8, %C-127, %D103) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.576, %C127, %C23, %D103) {round_mode = 1} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, ui8, none) -> (memref<1x32x53x80xsi32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "sfu.taylor_4x"(%R11, %R15.1152, %D103) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.576, %R8, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D103) {iter = 3} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D103) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21318528, %B1433) : (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6621756, %B1433) : (memref<1x32x54x81xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x54x81xf32, strides: [4376, 4376, 81, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D105) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R0, %R15.1920, %R4.2336, %C0.0, %D105) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 0, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x32x54x81xf32, strides: [4376, 4376, 81, 1]>, memref<32x32x3x3xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %G18041728, %D0 = "dma.tensor"(%R11, %B1450) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none), %R15.1152, %B1 = "tsbc.s_bc"(%R0, %D106) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.576, %B1 = "arith.sub"(%C0.0, %R5, %D106) {round_mode = 0} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R13.576, %C-3.4028198694267105e+35, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R13.576, %C1.4426950216293335, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.cast"(%R11, %D106) {round_mode = 3} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.sub"(%R13.576, %R11, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.cast"(%R8, %D106) {round_mode = 1} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.min"(%R13.576, %C127, %D106) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "arith.max"(%R8, %C-127, %D106) {round_mode = 0} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, none) -> (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.576, %C127, %C23, %D106) {round_mode = 1} : (memref<1x32x53x80xsi16, strides: [4240, 4240, 80, 1]>, si16, ui8, none) -> (memref<1x32x53x80xsi32, strides: [4240, 4240, 80, 1]>, none), %R13.576, %B1 = "sfu.taylor_4x"(%R11, %R15.1152, %D106) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R13.576, %R8, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.add"(%R8, %C1.0, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, f32, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R11, %D106) {iter = 3} : (f32, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R5, %D106) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R2.1440, %D0 = "dma.tensor"(%G21318848, %B1451) : (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R11, %B1 = "arith.add"(%R2.1440, %R8, %D107) {round_mode = 0} : (memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, memref<1x32x53x80xf32, strides: [0, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none), %R7.5248, %D0 = "dma.tensor"(%G159744, %B1467) : (memref<1x32x64x1xf32, strides: [2048, 64, 1, 1]>, none) -> (memref<1x32x64x1xf32, strides: [64, 64, 1, 1]>, none), %R15.7808, %D0 = "dma.tensor"(%G167936, %B1467) : (memref<1x32x1x1xf32, strides: [32, 1, 1, 1]>, none) -> (memref<1x32x1x1xf32, strides: [1, 1, 1, 1]>, none), %R0, %D0 = "dma.tensor"(%G0, %B1468) : (memref<1x64x20x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D110) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %G18042048, %D0 = "dma.tensor"(%R11, %B1468) : (memref<1x32x53x80xf32, strides: [4240, 4240, 80, 1]>, none) -> (memref<1x32x53x80xf32, strides: [819200, 25600, 160, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D111) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D111) {round_mode = 0} : (f32, memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, f32, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, f32, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D111) {round_mode = 3} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, f32, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R9.1024, %D111) {round_mode = 1} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.4608, %C127, %D111) {round_mode = 0} : (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, si16, none) -> (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, none), %R1.4608, %B1 = "arith.max"(%R9.1024, %C-127, %D111) {round_mode = 0} : (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, si16, none) -> (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D111) {round_mode = 1} : (memref<1x32x20x160xsi16, strides: [3200, 3200, 160, 1]>, si16, ui8, none) -> (memref<1x32x20x160xsi32, strides: [3200, 3200, 160, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D111) : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.4608, %R9.1024, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, f32, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D111) {iter = 3} : (f32, memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D111) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G17973248, %B1469) : (memref<1x32x20x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D112) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R1.4608, %B1 = "arith.copy"(%R9.1024, %D112) {round_mode = 0} : (memref<1x32x20x160xf32, strides: [3200, 3200, 160, 1]>, none) -> (memref<1x32x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R15.6784, %D0 = "dma.tensor"(%G172032, %B1485) : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [128, 64, 1, 1]>, none), %R7.5536, %D0 = "dma.tensor"(%G188416, %B1485) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D114) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R15.3072, %B1 = "tsbc.s_bc"(%R0, %D114) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.2048, %B1 = "arith.sub"(%C0.0, %R4, %D114) {round_mode = 0} : (f32, memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R12.2048, %B1 = "arith.max"(%R12.2048, %C-3.4028198694267105e+35, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, f32, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.2048, %C1.4426950216293335, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, f32, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D114) {round_mode = 3} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, f32, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.2048, %R9.1024, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R12.2048, %B1 = "arith.cast"(%R0, %D114) {round_mode = 1} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.2048, %C127, %D114) {round_mode = 0} : (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, si16, none) -> (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, none), %R12.2048, %B1 = "arith.max"(%R0, %C-127, %D114) {round_mode = 0} : (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, si16, none) -> (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.2048, %C127, %C23, %D114) {round_mode = 1} : (memref<1x64x20x160xsi16, strides: [6400, 3200, 160, 1]>, si16, ui8, none) -> (memref<1x64x20x160xsi32, strides: [6400, 3200, 160, 1]>, none), %R12.2048, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.3072, %D114) : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.2048, %R0, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, f32, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D114) {iter = 3} : (f32, memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D114) {round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none) -> (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, none), %R8, %D0 = "dma.tensor"(%G192512, %B1488) : (memref<1x128x64x9xf32, strides: [73728, 576, 9, 1]>, none) -> (memref<1x128x64x9xf32, strides: [2304, 576, 9, 1]>, none), %R7.5504, %D0 = "dma.tensor"(%G487424, %B1488) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D116) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x20x160xf32, strides: [6400, 3200, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D116) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D116) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D116) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D116) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D116) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D116) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D116) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D116) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D116) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D116) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R15.5760, %D0 = "dma.tensor"(%G491520, %B1505) : (memref<1x64x128x1xf32, strides: [8192, 128, 1, 1]>, none) -> (memref<1x64x128x1xf32, strides: [256, 128, 1, 1]>, none), %R7.5520, %D0 = "dma.tensor"(%G524288, %B1505) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D118) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6553600, %D0 = "dma.tensor"(%R6, %B1521) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G12160, %B1521) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D120) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D120) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D120) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D120) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D120) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D120) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D120) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D120) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D120) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D120) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R15.7296, %D0 = "dma.tensor"(%G528384, %B1522) : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [128, 64, 1, 1]>, none), %R7.5552, %D0 = "dma.tensor"(%G544768, %B1522) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D122) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22888448, %D0 = "dma.tensor"(%R3.2304, %B1538) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D123) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D123) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D123) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D123) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D123) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D123) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D123) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D123) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D123) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D123) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D123) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21250048, %D0 = "dma.tensor"(%R6, %B1555) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D124) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D124) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D124) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D124) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D124) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D124) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D124) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D124) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D124) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D124) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G17985408, %B1556) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D125) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D125) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D125) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D125) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D125) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D125) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D125) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D125) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D125) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D125) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D125) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D125) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D125) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D125) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D125) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D125) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D125) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D125) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D125) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D125) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D125) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D125) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D125) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D125) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D125) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6556800, %D0 = "dma.tensor"(%R6, %B1608) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G24960, %B1608) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D127) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D127) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D127) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D127) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D127) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D127) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D127) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D127) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D127) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D127) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D127) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22891648, %D0 = "dma.tensor"(%R3.2304, %B1625) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D128) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D128) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D128) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D128) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D128) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D128) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D128) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D128) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D128) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D128) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D128) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21253248, %D0 = "dma.tensor"(%R6, %B1642) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D129) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D129) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D129) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D129) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D129) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D129) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D129) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D129) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D129) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D129) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G17998208, %B1643) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D130) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D130) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D130) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D130) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D130) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D130) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D130) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D130) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D130) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D130) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D130) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D130) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D130) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D130) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D130) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D130) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D130) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D130) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D130) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D130) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D130) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D130) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D130) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D130) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D130) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6560000, %D0 = "dma.tensor"(%R6, %B1695) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G37760, %B1695) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D132) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D132) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D132) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D132) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D132) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D132) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D132) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D132) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D132) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D132) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D132) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22894848, %D0 = "dma.tensor"(%R3.2304, %B1712) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D133) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D133) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D133) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D133) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D133) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D133) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D133) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D133) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D133) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D133) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D133) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21256448, %D0 = "dma.tensor"(%R6, %B1729) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D134) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D134) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D134) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D134) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D134) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D134) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D134) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D134) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D134) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D134) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G18011008, %B1730) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D135) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D135) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D135) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D135) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D135) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D135) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D135) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D135) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D135) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D135) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D135) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D135) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D135) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D135) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D135) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D135) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D135) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D135) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D135) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D135) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D135) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D135) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D135) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D135) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D135) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6563200, %D0 = "dma.tensor"(%R6, %B1782) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G50560, %B1782) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D137) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D137) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D137) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D137) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D137) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D137) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D137) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D137) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D137) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D137) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D137) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22898048, %D0 = "dma.tensor"(%R3.2304, %B1799) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D138) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D138) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D138) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D138) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D138) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D138) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D138) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D138) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D138) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D138) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D138) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21259648, %D0 = "dma.tensor"(%R6, %B1816) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D139) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D139) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D139) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D139) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D139) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D139) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D139) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D139) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D139) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D139) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G18023808, %B1817) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D140) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D140) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D140) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D140) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D140) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D140) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D140) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D140) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D140) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D140) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D140) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D140) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D140) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D140) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D140) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D140) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D140) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D140) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D140) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D140) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D140) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D140) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D140) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D140) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D140) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6566400, %D0 = "dma.tensor"(%R6, %B1869) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G63360, %B1869) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D142) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D142) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D142) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D142) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D142) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D142) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D142) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D142) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D142) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D142) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D142) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22901248, %D0 = "dma.tensor"(%R3.2304, %B1886) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D143) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D143) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D143) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D143) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D143) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D143) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D143) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D143) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D143) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D143) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D143) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21262848, %D0 = "dma.tensor"(%R6, %B1903) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D144) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D144) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D144) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D144) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D144) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D144) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D144) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D144) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D144) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D144) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G18036608, %B1904) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D145) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D145) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D145) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D145) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D145) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D145) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D145) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D145) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D145) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D145) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D145) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D145) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D145) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D145) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D145) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D145) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D145) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D145) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D145) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D145) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D145) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D145) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D145) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D145) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D145) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6569600, %D0 = "dma.tensor"(%R6, %B1956) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G76160, %B1956) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D147) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D147) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D147) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D147) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D147) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D147) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D147) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D147) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D147) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D147) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D147) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22904448, %D0 = "dma.tensor"(%R3.2304, %B1973) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D148) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D148) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D148) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D148) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D148) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D148) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D148) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D148) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D148) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D148) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D148) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21266048, %D0 = "dma.tensor"(%R6, %B1990) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D149) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D149) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D149) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D149) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D149) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D149) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D149) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D149) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D149) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D149) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G18049408, %B1991) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D150) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D150) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D150) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D150) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D150) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D150) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D150) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D150) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D150) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D150) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D150) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D150) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D150) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D150) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D150) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D150) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D150) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D150) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D150) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D150) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D150) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D150) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D150) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D150) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D150) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6572800, %D0 = "dma.tensor"(%R6, %B2043) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G88960, %B2043) : (memref<1x64x21x160xf32, strides: [1638400, 25600, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D152) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D152) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D152) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D152) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D152) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D152) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D152) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D152) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D152) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D152) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D152) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22907648, %D0 = "dma.tensor"(%R3.2304, %B2060) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D153) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D153) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D153) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D153) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D153) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D153) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D153) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D153) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D153) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D153) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R7.5248, %R15.7808, %C0.0, %D153) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<32x64x1x1xf32>, memref<1x32x1x1xui32, strides: [1, 1, 1, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %G21269248, %D0 = "dma.tensor"(%R6, %B2077) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R3.2304, %B1 = "tsbc.s_bc"(%R0, %D154) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.5248, %B1 = "arith.sub"(%C0.0, %R4, %D154) {round_mode = 0} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R1.5248, %C-3.4028198694267105e+35, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R1.5248, %C1.4426950216293335, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.cast"(%R0, %D154) {round_mode = 3} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R9.1024, %C0.6931471824645996, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.sub"(%R1.5248, %R0, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.cast"(%R9.1024, %D154) {round_mode = 1} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.min"(%R1.5248, %C127, %D154) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.max"(%R9.1024, %C-127, %D154) {round_mode = 0} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, none) -> (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add_satu"(%R1.5248, %C127, %C23, %D154) {round_mode = 1} : (memref<1x32x21x160xsi16, strides: [3360, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x32x21x160xsi32, strides: [3360, 3360, 160, 1]>, none), %R1.5248, %B1 = "sfu.taylor_4x"(%R0, %R3.2304, %D154) : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R1.5248, %R9.1024, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.add"(%R9.1024, %C1.0, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, f32, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.div"(%C1.0, %R0, %D154) {iter = 3} : (f32, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R9.1024, %R4, %D154) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R6, %D0 = "dma.tensor"(%G18062208, %B2078) : (memref<1x32x21x160xf32, strides: [819200, 25600, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D155) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R1.5248, %B1 = "arith.copy"(%R9.1024, %D155) {round_mode = 0} : (memref<1x32x21x160xf32, strides: [3360, 3360, 160, 1]>, none) -> (memref<1x32x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R15.6784, %R7.5536, %C0.0, %D155) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D155) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.3328, %B1 = "arith.sub"(%C0.0, %R4, %D155) {round_mode = 0} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R12.3328, %C-3.4028198694267105e+35, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R12.3328, %C1.4426950216293335, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.cast"(%R9.1024, %D155) {round_mode = 3} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.sub"(%R12.3328, %R9.1024, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.cast"(%R0, %D155) {round_mode = 1} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.min"(%R12.3328, %C127, %D155) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "arith.max"(%R0, %C-127, %D155) {round_mode = 0} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, none) -> (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.add_satu"(%R12.3328, %C127, %C23, %D155) {round_mode = 1} : (memref<1x64x21x160xsi16, strides: [6720, 3360, 160, 1]>, si16, ui8, none) -> (memref<1x64x21x160xsi32, strides: [6720, 3360, 160, 1]>, none), %R12.3328, %B1 = "sfu.taylor_4x"(%R9.1024, %R15.5632, %D155) : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R12.3328, %R0, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R9.1024, %B1 = "arith.add"(%R0, %C1.0, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, f32, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9.1024, %D155) {iter = 3} : (f32, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R4, %D155) {round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none) -> (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R8, %R7.5504, %C0.0, %D155) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x21x160xf32, strides: [6720, 3360, 160, 1]>, memref<128x64x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D155) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D155) {round_mode = 0} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D155) {round_mode = 3} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R6, %D155) {round_mode = 1} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.4608, %C127, %D155) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "arith.max"(%R6, %C-127, %D155) {round_mode = 0} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, none) -> (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D155) {round_mode = 1} : (memref<1x128x10x80xsi16, strides: [3200, 800, 80, 1]>, si16, ui8, none) -> (memref<1x128x10x80xsi32, strides: [3200, 800, 80, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D155) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.4608, %R6, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, f32, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D155) {iter = 3} : (f32, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4, %D155) {round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R6, %R15.5760, %R7.5520, %C0.0, %D155) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G6576000, %D0 = "dma.tensor"(%R6, %B2130) : (memref<1x128x10x80xf32, strides: [3200, 800, 80, 1]>, none) -> (memref<1x128x10x80xf32, strides: [819200, 6400, 80, 1]>, none), %R7.4608, %B1 = "tsbc.s_bc"(%R0, %D156) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.6400, %B1 = "arith.sub"(%C0.0, %R5, %D156) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R6.6400, %C-3.4028198694267105e+35, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6.6400, %C1.4426950216293335, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.cast"(%R6, %D156) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R3.2304, %C0.6931471824645996, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.sub"(%R6.6400, %R6, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.cast"(%R3.2304, %D156) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.min"(%R6.6400, %C127, %D156) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "arith.max"(%R3.2304, %C-127, %D156) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add_satu"(%R6.6400, %C127, %C23, %D156) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R6.6400, %B1 = "sfu.taylor_4x"(%R6, %R7.4608, %D156) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6.6400, %R3.2304, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add"(%R3.2304, %C1.0, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.div"(%C1.0, %R6, %D156) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R3.2304, %R5, %D156) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G21250048, %B2131) : (memref<1x64x28x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x28x80xf32, strides: [4480, 2240, 80, 1]>, none), %R5, %B1 = "conv.normal"(%R3.2304, %R15.7296, %R7.5552, %C0.0, %D157) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %G22910848, %D0 = "dma.tensor"(%R3.2304, %B2147) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R14, %D0 = "dma.tensor"(%G548864, %B2147) : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [1152, 576, 9, 1]>, none), %R4.6912, %B1 = "tsbc.s_bc"(%R0, %D159) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.512, %B1 = "arith.sub"(%C0.0, %R5, %D159) {round_mode = 0} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R4.512, %C-3.4028198694267105e+35, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R4.512, %C1.4426950216293335, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R3.2304, %D159) {round_mode = 3} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.sub"(%R4.512, %R3.2304, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.cast"(%R6, %D159) {round_mode = 1} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.min"(%R4.512, %C127, %D159) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "arith.max"(%R6, %C-127, %D159) {round_mode = 0} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, none) -> (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R4.512, %C127, %C23, %D159) {round_mode = 1} : (memref<1x64x10x80xsi16, strides: [1600, 800, 80, 1]>, si16, ui8, none) -> (memref<1x64x10x80xsi32, strides: [1600, 800, 80, 1]>, none), %R4.512, %B1 = "sfu.taylor_4x"(%R3.2304, %R4.6912, %D159) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R4.512, %R6, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R3.2304, %B1 = "arith.add"(%R6, %C1.0, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, f32, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R3.2304, %D159) {iter = 3} : (f32, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R5, %D159) {round_mode = 0} : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none), %R15, %D0 = "dma.tensor"(%G696320, %B2148) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R14, %R15, %C0.0, %D160) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x28x80xf32, strides: [4480, 2240, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %G21272448, %D0 = "dma.tensor"(%R6, %B2164) : (memref<1x64x10x80xf32, strides: [1600, 800, 80, 1]>, none) -> (memref<1x64x10x80xf32, strides: [409600, 6400, 80, 1]>, none), %R13.1792, %B1 = "tsbc.s_bc"(%R0, %D161) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.896, %B1 = "arith.sub"(%C0.0, %R3, %D161) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.max"(%R11.896, %C-3.4028198694267105e+35, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R11.896, %C1.4426950216293335, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R9, %D161) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R11.896, %R9, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.cast"(%R0, %D161) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.min"(%R11.896, %C127, %D161) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.max"(%R0, %C-127, %D161) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R11.896, %C127, %C23, %D161) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "sfu.taylor_4x"(%R9, %R13.1792, %D161) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R11.896, %R0, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.add"(%R0, %C1.0, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9, %D161) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D161) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G22888448, %B2165) : (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.add"(%R6, %R0, %D162) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [0, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [0, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R13.1920, %D0 = "dma.tensor"(%G700416, %B2181) : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [128, 64, 1, 1]>, none), %R14.4608, %D0 = "dma.tensor"(%G716800, %B2181) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R8, %B1 = "conv.normal"(%R3, %R13.1920, %R14.4608, %C0.0, %D164) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %G0, %D0 = "dma.tensor"(%R3, %B2182) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R7.1792, %B1 = "tsbc.s_bc"(%R0, %D165) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.896, %B1 = "arith.sub"(%C0.0, %R8, %D165) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.max"(%R5.896, %C-3.4028198694267105e+35, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R5.896, %C1.4426950216293335, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.cast"(%R3, %D165) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.sub"(%R5.896, %R3, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.cast"(%R11, %D165) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.min"(%R5.896, %C127, %D165) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.max"(%R11, %C-127, %D165) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.add_satu"(%R5.896, %C127, %C23, %D165) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "sfu.taylor_4x"(%R3, %R7.1792, %D165) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R5.896, %R11, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.add"(%R11, %C1.0, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R3, %D165) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R8, %D165) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G21258368, %B2183) : (memref<1x64x29x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x29x80xf32, strides: [4640, 2320, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R14, %R15, %C0.0, %D166) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x29x80xf32, strides: [4640, 2320, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %G9830400, %D0 = "dma.tensor"(%R11, %B2199) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R13.1792, %B1 = "tsbc.s_bc"(%R0, %D167) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.896, %B1 = "arith.sub"(%C0.0, %R3, %D167) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.max"(%R11.896, %C-3.4028198694267105e+35, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R11.896, %C1.4426950216293335, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R9, %D167) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R11.896, %R9, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.cast"(%R0, %D167) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.min"(%R11.896, %C127, %D167) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "arith.max"(%R0, %C-127, %D167) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R11.896, %C127, %C23, %D167) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R11.896, %B1 = "sfu.taylor_4x"(%R9, %R13.1792, %D167) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R11.896, %R0, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R9, %B1 = "arith.add"(%R0, %C1.0, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9, %D167) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D167) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G22897088, %B2200) : (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.add"(%R6, %R0, %D168) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [0, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [0, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R8, %B1 = "conv.normal"(%R3, %R13.1920, %R14.4608, %C0.0, %D168) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %G8640, %D0 = "dma.tensor"(%R3, %B2217) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R7.1792, %B1 = "tsbc.s_bc"(%R0, %D169) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.896, %B1 = "arith.sub"(%C0.0, %R8, %D169) {round_mode = 0} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.max"(%R5.896, %C-3.4028198694267105e+35, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R5.896, %C1.4426950216293335, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.cast"(%R3, %D169) {round_mode = 3} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.sub"(%R5.896, %R3, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.cast"(%R11, %D169) {round_mode = 1} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.min"(%R5.896, %C127, %D169) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "arith.max"(%R11, %C-127, %D169) {round_mode = 0} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, none) -> (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.add_satu"(%R5.896, %C127, %C23, %D169) {round_mode = 1} : (memref<1x64x27x80xsi16, strides: [4320, 2160, 80, 1]>, si16, ui8, none) -> (memref<1x64x27x80xsi32, strides: [4320, 2160, 80, 1]>, none), %R5.896, %B1 = "sfu.taylor_4x"(%R3, %R7.1792, %D169) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R5.896, %R11, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "arith.add"(%R11, %C1.0, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, f32, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R3, %D169) {iter = 3} : (f32, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R8, %D169) {round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G21267008, %B2218) : (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R14, %R15, %C0.0, %D170) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %G9839040, %D0 = "dma.tensor"(%R11, %B2234) : (memref<1x64x27x80xf32, strides: [4320, 2160, 80, 1]>, none) -> (memref<1x64x27x80xf32, strides: [409600, 6400, 80, 1]>, none), %R13.512, %B1 = "tsbc.s_bc"(%R0, %D171) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.256, %B1 = "arith.sub"(%C0.0, %R3, %D171) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11.256, %B1 = "arith.max"(%R11.256, %C-3.4028198694267105e+35, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R11.256, %C1.4426950216293335, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R9, %D171) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R11.256, %R9, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11.256, %B1 = "arith.cast"(%R0, %D171) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.min"(%R11.256, %C127, %D171) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R11.256, %B1 = "arith.max"(%R0, %C-127, %D171) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R11.256, %C127, %C23, %D171) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [4160, 2080, 80, 1]>, none), %R11.256, %B1 = "sfu.taylor_4x"(%R9, %R13.512, %D171) : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R11.256, %R0, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R9, %B1 = "arith.add"(%R0, %C1.0, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R9, %D171) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D171) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G22905728, %B2235) : (memref<1x64x26x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R14, %D0 = "dma.tensor"(%G720896, %B2235) : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [1152, 576, 9, 1]>, none), %R3, %B1 = "arith.add"(%R6, %R0, %D173) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [0, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [0, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R15, %D0 = "dma.tensor"(%G868352, %B2251) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R8, %B1 = "conv.normal"(%R3, %R13.1920, %R14.4608, %C0.0, %D174) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %G17280, %D0 = "dma.tensor"(%R3, %B2252) : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [409600, 6400, 80, 1]>, none), %R7.512, %B1 = "tsbc.s_bc"(%R0, %D175) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.256, %B1 = "arith.sub"(%C0.0, %R8, %D175) {round_mode = 0} : (f32, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1 = "arith.max"(%R5.256, %C-3.4028198694267105e+35, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R5.256, %C1.4426950216293335, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.cast"(%R3, %D175) {round_mode = 3} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R3, %B1 = "arith.sub"(%R5.256, %R3, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1 = "arith.cast"(%R11, %D175) {round_mode = 1} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.min"(%R5.256, %C127, %D175) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1 = "arith.max"(%R11, %C-127, %D175) {round_mode = 0} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, none) -> (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.add_satu"(%R5.256, %C127, %C23, %D175) {round_mode = 1} : (memref<1x64x26x80xsi16, strides: [4160, 2080, 80, 1]>, si16, ui8, none) -> (memref<1x64x26x80xsi32, strides: [4160, 2080, 80, 1]>, none), %R5.256, %B1 = "sfu.taylor_4x"(%R3, %R7.512, %D175) : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R5.256, %R11, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R3, %B1 = "arith.add"(%R11, %C1.0, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, f32, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R3, %D175) {iter = 3} : (f32, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R8, %D175) {round_mode = 0} : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G9830400, %B2269) : (memref<1x64x17x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R6, %R14, %R15, %C0.0, %D176) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %D0 = "dma.tensor"(%G0, %B2269) : (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G9847680, %D0 = "dma.tensor"(%R11, %B2269) : (memref<1x64x26x80xf32, strides: [4160, 2080, 80, 1]>, none) -> (memref<1x64x26x80xf32, strides: [409600, 6400, 80, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D178) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R9, %D178) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R3.6144, %D178) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.sub"(%R5, %R3.6144, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R7, %D178) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R5, %C127, %D178) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R7, %C-127, %D178) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D178) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R3.6144, %R6.2048, %D178) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R5, %R7, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.add"(%R7, %C1.0, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R3.6144, %D178) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R9, %D178) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6553600, %B2270) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R4.2048, %B1 = "arith.add"(%R2.4096, %R7, %D179) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R13.2176, %D0 = "dma.tensor"(%G872448, %B2286) : (memref<1x64x128x1xf32, strides: [8192, 128, 1, 1]>, none) -> (memref<1x64x128x1xf32, strides: [256, 128, 1, 1]>, none), %R14.4624, %D0 = "dma.tensor"(%G905216, %B2286) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.2176, %R14.4624, %C0.0, %D181) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D181) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R3, %D181) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D181) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R6, %D181) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.2048, %C127, %D181) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R6, %C-127, %D181) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D181) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D181) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.2048, %R6, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D181) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R3, %D181) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R13.128, %D0 = "dma.tensor"(%G909312, %B2288) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R0, %B1 = "arith.copy"(%R4.2048, %D182) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R6, %D182) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R14.4608, %D0 = "dma.tensor"(%G974848, %B2304) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.128, %R14.4608, %C0.0, %D183) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %B1 = "tsbc.s_bc"(%R0, %D183) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.4096, %B1 = "arith.sub"(%C0.0, %R3, %D183) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R10.4096, %C-3.4028198694267105e+35, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R10.4096, %C1.4426950216293335, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R8, %D183) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R10.4096, %R8, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.cast"(%R0, %D183) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R10.4096, %C127, %D183) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R0, %C-127, %D183) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R10.4096, %C127, %C23, %D183) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "sfu.taylor_4x"(%R8, %R13, %D183) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R10.4096, %R0, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R0, %C1.0, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R8, %D183) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D183) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G9835200, %B2307) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R6, %R14, %R15, %C0.0, %D184) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17973248, %D0 = "dma.tensor"(%R0, %B2323) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R2.4096, %D0 = "dma.tensor"(%G5120, %B2323) : (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D186) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R9, %D186) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R3.6144, %D186) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.sub"(%R5, %R3.6144, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R7, %D186) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R5, %C127, %D186) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R7, %C-127, %D186) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D186) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R3.6144, %R6.2048, %D186) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R5, %R7, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.add"(%R7, %C1.0, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R3.6144, %D186) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R9, %D186) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6558720, %B2324) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R4.2048, %B1 = "arith.add"(%R2.4096, %R7, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.2176, %R14.4624, %C0.0, %D187) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D187) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R3, %D187) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D187) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R6, %D187) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.2048, %C127, %D187) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R6, %C-127, %D187) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D187) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D187) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.2048, %R6, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D187) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R3, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R4.2048, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R6, %D187) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.128, %R14.4608, %C0.0, %D187) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %B1 = "tsbc.s_bc"(%R0, %D187) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.4096, %B1 = "arith.sub"(%C0.0, %R3, %D187) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R10.4096, %C-3.4028198694267105e+35, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R10.4096, %C1.4426950216293335, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R8, %D187) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R10.4096, %R8, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.cast"(%R0, %D187) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R10.4096, %C127, %D187) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R0, %C-127, %D187) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R10.4096, %C127, %C23, %D187) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "sfu.taylor_4x"(%R8, %R13, %D187) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R10.4096, %R0, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R0, %C1.0, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R8, %D187) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D187) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G9840320, %B2361) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R6, %R14, %R15, %C0.0, %D188) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17978368, %D0 = "dma.tensor"(%R0, %B2377) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R2.4096, %D0 = "dma.tensor"(%G10240, %B2377) : (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D190) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R9, %D190) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R3.6144, %D190) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.sub"(%R5, %R3.6144, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R7, %D190) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R5, %C127, %D190) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R7, %C-127, %D190) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D190) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R3.6144, %R6.2048, %D190) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R5, %R7, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.add"(%R7, %C1.0, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R3.6144, %D190) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R9, %D190) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6563840, %B2378) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R4.2048, %B1 = "arith.add"(%R2.4096, %R7, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.2176, %R14.4624, %C0.0, %D191) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D191) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R3, %D191) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D191) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R6, %D191) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.2048, %C127, %D191) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R6, %C-127, %D191) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D191) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D191) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.2048, %R6, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D191) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R3, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R4.2048, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R6, %D191) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.128, %R14.4608, %C0.0, %D191) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %B1 = "tsbc.s_bc"(%R0, %D191) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.4096, %B1 = "arith.sub"(%C0.0, %R3, %D191) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R10.4096, %C-3.4028198694267105e+35, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R10.4096, %C1.4426950216293335, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R8, %D191) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R10.4096, %R8, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.cast"(%R0, %D191) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R10.4096, %C127, %D191) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R0, %C-127, %D191) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R10.4096, %C127, %C23, %D191) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "sfu.taylor_4x"(%R8, %R13, %D191) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R10.4096, %R0, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R0, %C1.0, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R8, %D191) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D191) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G9845440, %B2415) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R6, %R14, %R15, %C0.0, %D192) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17983488, %D0 = "dma.tensor"(%R0, %B2431) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R2.4096, %D0 = "dma.tensor"(%G15360, %B2431) : (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D194) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R9, %D194) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R3.6144, %D194) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.sub"(%R5, %R3.6144, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R7, %D194) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R5, %C127, %D194) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R7, %C-127, %D194) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D194) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R3.6144, %R6.2048, %D194) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R5, %R7, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.add"(%R7, %C1.0, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R3.6144, %D194) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R9, %D194) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6568960, %B2432) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R4.2048, %B1 = "arith.add"(%R2.4096, %R7, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.2176, %R14.4624, %C0.0, %D195) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D195) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R3, %D195) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D195) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R6, %D195) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.2048, %C127, %D195) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R6, %C-127, %D195) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D195) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D195) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.2048, %R6, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D195) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R3, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R4.2048, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R6, %D195) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.128, %R14.4608, %C0.0, %D195) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %B1 = "tsbc.s_bc"(%R0, %D195) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.4096, %B1 = "arith.sub"(%C0.0, %R3, %D195) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R10.4096, %C-3.4028198694267105e+35, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R10.4096, %C1.4426950216293335, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R8, %D195) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R10.4096, %R8, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.cast"(%R0, %D195) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R10.4096, %C127, %D195) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R0, %C-127, %D195) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R10.4096, %C127, %C23, %D195) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "sfu.taylor_4x"(%R8, %R13, %D195) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R10.4096, %R0, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R0, %C1.0, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R8, %D195) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D195) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G9850560, %B2469) : (memref<1x64x17x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R6, %R14, %R15, %C0.0, %D196) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17988608, %D0 = "dma.tensor"(%R0, %B2485) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R2.4096, %D0 = "dma.tensor"(%G20480, %B2485) : (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D198) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R9, %D198) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R3.6144, %D198) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.sub"(%R5, %R3.6144, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R7, %D198) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R5, %C127, %D198) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.max"(%R7, %C-127, %D198) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D198) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R3.6144, %R6.2048, %D198) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R5, %R7, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3.6144, %B1 = "arith.add"(%R7, %C1.0, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R3.6144, %D198) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R9, %D198) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6574080, %B2486) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R4.2048, %B1 = "arith.add"(%R2.4096, %R7, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [0, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.2176, %R14.4624, %C0.0, %D199) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<64x128x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D199) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R3, %D199) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R0, %D199) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R6, %D199) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R1.2048, %C127, %D199) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R6, %C-127, %D199) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D199) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D199) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R1.2048, %R6, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R6, %C1.0, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R0, %D199) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R3, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R4.2048, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R6, %D199) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R15, %D0 = "dma.tensor"(%G2158592, %B2520) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R0, %R13.128, %R14.4608, %C0.0, %D200) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %B1 = "tsbc.s_bc"(%R0, %D200) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.4096, %B1 = "arith.sub"(%C0.0, %R3, %D200) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R10.4096, %C-3.4028198694267105e+35, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R10.4096, %C1.4426950216293335, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.cast"(%R8, %D200) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R10.4096, %R8, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.cast"(%R0, %D200) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.min"(%R10.4096, %C127, %D200) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "arith.max"(%R0, %C-127, %D200) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add_satu"(%R10.4096, %C127, %C23, %D200) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R10.4096, %B1 = "sfu.taylor_4x"(%R8, %R13, %D200) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R10.4096, %R0, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R0, %C1.0, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R8, %D200) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R3, %D200) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %G17993728, %D0 = "dma.tensor"(%R0, %B2539) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R5, %D0 = "dma.tensor"(%G17973248, %B2539) : (memref<1x128x20x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x20x80xf32, strides: [6400, 1600, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G978944, %B2539) : (memref<1x256x128x9xf32, strides: [294912, 1152, 9, 1]>, none) -> (memref<1x256x128x9xf32, strides: [9216, 1152, 9, 1]>, none), %R9, %B1 = "conv.normal"(%R5, %R0, %R15, %C0.0, %D203) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x80xf32, strides: [6400, 1600, 80, 1]>, memref<256x128x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R7.5120, %B1 = "tsbc.s_bc"(%R0, %D203) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.512, %B1 = "arith.sub"(%C0.0, %R9, %D203) {round_mode = 0} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R6.512, %C-3.4028198694267105e+35, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R6.512, %C1.4426950216293335, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4.4096, %D203) {round_mode = 3} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.sub"(%R6.512, %R4.4096, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.cast"(%R11, %D203) {round_mode = 1} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.min"(%R6.512, %C127, %D203) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R11, %C-127, %D203) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R6.512, %C127, %C23, %D203) {round_mode = 1} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, ui8, none) -> (memref<1x256x10x40xsi32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "sfu.taylor_4x"(%R4.4096, %R7.5120, %D203) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R6.512, %R11, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.add"(%R11, %C1.0, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4.4096, %D203) {iter = 3} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R9, %D203) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R14, %D0 = "dma.tensor"(%G2162688, %B2540) : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R8.2304, %D0 = "dma.tensor"(%G2293760, %B2540) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R13, %B1 = "conv.normal"(%R11, %R14, %R8.2304, %C0.0, %D205) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G0, %D0 = "dma.tensor"(%R11, %B2556) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none), %R5, %D0 = "dma.tensor"(%G17979328, %B2556) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R5, %R0, %R15, %C0.0, %D207) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<256x128x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %G1638400, %D0 = "dma.tensor"(%R13, %B2557) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7.5120, %B1 = "tsbc.s_bc"(%R0, %D208) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.512, %B1 = "arith.sub"(%C0.0, %R9, %D208) {round_mode = 0} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R6.512, %C-3.4028198694267105e+35, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R6.512, %C1.4426950216293335, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4.4096, %D208) {round_mode = 3} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.sub"(%R6.512, %R4.4096, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.cast"(%R11, %D208) {round_mode = 1} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.min"(%R6.512, %C127, %D208) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R11, %C-127, %D208) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R6.512, %C127, %C23, %D208) {round_mode = 1} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, ui8, none) -> (memref<1x256x10x40xsi32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "sfu.taylor_4x"(%R4.4096, %R7.5120, %D208) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R6.512, %R11, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.add"(%R11, %C1.0, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4.4096, %D208) {iter = 3} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R9, %D208) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R13, %B1 = "conv.normal"(%R11, %R14, %R8.2304, %C0.0, %D208) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G1600, %D0 = "dma.tensor"(%R11, %B2574) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none), %R5, %D0 = "dma.tensor"(%G17985728, %B2574) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R5, %R0, %R15, %C0.0, %D210) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<256x128x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %G1640000, %D0 = "dma.tensor"(%R13, %B2575) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7.5120, %B1 = "tsbc.s_bc"(%R0, %D211) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.512, %B1 = "arith.sub"(%C0.0, %R9, %D211) {round_mode = 0} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R6.512, %C-3.4028198694267105e+35, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R6.512, %C1.4426950216293335, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4.4096, %D211) {round_mode = 3} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.sub"(%R6.512, %R4.4096, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.cast"(%R11, %D211) {round_mode = 1} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.min"(%R6.512, %C127, %D211) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R11, %C-127, %D211) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R6.512, %C127, %C23, %D211) {round_mode = 1} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, ui8, none) -> (memref<1x256x10x40xsi32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "sfu.taylor_4x"(%R4.4096, %R7.5120, %D211) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R6.512, %R11, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.add"(%R11, %C1.0, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4.4096, %D211) {iter = 3} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R9, %D211) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R13, %B1 = "conv.normal"(%R11, %R14, %R8.2304, %C0.0, %D211) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G3200, %D0 = "dma.tensor"(%R11, %B2592) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none), %R5, %D0 = "dma.tensor"(%G17992128, %B2592) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R9, %B1 = "conv.normal"(%R5, %R0, %R15, %C0.0, %D213) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<256x128x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %G1641600, %D0 = "dma.tensor"(%R13, %B2593) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7.5120, %B1 = "tsbc.s_bc"(%R0, %D214) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.512, %B1 = "arith.sub"(%C0.0, %R9, %D214) {round_mode = 0} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R6.512, %C-3.4028198694267105e+35, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R6.512, %C1.4426950216293335, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4.4096, %D214) {round_mode = 3} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.sub"(%R6.512, %R4.4096, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.cast"(%R11, %D214) {round_mode = 1} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.min"(%R6.512, %C127, %D214) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "arith.max"(%R11, %C-127, %D214) {round_mode = 0} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, none) -> (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R6.512, %C127, %C23, %D214) {round_mode = 1} : (memref<1x256x10x40xsi16, strides: [3200, 400, 40, 1]>, si16, ui8, none) -> (memref<1x256x10x40xsi32, strides: [3200, 400, 40, 1]>, none), %R6.512, %B1 = "sfu.taylor_4x"(%R4.4096, %R7.5120, %D214) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R6.512, %R11, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R4.4096, %B1 = "arith.add"(%R11, %C1.0, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, f32, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4.4096, %D214) {iter = 3} : (f32, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R9, %D214) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R2, %D0 = "dma.tensor"(%G1638400, %B2594) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "conv.normal"(%R11, %R14, %R8.2304, %C0.0, %D215) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G4800, %D0 = "dma.tensor"(%R11, %B2610) : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none), %R10.1024, %B1 = "tsbc.s_bc"(%R0, %D216) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4608, %B1 = "arith.sub"(%C0.0, %R2, %D216) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.max"(%R8.4608, %C-3.4028198694267105e+35, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.mul"(%R8.4608, %C1.4426950216293335, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R7, %D216) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.sub"(%R8.4608, %R7, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.cast"(%R0, %D216) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R8.4608, %C127, %D216) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.max"(%R0, %C-127, %D216) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R8.4608, %C127, %C23, %D216) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "sfu.taylor_4x"(%R7, %R10.1024, %D216) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R8.4608, %R0, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.add"(%R0, %C1.0, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R7, %D216) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R2, %D216) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %D0 = "dma.tensor"(%G2297856, %B2611) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R11, %D0 = "dma.tensor"(%G2363392, %B2611) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %G1643200, %D0 = "dma.tensor"(%R13, %B2611) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R6, %R11, %C0.0, %D219) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G4096000, %D0 = "dma.tensor"(%R0, %B2627) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R2, %D0 = "dma.tensor"(%G1641600, %B2627) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.1024, %B1 = "tsbc.s_bc"(%R0, %D221) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4608, %B1 = "arith.sub"(%C0.0, %R2, %D221) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.max"(%R8.4608, %C-3.4028198694267105e+35, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.mul"(%R8.4608, %C1.4426950216293335, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R7, %D221) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.sub"(%R8.4608, %R7, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.cast"(%R0, %D221) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R8.4608, %C127, %D221) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.max"(%R0, %C-127, %D221) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R8.4608, %C127, %C23, %D221) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R8.4608, %B1 = "sfu.taylor_4x"(%R7, %R10.1024, %D221) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R8.4608, %R0, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.add"(%R0, %C1.0, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R7, %D221) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R2, %D221) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G3276800, %D0 = "dma.tensor"(%R4, %B2628) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R6, %R11, %C0.0, %D222) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G4099200, %D0 = "dma.tensor"(%R0, %B2644) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %G3280000, %D0 = "dma.tensor"(%R4, %B2645) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R9, %D0 = "dma.tensor"(%G3276800, %B2645) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R8.2304, %B1 = "tsbc.s_bc"(%R0, %D225) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.5248, %B1 = "arith.sub"(%C0.0, %R9, %D225) {round_mode = 0} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R6.5248, %C-3.4028198694267105e+35, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R6.5248, %C1.4426950216293335, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.cast"(%R5, %D225) {round_mode = 3} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R6.5248, %R5, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.cast"(%R3, %D225) {round_mode = 1} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.min"(%R6.5248, %C127, %D225) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R3, %C-127, %D225) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.add_satu"(%R6.5248, %C127, %C23, %D225) {round_mode = 1} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, ui8, none) -> (memref<1x128x21x40xsi32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "sfu.taylor_4x"(%R5, %R8.2304, %D225) : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R6.5248, %R3, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.add"(%R3, %C1.0, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R5, %D225) {iter = 3} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R9, %D225) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G2367488, %B2645) : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R12.6656, %D0 = "dma.tensor"(%G2957312, %B2645) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R6, %B1 = "conv.normal"(%R3, %R0, %R12.6656, %C0.0, %D227) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11.1024, %B1 = "tsbc.s_bc"(%R0, %D227) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.4608, %B1 = "arith.sub"(%C0.0, %R6, %D227) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.max"(%R9.4608, %C-3.4028198694267105e+35, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R9.4608, %C1.4426950216293335, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.cast"(%R8, %D227) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R4, %C0.6931471824645996, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.sub"(%R9.4608, %R8, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.cast"(%R4, %D227) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.min"(%R9.4608, %C127, %D227) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.max"(%R4, %C-127, %D227) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add_satu"(%R9.4608, %C127, %C23, %D227) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "sfu.taylor_4x"(%R8, %R11.1024, %D227) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R9.4608, %R4, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.add"(%R4, %C1.0, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.div"(%C1.0, %R8, %D227) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R4, %R6, %D227) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G4096000, %B2662) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R2.2048, %R4, %D228) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %D0 = "dma.tensor"(%G2961408, %B2678) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R13, %D0 = "dma.tensor"(%G3026944, %B2678) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R2.2048, %B1 = "conv.normal"(%R6, %R12.4608, %R13, %C0.0, %D230) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G2457600, %D0 = "dma.tensor"(%R6, %B2679) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D231) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R2.2048, %D231) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4, %D231) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R11, %D231) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.min"(%R5.4608, %C127, %D231) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R11, %C-127, %D231) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D231) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D231) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R5.4608, %R11, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R11, %C1.0, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4, %D231) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R2.2048, %D231) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %D0 = "dma.tensor"(%G3279840, %B2680) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R8.2304, %B1 = "tsbc.s_bc"(%R0, %D232) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.5248, %B1 = "arith.sub"(%C0.0, %R9, %D232) {round_mode = 0} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R6.5248, %C-3.4028198694267105e+35, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R6.5248, %C1.4426950216293335, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.cast"(%R5, %D232) {round_mode = 3} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R6.5248, %R5, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.cast"(%R3, %D232) {round_mode = 1} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.min"(%R6.5248, %C127, %D232) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R3, %C-127, %D232) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.add_satu"(%R6.5248, %C127, %C23, %D232) {round_mode = 1} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, ui8, none) -> (memref<1x128x21x40xsi32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "sfu.taylor_4x"(%R5, %R8.2304, %D232) : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R6.5248, %R3, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.add"(%R3, %C1.0, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R5, %D232) {iter = 3} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R9, %D232) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %G1638400, %D0 = "dma.tensor"(%R11, %B2696) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R6, %B1 = "conv.normal"(%R3, %R0, %R12.6656, %C0.0, %D233) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11.1024, %B1 = "tsbc.s_bc"(%R0, %D233) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.4608, %B1 = "arith.sub"(%C0.0, %R6, %D233) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.max"(%R9.4608, %C-3.4028198694267105e+35, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R9.4608, %C1.4426950216293335, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.cast"(%R8, %D233) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R4, %C0.6931471824645996, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.sub"(%R9.4608, %R8, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.cast"(%R4, %D233) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.min"(%R9.4608, %C127, %D233) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "arith.max"(%R4, %C-127, %D233) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add_satu"(%R9.4608, %C127, %C23, %D233) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %B1 = "sfu.taylor_4x"(%R8, %R11.1024, %D233) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R9.4608, %R4, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %B1 = "arith.add"(%R4, %C1.0, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.div"(%C1.0, %R8, %D233) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R4, %R6, %D233) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G4099200, %B2713) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R2.2048, %R4, %D234) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "conv.normal"(%R6, %R12.4608, %R13, %C0.0, %D234) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G2460800, %D0 = "dma.tensor"(%R6, %B2730) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D235) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R2.2048, %D235) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.cast"(%R4, %D235) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R11, %C0.6931471824645996, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R11, %D235) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.min"(%R5.4608, %C127, %D235) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R11, %C-127, %D235) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D235) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D235) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R5.4608, %R11, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R11, %C1.0, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.div"(%C1.0, %R4, %D235) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "arith.mul"(%R11, %R2.2048, %D235) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G1641600, %D0 = "dma.tensor"(%R11, %B2747) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R3, %D0 = "dma.tensor"(%G1638400, %B2747) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G3031040, %B2747) : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R12, %D0 = "dma.tensor"(%G3620864, %B2747) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R10, %B1 = "conv.normal"(%R3, %R0, %R12, %C0.0, %D239) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.1024, %B1 = "tsbc.s_bc"(%R0, %D239) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.4608, %B1 = "arith.sub"(%C0.0, %R10, %D239) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R7.4608, %C-3.4028198694267105e+35, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R7.4608, %C1.4426950216293335, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.cast"(%R6, %D239) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R4, %C0.6931471824645996, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.sub"(%R7.4608, %R6, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.cast"(%R4, %D239) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.min"(%R7.4608, %C127, %D239) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R4, %C-127, %D239) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add_satu"(%R7.4608, %C127, %C23, %D239) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "sfu.taylor_4x"(%R6, %R9.1024, %D239) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R7.4608, %R4, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R4, %C1.0, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.div"(%C1.0, %R6, %D239) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R4, %R10, %D239) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G2457600, %B2748) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.add"(%R2.2048, %R4, %D240) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.1152, %D0 = "dma.tensor"(%G3624960, %B2764) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G3690496, %B2764) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R5, %B1 = "conv.normal"(%R7, %R9.1152, %R11.4608, %C0.0, %D242) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G4096000, %D0 = "dma.tensor"(%R7, %B2765) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R3, %D0 = "dma.tensor"(%G1641440, %B2765) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R3, %R0, %R12, %C0.0, %D244) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G3276800, %D0 = "dma.tensor"(%R5, %B2766) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R9.1024, %B1 = "tsbc.s_bc"(%R0, %D245) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.4608, %B1 = "arith.sub"(%C0.0, %R10, %D245) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R7.4608, %C-3.4028198694267105e+35, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R7.4608, %C1.4426950216293335, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.cast"(%R6, %D245) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R4, %C0.6931471824645996, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.sub"(%R7.4608, %R6, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.cast"(%R4, %D245) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.min"(%R7.4608, %C127, %D245) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R4, %C-127, %D245) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add_satu"(%R7.4608, %C127, %C23, %D245) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "sfu.taylor_4x"(%R6, %R9.1024, %D245) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R7.4608, %R4, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R4, %C1.0, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.div"(%C1.0, %R6, %D245) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R4, %R10, %D245) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G2460800, %B2767) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.add"(%R2.2048, %R4, %D246) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5, %B1 = "conv.normal"(%R7, %R9.1152, %R11.4608, %C0.0, %D246) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G4099200, %D0 = "dma.tensor"(%R7, %B2784) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %G3280000, %D0 = "dma.tensor"(%R5, %B2785) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R9, %D0 = "dma.tensor"(%G3276800, %B2785) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R8.2304, %B1 = "tsbc.s_bc"(%R0, %D249) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.5248, %B1 = "arith.sub"(%C0.0, %R9, %D249) {round_mode = 0} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R6.5248, %C-3.4028198694267105e+35, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R6.5248, %C1.4426950216293335, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.cast"(%R5, %D249) {round_mode = 3} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R6.5248, %R5, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.cast"(%R3, %D249) {round_mode = 1} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.min"(%R6.5248, %C127, %D249) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R3, %C-127, %D249) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.add_satu"(%R6.5248, %C127, %C23, %D249) {round_mode = 1} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, ui8, none) -> (memref<1x128x21x40xsi32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "sfu.taylor_4x"(%R5, %R8.2304, %D249) : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R6.5248, %R3, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.add"(%R3, %C1.0, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R5, %D249) {iter = 3} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R9, %D249) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G3694592, %B2785) : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R14.4608, %D0 = "dma.tensor"(%G4284416, %B2785) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R11, %B1 = "conv.normal"(%R3, %R0, %R14.4608, %C0.0, %D251) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.3072, %D0 = "dma.tensor"(%G4096000, %B2801) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.512, %B1 = "tsbc.s_bc"(%R0, %D252) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4096, %B1 = "arith.sub"(%C0.0, %R11, %D252) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.max"(%R8.4096, %C-3.4028198694267105e+35, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.mul"(%R8.4096, %C1.4426950216293335, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.cast"(%R6.7680, %D252) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.mul"(%R13, %C0.6931471824645996, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.sub"(%R8.4096, %R6.7680, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.cast"(%R13, %D252) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.min"(%R8.4096, %C127, %D252) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.max"(%R13, %C-127, %D252) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.add_satu"(%R8.4096, %C127, %C23, %D252) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "sfu.taylor_4x"(%R6.7680, %R10.512, %D252) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.mul"(%R8.4096, %R13, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.add"(%R13, %C1.0, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.div"(%C1.0, %R6.7680, %D252) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.mul"(%R13, %R11, %D252) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G0, %B2802) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R15, %D0 = "dma.tensor"(%G4288512, %B2802) : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R7, %B1 = "arith.add"(%R5.3072, %R13, %D254) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.5248, %D0 = "dma.tensor"(%G4419584, %B2818) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R11, %B1 = "conv.normal"(%R2.2048, %R15, %R10.5248, %C0.0, %D255) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G2457600, %D0 = "dma.tensor"(%R7, %B2819) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R9, %D0 = "dma.tensor"(%G3279840, %B2819) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R8.2304, %B1 = "tsbc.s_bc"(%R0, %D257) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R6.5248, %B1 = "arith.sub"(%C0.0, %R9, %D257) {round_mode = 0} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R6.5248, %C-3.4028198694267105e+35, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R6.5248, %C1.4426950216293335, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.cast"(%R5, %D257) {round_mode = 3} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R6.5248, %R5, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.cast"(%R3, %D257) {round_mode = 1} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.min"(%R6.5248, %C127, %D257) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "arith.max"(%R3, %C-127, %D257) {round_mode = 0} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, none) -> (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.add_satu"(%R6.5248, %C127, %C23, %D257) {round_mode = 1} : (memref<1x128x21x40xsi16, strides: [3360, 840, 40, 1]>, si16, ui8, none) -> (memref<1x128x21x40xsi32, strides: [3360, 840, 40, 1]>, none), %R6.5248, %B1 = "sfu.taylor_4x"(%R5, %R8.2304, %D257) : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R6.5248, %R3, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R5, %B1 = "arith.add"(%R3, %C1.0, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, f32, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R5, %D257) {iter = 3} : (f32, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R9, %D257) {round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %G1638400, %D0 = "dma.tensor"(%R11, %B2820) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R11, %B1 = "conv.normal"(%R3, %R0, %R14.4608, %C0.0, %D258) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.3072, %D0 = "dma.tensor"(%G4099200, %B2836) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.512, %B1 = "tsbc.s_bc"(%R0, %D259) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4096, %B1 = "arith.sub"(%C0.0, %R11, %D259) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.max"(%R8.4096, %C-3.4028198694267105e+35, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.mul"(%R8.4096, %C1.4426950216293335, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.cast"(%R6.7680, %D259) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.mul"(%R13, %C0.6931471824645996, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.sub"(%R8.4096, %R6.7680, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.cast"(%R13, %D259) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.min"(%R8.4096, %C127, %D259) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "arith.max"(%R13, %C-127, %D259) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.add_satu"(%R8.4096, %C127, %C23, %D259) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R8.4096, %B1 = "sfu.taylor_4x"(%R6.7680, %R10.512, %D259) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.mul"(%R8.4096, %R13, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.7680, %B1 = "arith.add"(%R13, %C1.0, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.div"(%C1.0, %R6.7680, %D259) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R13, %B1 = "arith.mul"(%R13, %R11, %D259) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G3200, %B2837) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R7, %B1 = "arith.add"(%R5.3072, %R13, %D260) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [0, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11, %B1 = "conv.normal"(%R2.2048, %R15, %R10.5248, %C0.0, %D260) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G2460800, %D0 = "dma.tensor"(%R7, %B2854) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %G1641600, %D0 = "dma.tensor"(%R11, %B2855) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R11, %D0 = "dma.tensor"(%G1638400, %B2855) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D263) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R11, %D263) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D263) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R9, %D263) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.min"(%R1.4608, %C127, %D263) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.max"(%R9, %C-127, %D263) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D263) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D263) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R1.4608, %R9, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D263) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R11, %D263) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %D0 = "dma.tensor"(%G2457600, %B2855) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.copy"(%R7.1024, %D264) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.copy"(%R9, %D264) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R13, %D0 = "dma.tensor"(%G4423680, %B2871) : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R14, %D0 = "dma.tensor"(%G4685824, %B2871) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R13, %R14, %C0.0, %D266) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R11, %D0 = "dma.tensor"(%G1641600, %B2873) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D267) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R11, %D267) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D267) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R9, %D267) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.min"(%R1.4608, %C127, %D267) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.max"(%R9, %C-127, %D267) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D267) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D267) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R1.4608, %R9, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D267) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R11, %D267) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %D0 = "dma.tensor"(%G2460800, %B2874) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G0, %D0 = "dma.tensor"(%R4, %B2874) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R0, %B1 = "arith.copy"(%R7.1024, %D269) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.copy"(%R9, %D269) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R13, %R14, %C0.0, %D269) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %G3200, %D0 = "dma.tensor"(%R4, %B2893) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R0, %D0 = "dma.matrix"(%G0, %B2893) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D271) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D271) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D271) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D271) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D271) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D271) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D271) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D271) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D271) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D271) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R1, %D0 = "dma.matrix"(%G262144, %B2893) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D272) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D272) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D272) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D272) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D272) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D272) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D272) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D272) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D272) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D272) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G9830400, %D0 = "dma.matrix"(%R2, %B2909) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G524288, %B2909) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D274) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D274) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D274) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D274) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D274) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D274) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D274) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D274) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D274) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D274) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G10092544, %D0 = "dma.matrix"(%R3, %B2925) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R1, %D0 = "dma.matrix"(%G786432, %B2925) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D276) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D276) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D276) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D276) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D276) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D276) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D276) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D276) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D276) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D276) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G10354688, %D0 = "dma.matrix"(%R2, %B2941) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G1048576, %B2941) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D278) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D278) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D278) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D278) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D278) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D278) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D278) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D278) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D278) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D278) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G10616832, %D0 = "dma.matrix"(%R3, %B2957) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R1, %D0 = "dma.matrix"(%G1310720, %B2957) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D280) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D280) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D280) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D280) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D280) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D280) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D280) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D280) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D280) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D280) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G10878976, %D0 = "dma.matrix"(%R2, %B2973) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G1572864, %B2973) : (memref<1x1x1x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<1x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D282) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D282) {round_mode = 0} : (f32, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D282) {round_mode = 3} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D282) {round_mode = 1} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D282) {round_mode = 0} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D282) {round_mode = 0} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D282) {round_mode = 1} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<1x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D282) : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D282) {iter = 3} : (f32, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D282) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G11141120, %D0 = "dma.matrix"(%R3, %B2989) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %G11403264, %D0 = "dma.matrix"(%R2, %B3005) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.tensor"(%G4689920, %B3005) : (memref<1x256x2304x1xf32, strides: [589824, 2304, 1, 1]>, none) -> (memref<1x256x2304x1xf32, strides: [18432, 2304, 1, 1]>, none), %R9, %D0 = "dma.tensor"(%G9408512, %B3005) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10, %D0 = "dma.tensor"(%G9830400, %B3005) : (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R14, %B1 = "conv.normal"(%R10, %R0, %R9, %C0.0, %D287) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R12, %D0 = "dma.tensor"(%G9831840, %B3005) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R15, %B1 = "conv.normal"(%R12, %R0, %R9, %C0.0, %D288) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G0, %D0 = "dma.tensor"(%R14, %B3006) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R10, %D0 = "dma.tensor"(%G9833440, %B3006) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R14, %B1 = "conv.normal"(%R10, %R0, %R9, %C0.0, %D290) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G400, %D0 = "dma.tensor"(%R15, %B3007) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R12, %D0 = "dma.tensor"(%G9835040, %B3007) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R15, %B1 = "conv.normal"(%R12, %R0, %R9, %C0.0, %D292) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G800, %D0 = "dma.tensor"(%R14, %B3008) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %G1200, %D0 = "dma.tensor"(%R15, %B3009) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G7049216, %B3009) : (memref<1x256x2304x1xf32, strides: [589824, 2304, 1, 1]>, none) -> (memref<1x256x2304x1xf32, strides: [18432, 2304, 1, 1]>, none), %R9, %D0 = "dma.tensor"(%G9409536, %B3009) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10, %D0 = "dma.tensor"(%G9830400, %B3009) : (memref<1x256x10x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, none), %R14, %B1 = "conv.normal"(%R10, %R0, %R9, %C0.0, %D297) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x10x40xf32, strides: [3200, 400, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R12, %D0 = "dma.tensor"(%G9831840, %B3009) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R15, %B1 = "conv.normal"(%R12, %R0, %R9, %C0.0, %D298) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G409600, %D0 = "dma.tensor"(%R14, %B3010) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R10, %D0 = "dma.tensor"(%G9833440, %B3010) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R14, %B1 = "conv.normal"(%R10, %R0, %R9, %C0.0, %D300) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G410000, %D0 = "dma.tensor"(%R15, %B3011) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R12, %D0 = "dma.tensor"(%G9835040, %B3011) : (memref<1x256x11x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, none), %R15, %B1 = "conv.normal"(%R12, %R0, %R9, %C0.0, %D302) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x11x40xf32, strides: [3520, 440, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %G410400, %D0 = "dma.tensor"(%R14, %B3012) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %G410800, %D0 = "dma.tensor"(%R15, %B3013) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [204800, 400, 20, 1]>, none), %R4, %D0 = "dma.tensor"(%G0, %B3013) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10.1024, %B1 = "tsbc.s_bc"(%R0, %D305) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4608, %B1 = "arith.sub"(%C0.0, %R4, %D305) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.max"(%R8.4608, %C-3.4028198694267105e+35, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.mul"(%R8.4608, %C1.4426950216293335, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.cast"(%R7, %D305) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.sub"(%R8.4608, %R7, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.cast"(%R2, %D305) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.min"(%R8.4608, %C127, %D305) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.max"(%R2, %C-127, %D305) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.add_satu"(%R8.4608, %C127, %C23, %D305) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "sfu.taylor_4x"(%R7, %R10.1024, %D305) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.mul"(%R8.4608, %R2, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.add"(%R2, %C1.0, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R7, %D305) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R4, %D305) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G9412608, %B3013) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R11, %D0 = "dma.tensor"(%G9936896, %B3013) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R6, %B1 = "conv.normal"(%R2, %R0, %R11, %C0.0, %D307) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G11468800, %D0 = "dma.tensor"(%R2, %B3029) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none), %R4, %D0 = "dma.tensor"(%G800, %B3029) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10.1024, %B1 = "tsbc.s_bc"(%R0, %D309) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.4608, %B1 = "arith.sub"(%C0.0, %R4, %D309) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.max"(%R8.4608, %C-3.4028198694267105e+35, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.mul"(%R8.4608, %C1.4426950216293335, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.cast"(%R7, %D309) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.sub"(%R8.4608, %R7, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.cast"(%R2, %D309) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.min"(%R8.4608, %C127, %D309) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "arith.max"(%R2, %C-127, %D309) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.add_satu"(%R8.4608, %C127, %C23, %D309) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R8.4608, %B1 = "sfu.taylor_4x"(%R7, %R10.1024, %D309) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.mul"(%R8.4608, %R2, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R7, %B1 = "arith.add"(%R2, %C1.0, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R7, %D309) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R4, %D309) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %G819200, %D0 = "dma.tensor"(%R6, %B3030) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R6, %B1 = "conv.normal"(%R2, %R0, %R11, %C0.0, %D310) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G11469600, %D0 = "dma.tensor"(%R2, %B3046) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none), %G820000, %D0 = "dma.tensor"(%R6, %B3047) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G819200, %B3047) : (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D313) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R11, %D313) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D313) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R9, %D313) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.min"(%R1.4608, %C127, %D313) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.max"(%R9, %C-127, %D313) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D313) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D313) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R1.4608, %R9, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D313) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R11, %D313) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G9940992, %B3047) : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R15.4608, %D0 = "dma.tensor"(%G10203136, %B3047) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R13, %R15.4608, %C0.0, %D315) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G12288000, %D0 = "dma.tensor"(%R9, %B3063) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G10207232, %B3063) : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [18432, 2304, 9, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D317) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R14.1024, %B1 = "arith.sub"(%C0.0, %R11, %D317) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.max"(%R14.1024, %C-3.4028198694267105e+35, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R14.1024, %C1.4426950216293335, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.cast"(%R12.4608, %D317) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.sub"(%R14.1024, %R12.4608, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.cast"(%R9, %D317) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.min"(%R14.1024, %C127, %D317) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.max"(%R9, %C-127, %D317) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.add_satu"(%R14.1024, %C127, %C23, %D317) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "sfu.taylor_4x"(%R12.4608, %R15.5632, %D317) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R14.1024, %R9, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.add"(%R9, %C1.0, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R12.4608, %D317) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R11, %D317) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R10.4608, %D0 = "dma.tensor"(%G12566528, %B3064) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R14, %B1 = "conv.normal"(%R9, %R0, %R10.4608, %C0.0, %D318) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G0, %D0 = "dma.tensor"(%R14, %B3081) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G0, %B3081) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15.4608, %B1 = "tsbc.s_bc"(%R0, %D320) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R14.6400, %B1 = "arith.sub"(%C0.0, %R11, %D320) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.max"(%R14.6400, %C-3.4028198694267105e+35, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.mul"(%R14.6400, %C1.4426950216293335, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.cast"(%R14, %D320) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.mul"(%R12.6400, %C0.6931471824645996, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.sub"(%R14.6400, %R14, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.cast"(%R12.6400, %D320) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.min"(%R14.6400, %C127, %D320) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.max"(%R12.6400, %C-127, %D320) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.add_satu"(%R14.6400, %C127, %C23, %D320) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "sfu.taylor_4x"(%R14, %R15.4608, %D320) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.mul"(%R14.6400, %R12.6400, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.add"(%R12.6400, %C1.0, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.div"(%C1.0, %R14, %D320) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.mul"(%R12.6400, %R11, %D320) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.4608, %D0 = "dma.tensor"(%G12288000, %B3081) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %D0 = "dma.tensor"(%G11468800, %B3081) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R11, %B1 = "arith.add"(%R9.4608, %R12.6400, %D322) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [0, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [0, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R4, %D0 = "dma.tensor"(%G12570624, %B3097) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R15.4800, %D0 = "dma.tensor"(%G13094912, %B3097) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R4, %R15.4800, %C0.0, %D324) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G13099008, %B3098) : (memref<1x512x512x1xf32, strides: [262144, 512, 1, 1]>, none) -> (memref<1x512x512x1xf32, strides: [8192, 512, 1, 1]>, none), %R9.4608, %B1 = "tsbc.s_bc"(%R0, %D325) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.6400, %B1 = "arith.sub"(%C0.0, %R10, %D325) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.max"(%R8.6400, %C-3.4028198694267105e+35, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8.6400, %C1.4426950216293335, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.cast"(%R8, %D325) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R12, %C0.6931471824645996, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.sub"(%R8.6400, %R8, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.cast"(%R12, %D325) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.min"(%R8.6400, %C127, %D325) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.max"(%R12, %C-127, %D325) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.add_satu"(%R8.6400, %C127, %C23, %D325) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "sfu.taylor_4x"(%R8, %R9.4608, %D325) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8.6400, %R12, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.add"(%R12, %C1.0, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.div"(%C1.0, %R8, %D325) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12, %R10, %D325) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15.4736, %D0 = "dma.tensor"(%G14147584, %B3099) : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [16, 1, 1, 1]>, none), %R8, %B1 = "arith.copy"(%R11, %D326) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.copy"(%R12, %D326) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R0, %R15.4736, %C0.0, %D326) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R15.1024, %B1 = "tsbc.s_bc"(%R0, %D326) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.4608, %B1 = "arith.sub"(%C0.0, %R10, %D326) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R13.4608, %C-3.4028198694267105e+35, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R13.4608, %C1.4426950216293335, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.cast"(%R12, %D326) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R13.4608, %R12, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.cast"(%R8, %D326) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.min"(%R13.4608, %C127, %D326) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R8, %C-127, %D326) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.4608, %C127, %C23, %D326) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "sfu.taylor_4x"(%R12, %R15.1024, %D326) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R13.4608, %R8, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.add"(%R8, %C1.0, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R12, %D326) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R10, %D326) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R6, %D0 = "dma.tensor"(%G14151680, %B3118) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R11.6400, %D0 = "dma.tensor"(%G14675968, %B3118) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R12, %B1 = "conv.normal"(%R8, %R6, %R11.6400, %C0.0, %D328) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G800, %B3134) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15.4608, %B1 = "tsbc.s_bc"(%R0, %D329) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R14.6400, %B1 = "arith.sub"(%C0.0, %R11, %D329) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.max"(%R14.6400, %C-3.4028198694267105e+35, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.mul"(%R14.6400, %C1.4426950216293335, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.cast"(%R14, %D329) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.mul"(%R12.6400, %C0.6931471824645996, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.sub"(%R14.6400, %R14, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.cast"(%R12.6400, %D329) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.min"(%R14.6400, %C127, %D329) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "arith.max"(%R12.6400, %C-127, %D329) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.add_satu"(%R14.6400, %C127, %C23, %D329) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R14.6400, %B1 = "sfu.taylor_4x"(%R14, %R15.4608, %D329) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.mul"(%R14.6400, %R12.6400, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R14, %B1 = "arith.add"(%R12.6400, %C1.0, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.div"(%C1.0, %R14, %D329) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12.6400, %B1 = "arith.mul"(%R12.6400, %R11, %D329) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.4608, %D0 = "dma.tensor"(%G12288800, %B3135) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %D0 = "dma.tensor"(%G11469600, %B3135) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R11, %B1 = "arith.add"(%R9.4608, %R12.6400, %D331) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [0, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [0, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R4, %R15.4800, %C0.0, %D331) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G819200, %D0 = "dma.tensor"(%R12, %B3152) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R9.4608, %B1 = "tsbc.s_bc"(%R0, %D332) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R8.6400, %B1 = "arith.sub"(%C0.0, %R10, %D332) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.max"(%R8.6400, %C-3.4028198694267105e+35, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8.6400, %C1.4426950216293335, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.cast"(%R8, %D332) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R12, %C0.6931471824645996, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.sub"(%R8.6400, %R8, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.cast"(%R12, %D332) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.min"(%R8.6400, %C127, %D332) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.max"(%R12, %C-127, %D332) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.add_satu"(%R8.6400, %C127, %C23, %D332) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R8.6400, %B1 = "sfu.taylor_4x"(%R8, %R9.4608, %D332) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8.6400, %R12, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.add"(%R12, %C1.0, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.div"(%C1.0, %R8, %D332) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12, %R10, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.copy"(%R11, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.copy"(%R12, %D332) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R0, %R15.4736, %C0.0, %D332) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R15.1024, %B1 = "tsbc.s_bc"(%R0, %D332) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.4608, %B1 = "arith.sub"(%C0.0, %R10, %D332) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R13.4608, %C-3.4028198694267105e+35, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R13.4608, %C1.4426950216293335, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.cast"(%R12, %D332) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R13.4608, %R12, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.cast"(%R8, %D332) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.min"(%R13.4608, %C127, %D332) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R8, %C-127, %D332) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.4608, %C127, %C23, %D332) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "sfu.taylor_4x"(%R12, %R15.1024, %D332) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R13.4608, %R8, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.add"(%R8, %C1.0, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R12, %D332) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R10, %D332) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "conv.normal"(%R8, %R6, %R11.6400, %C0.0, %D332) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G820000, %D0 = "dma.tensor"(%R12, %B3189) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R2, %D0 = "dma.tensor"(%G819200, %B3189) : (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D334) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R2, %D334) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.cast"(%R4, %D334) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R0, %D334) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.min"(%R5.4608, %C127, %D334) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.max"(%R0, %C-127, %D334) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D334) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D334) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R5.4608, %R0, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.add"(%R0, %C1.0, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R4, %D334) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R2, %D334) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R2, %B1 = "pord.maxpooling"(%R0, %C-3.4028234663852886e+38, %D334) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G11878400, %D0 = "dma.tensor"(%R0, %B3205) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R0, %B1 = "pord.maxpooling"(%R2, %C-3.4028234663852886e+38, %D335) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G12288000, %D0 = "dma.tensor"(%R2, %B3206) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R8, %B1 = "pord.maxpooling"(%R0, %C-3.4028234663852886e+38, %D336) {kernel = [5, 5], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [2, 2, 2, 2], round_mode = 0, shift = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G0, %D0 = "dma.tensor"(%R0, %B3207) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %G409600, %D0 = "dma.tensor"(%R8, %B3208) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G11878400, %B3208) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R14, %D0 = "dma.tensor"(%G12288000, %B3208) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G0, %B3208) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G409600, %B3208) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R10, %B1 = "arith.copy"(%R13, %D342) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.3200, %B1 = "arith.copy"(%R14, %D342) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.6400, %B1 = "arith.copy"(%R15, %D342) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R11.1408, %B1 = "arith.copy"(%R11.4608, %D342) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G14680064, %B3208) : (memref<1x512x1024x1xf32, strides: [524288, 1024, 1, 1]>, none) -> (memref<1x512x1024x1xf32, strides: [16384, 1024, 1, 1]>, none), %R12.4736, %D0 = "dma.tensor"(%G16777216, %B3208) : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [16, 1, 1, 1]>, none), %R13, %B1 = "conv.normal"(%R10, %R0, %R12.4736, %C0.0, %D344) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x1024x5x20xf32, strides: [3200, 100, 20, 1]>, memref<512x1024x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R12.4608, %B1 = "tsbc.s_bc"(%R0, %D344) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.6400, %B1 = "arith.sub"(%C0.0, %R13, %D344) {round_mode = 0} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R11.6400, %C-3.4028198694267105e+35, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R11.6400, %C1.4426950216293335, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.cast"(%R11, %D344) {round_mode = 3} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R10, %C0.6931471824645996, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.sub"(%R11.6400, %R11, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.cast"(%R10, %D344) {round_mode = 1} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.min"(%R11.6400, %C127, %D344) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R10, %C-127, %D344) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.add_satu"(%R11.6400, %C127, %C23, %D344) {round_mode = 1} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, ui8, none) -> (memref<1x512x5x20xsi32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "sfu.taylor_4x"(%R11, %R12.4608, %D344) : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R11.6400, %R10, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.add"(%R10, %C1.0, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.div"(%C1.0, %R11, %D344) {iter = 3} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R10, %R13, %D344) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R8, %D0 = "dma.tensor"(%G16781312, %B3213) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R12.4800, %D0 = "dma.tensor"(%G17305600, %B3213) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R12, %B1 = "conv.normal"(%R10, %R8, %R12.4800, %C0.0, %D346) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G11878800, %B3229) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R14, %D0 = "dma.tensor"(%G12288400, %B3229) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G400, %B3229) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G410000, %B3229) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R10, %B1 = "arith.copy"(%R13, %D350) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.3200, %B1 = "arith.copy"(%R14, %D350) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.6400, %B1 = "arith.copy"(%R15, %D350) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R11.1408, %B1 = "arith.copy"(%R11.4608, %D350) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R13, %B1 = "conv.normal"(%R10, %R0, %R12.4736, %C0.0, %D350) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x1024x5x20xf32, strides: [3200, 100, 20, 1]>, memref<512x1024x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %G11468800, %D0 = "dma.tensor"(%R12, %B3234) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none), %R12.4608, %B1 = "tsbc.s_bc"(%R0, %D351) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.6400, %B1 = "arith.sub"(%C0.0, %R13, %D351) {round_mode = 0} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R11.6400, %C-3.4028198694267105e+35, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R11.6400, %C1.4426950216293335, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.cast"(%R11, %D351) {round_mode = 3} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R10, %C0.6931471824645996, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.sub"(%R11.6400, %R11, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.cast"(%R10, %D351) {round_mode = 1} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.min"(%R11.6400, %C127, %D351) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R10, %C-127, %D351) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.add_satu"(%R11.6400, %C127, %C23, %D351) {round_mode = 1} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, ui8, none) -> (memref<1x512x5x20xsi32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "sfu.taylor_4x"(%R11, %R12.4608, %D351) : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R11.6400, %R10, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.add"(%R10, %C1.0, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.div"(%C1.0, %R11, %D351) {iter = 3} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R10, %R13, %D351) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R12, %B1 = "conv.normal"(%R10, %R8, %R12.4800, %C0.0, %D351) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G11879200, %B3251) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R14, %D0 = "dma.tensor"(%G12288800, %B3251) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G800, %B3251) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G410400, %B3251) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R10, %B1 = "arith.copy"(%R13, %D355) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.3200, %B1 = "arith.copy"(%R14, %D355) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.6400, %B1 = "arith.copy"(%R15, %D355) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R11.1408, %B1 = "arith.copy"(%R11.4608, %D355) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R13, %B1 = "conv.normal"(%R10, %R0, %R12.4736, %C0.0, %D355) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x1024x5x20xf32, strides: [3200, 100, 20, 1]>, memref<512x1024x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %G11469200, %D0 = "dma.tensor"(%R12, %B3256) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none), %R12.4608, %B1 = "tsbc.s_bc"(%R0, %D356) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.6400, %B1 = "arith.sub"(%C0.0, %R13, %D356) {round_mode = 0} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R11.6400, %C-3.4028198694267105e+35, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R11.6400, %C1.4426950216293335, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.cast"(%R11, %D356) {round_mode = 3} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R10, %C0.6931471824645996, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.sub"(%R11.6400, %R11, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.cast"(%R10, %D356) {round_mode = 1} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.min"(%R11.6400, %C127, %D356) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R10, %C-127, %D356) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.add_satu"(%R11.6400, %C127, %C23, %D356) {round_mode = 1} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, ui8, none) -> (memref<1x512x5x20xsi32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "sfu.taylor_4x"(%R11, %R12.4608, %D356) : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R11.6400, %R10, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.add"(%R10, %C1.0, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.div"(%C1.0, %R11, %D356) {iter = 3} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R10, %R13, %D356) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R12, %B1 = "conv.normal"(%R10, %R8, %R12.4800, %C0.0, %D356) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G11879600, %B3273) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R14, %D0 = "dma.tensor"(%G12289200, %B3273) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G1200, %B3273) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G410800, %B3273) : (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R10, %B1 = "arith.copy"(%R13, %D360) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.3200, %B1 = "arith.copy"(%R14, %D360) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R10.6400, %B1 = "arith.copy"(%R15, %D360) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R11.1408, %B1 = "arith.copy"(%R11.4608, %D360) {round_mode = 0} : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [3200, 100, 20, 1]>, none), %R13, %B1 = "conv.normal"(%R10, %R0, %R12.4736, %C0.0, %D360) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x1024x5x20xf32, strides: [3200, 100, 20, 1]>, memref<512x1024x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %G11469600, %D0 = "dma.tensor"(%R12, %B3278) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none), %R12.4608, %B1 = "tsbc.s_bc"(%R0, %D361) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R11.6400, %B1 = "arith.sub"(%C0.0, %R13, %D361) {round_mode = 0} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R11.6400, %C-3.4028198694267105e+35, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R11.6400, %C1.4426950216293335, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.cast"(%R11, %D361) {round_mode = 3} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.mul"(%R10, %C0.6931471824645996, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.sub"(%R11.6400, %R11, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "arith.cast"(%R10, %D361) {round_mode = 1} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.min"(%R11.6400, %C127, %D361) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R11.6400, %B1 = "arith.max"(%R10, %C-127, %D361) {round_mode = 0} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, none) -> (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, none), %R10, %B1 = "arith.add_satu"(%R11.6400, %C127, %C23, %D361) {round_mode = 1} : (memref<1x512x5x20xsi16, strides: [1664, 104, 20, 1]>, si16, ui8, none) -> (memref<1x512x5x20xsi32, strides: [1600, 100, 20, 1]>, none), %R11.6400, %B1 = "sfu.taylor_4x"(%R11, %R12.4608, %D361) : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R11.6400, %R10, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R11, %B1 = "arith.add"(%R10, %C1.0, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, f32, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.div"(%C1.0, %R11, %D361) {iter = 3} : (f32, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R10, %B1 = "arith.mul"(%R10, %R13, %D361) {round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none) -> (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, none), %R12, %B1 = "conv.normal"(%R10, %R8, %R12.4800, %C0.0, %D361) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x5x20xf32, strides: [1600, 100, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none), %R4, %D0 = "dma.tensor"(%G11468800, %B3295) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R1.4608, %B1 = "tsbc.s_bc"(%R0, %D362) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R0.6400, %B1 = "arith.sub"(%C0.0, %R4, %D362) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.max"(%R0.6400, %C-3.4028198694267105e+35, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R0.6400, %C1.4426950216293335, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.cast"(%R0, %D362) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.sub"(%R0.6400, %R0, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.cast"(%R15, %D362) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.min"(%R0.6400, %C127, %D362) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.max"(%R15, %C-127, %D362) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.add_satu"(%R0.6400, %C127, %C23, %D362) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "sfu.taylor_4x"(%R0, %R1.4608, %D362) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.mul"(%R0.6400, %R15, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.add"(%R15, %C1.0, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R0, %D362) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R4, %D362) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R10.1024, %D0 = "dma.tensor"(%G17309696, %B3296) : (memref<1x128x512x1xf32, strides: [65536, 512, 1, 1]>, none) -> (memref<1x128x512x1xf32, strides: [2048, 512, 1, 1]>, none), %G11470000, %D0 = "dma.tensor"(%R12, %B3296) : (memref<1x256x5x20xf32, strides: [800, 100, 20, 1]>, none) -> (memref<1x256x5x20xf32, strides: [102400, 400, 20, 1]>, none), %R11.1024, %B1 = "arith.copy"(%R15, %D364) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [6400, 800, 80, 2]>, none), %R11.1028, %B1 = "arith.copy"(%R15, %D364) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [6400, 800, 80, 2]>, none), %R11.1184, %B1 = "arith.copy"(%R11.1024, %D364) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [6400, 800, 80, 1]>, none) -> (memref<1x256x10x40xf32, strides: [6400, 800, 80, 1]>, none), %R7, %D0 = "dma.tensor"(%G9830400, %B3312) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R0, %B1 = "arith.copy"(%R11.1024, %D365) {round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [12800, 800, 40, 1]>, none), %R3.1024, %B1 = "arith.copy"(%R7, %D365) {round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [12800, 800, 40, 1]>, none), %R6.4096, %D0 = "dma.tensor"(%G17571840, %B3315) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R7, %B1 = "conv.normal"(%R0, %R10.1024, %R6.4096, %C0.0, %D366) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x20x40xf32, strides: [12800, 800, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G6553600, %D0 = "dma.tensor"(%R0, %B3317) : (memref<1x512x20x40xf32, strides: [12800, 800, 40, 1]>, none) -> (memref<1x512x20x40xf32, strides: [819200, 1600, 40, 1]>, none), %G23707648, %D0 = "dma.tensor"(%R15, %B3317) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D368) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R7, %D368) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D368) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R0, %D368) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R3.4608, %C127, %D368) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R0, %C-127, %D368) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D368) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D368) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R3.4608, %R0, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D368) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R7, %D368) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6.2048, %D0 = "dma.tensor"(%G17575936, %B3318) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R14.2048, %D0 = "dma.tensor"(%G17641472, %B3318) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R2, %B1 = "conv.normal"(%R0, %R6.2048, %R14.2048, %C0.0, %D370) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %D0 = "dma.tensor"(%G11469600, %B3334) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R1.4608, %B1 = "tsbc.s_bc"(%R0, %D371) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R0.6400, %B1 = "arith.sub"(%C0.0, %R4, %D371) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.max"(%R0.6400, %C-3.4028198694267105e+35, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R0.6400, %C1.4426950216293335, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.cast"(%R0, %D371) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R15, %C0.6931471824645996, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.sub"(%R0.6400, %R0, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.cast"(%R15, %D371) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.min"(%R0.6400, %C127, %D371) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "arith.max"(%R15, %C-127, %D371) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.add_satu"(%R0.6400, %C127, %C23, %D371) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R0.6400, %B1 = "sfu.taylor_4x"(%R0, %R1.4608, %D371) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.mul"(%R0.6400, %R15, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %B1 = "arith.add"(%R15, %C1.0, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.div"(%C1.0, %R0, %D371) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15, %B1 = "arith.mul"(%R15, %R4, %D371) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G21250048, %D0 = "dma.tensor"(%R2, %B3335) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R11.1024, %B1 = "arith.copy"(%R15, %D372) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [6400, 800, 80, 2]>, none), %R11.1028, %B1 = "arith.copy"(%R15, %D372) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [6400, 800, 80, 2]>, none), %R11.1184, %B1 = "arith.copy"(%R11.1024, %D372) {round_mode = 0} : (memref<1x256x10x40xf32, strides: [6400, 800, 80, 1]>, none) -> (memref<1x256x10x40xf32, strides: [6400, 800, 80, 1]>, none), %R7, %D0 = "dma.tensor"(%G9833600, %B3351) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R0, %B1 = "arith.copy"(%R11.1024, %D373) {round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [12800, 800, 40, 1]>, none), %R3.1024, %B1 = "arith.copy"(%R7, %D373) {round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [12800, 800, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R0, %R10.1024, %R6.4096, %C0.0, %D373) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x20x40xf32, strides: [12800, 800, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G6556800, %D0 = "dma.tensor"(%R0, %B3356) : (memref<1x512x20x40xf32, strides: [12800, 800, 40, 1]>, none) -> (memref<1x512x20x40xf32, strides: [819200, 1600, 40, 1]>, none), %G23708448, %D0 = "dma.tensor"(%R15, %B3356) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D375) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R7, %D375) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D375) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R0, %D375) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R3.4608, %C127, %D375) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R0, %C-127, %D375) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D375) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D375) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R3.4608, %R0, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D375) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R7, %D375) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "conv.normal"(%R0, %R6.2048, %R14.2048, %C0.0, %D375) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G21253248, %D0 = "dma.tensor"(%R2, %B3374) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R6, %D0 = "dma.tensor"(%G21250048, %B3374) : (memref<1x128x9x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R5.5376, %B1 = "tsbc.s_bc"(%R0, %D377) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.7808, %B1 = "arith.sub"(%C0.0, %R6, %D377) {round_mode = 0} : (f32, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.max"(%R4.7808, %C-3.4028198694267105e+35, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R4.7808, %C1.4426950216293335, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D377) {round_mode = 3} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R4.7808, %R4.2048, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.cast"(%R9, %D377) {round_mode = 1} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.min"(%R4.7808, %C127, %D377) {round_mode = 0} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.max"(%R9, %C-127, %D377) {round_mode = 0} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R4.7808, %C127, %C23, %D377) {round_mode = 1} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, ui8, none) -> (memref<1x128x9x40xsi32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "sfu.taylor_4x"(%R4.2048, %R5.5376, %D377) : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R4.7808, %R9, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D377) {iter = 3} : (f32, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D377) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G17645568, %B3374) : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R15.2592, %D0 = "dma.tensor"(%G18235392, %B3374) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R10, %B1 = "conv.normal"(%R9, %R0, %R15.2592, %C0.0, %D379) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G6553600, %B3390) : (memref<1x512x8x40xf32, strides: [819200, 1600, 40, 1]>, none) -> (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D380) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.3072, %B1 = "arith.sub"(%C0.0, %R10, %D380) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R7.3072, %C-3.4028198694267105e+35, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R7.3072, %C1.4426950216293335, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R6.6144, %D380) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.sub"(%R7.3072, %R6.6144, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.cast"(%R9, %D380) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R7.3072, %C127, %D380) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R9, %C-127, %D380) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R7.3072, %C127, %C23, %D380) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "sfu.taylor_4x"(%R6.6144, %R8, %D380) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R7.3072, %R9, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add"(%R9, %C1.0, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R6.6144, %D380) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R10, %D380) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G18239488, %B3391) : (memref<1x128x512x1xf32, strides: [65536, 512, 1, 1]>, none) -> (memref<1x128x512x1xf32, strides: [2048, 512, 1, 1]>, none), %R15.2608, %D0 = "dma.tensor"(%G18501632, %B3391) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R7, %B1 = "conv.normal"(%R4.2048, %R2.2048, %R15.2608, %C0.0, %D382) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D382) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.5120, %B1 = "arith.sub"(%C0.0, %R7, %D382) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R5.5120, %C-3.4028198694267105e+35, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R5.5120, %C1.4426950216293335, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.cast"(%R5, %D382) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R5.5120, %R5, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.cast"(%R8, %D382) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.min"(%R5.5120, %C127, %D382) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R8, %C-127, %D382) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.5120, %C127, %C23, %D382) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "sfu.taylor_4x"(%R5, %R6.2048, %D382) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R5.5120, %R8, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.add"(%R8, %C1.0, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R5, %D382) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R7, %D382) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R3.2048, %D0 = "dma.tensor"(%G18505728, %B3408) : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R15.2560, %D0 = "dma.tensor"(%G18767872, %B3408) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R5, %B1 = "arith.copy"(%R9, %D384) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.copy"(%R8, %D384) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R5, %R3.2048, %R15.2560, %C0.0, %D384) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R6.6144, %B1 = "tsbc.s_bc"(%R0, %D384) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R7, %D384) {round_mode = 0} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D384) {round_mode = 3} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.4096, %R4.2048, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R9, %D384) {round_mode = 1} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.4096, %C127, %D384) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R9, %C-127, %D384) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D384) {round_mode = 1} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, ui8, none) -> (memref<1x256x8x40xsi32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R4.2048, %R6.6144, %D384) : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.4096, %R9, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D384) {iter = 3} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R7, %D384) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R12, %D0 = "dma.tensor"(%G18771968, %B3427) : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R15.2624, %D0 = "dma.tensor"(%G18903040, %B3427) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R8, %B1 = "conv.normal"(%R9, %R12, %R15.2624, %C0.0, %D386) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G17973248, %B3443) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.2048, %B1 = "tsbc.s_bc"(%R0, %D387) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.5120, %B1 = "arith.sub"(%C0.0, %R8, %D387) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R9.5120, %C-3.4028198694267105e+35, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9.5120, %C1.4426950216293335, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.cast"(%R9, %D387) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R6.6144, %C0.6931471824645996, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.sub"(%R9.5120, %R9, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.cast"(%R6.6144, %D387) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.min"(%R9.5120, %C127, %D387) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R6.6144, %C-127, %D387) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add_satu"(%R9.5120, %C127, %C23, %D387) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "sfu.taylor_4x"(%R9, %R10.2048, %D387) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9.5120, %R6.6144, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add"(%R6.6144, %C1.0, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.div"(%C1.0, %R9, %D387) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R6.6144, %R8, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R12.4096, %B1 = "arith.copy"(%R6.6144, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4100, %B1 = "arith.copy"(%R6.6144, %D387) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4416, %B1 = "arith.copy"(%R12.4096, %D387) {round_mode = 0} : (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none) -> (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none), %G11468800, %D0 = "dma.tensor"(%R6.6144, %B3460) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [204800, 1600, 40, 1]>, none), %R15.2640, %D0 = "dma.tensor"(%G18972672, %B3460) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4096, %D389) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R9.4096, %B1 = "arith.copy"(%R4.2048, %D389) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R15, %D0 = "dma.tensor"(%G18907136, %B3463) : (memref<1x64x256x1xf32, strides: [16384, 256, 1, 1]>, none) -> (memref<1x64x256x1xf32, strides: [512, 256, 1, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R7, %R15, %R15.2640, %C0.0, %D390) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G0, %D0 = "dma.tensor"(%R7, %B3465) : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none), %R10.4096, %B1 = "tsbc.s_bc"(%R0, %D391) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D391) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R9.2048, %C-3.4028198694267105e+35, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R9.2048, %C1.4426950216293335, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R8, %D391) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R9.2048, %R8, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.cast"(%R6, %D391) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R9.2048, %C127, %D391) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R6, %C-127, %D391) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R9.2048, %C127, %C23, %D391) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "sfu.taylor_4x"(%R8, %R10.4096, %D391) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R9.2048, %R6, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R6, %C1.0, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R8, %D391) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4.2048, %D391) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R15.2048, %D0 = "dma.tensor"(%G18976768, %B3466) : (memref<1x64x64x1xf32, strides: [4096, 64, 1, 1]>, none) -> (memref<1x64x64x1xf32, strides: [128, 64, 1, 1]>, none), %R15.2656, %D0 = "dma.tensor"(%G18993152, %B3466) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R6, %R15.2048, %R15.2656, %C0.0, %D393) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D393) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D393) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R9, %D393) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R7, %D393) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R10.2048, %C127, %D393) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R7, %C-127, %D393) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D393) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D393) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R10.2048, %R7, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R7, %C1.0, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R9, %D393) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R4.2048, %D393) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G21251168, %B3483) : (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.6656, %B1 = "tsbc.s_bc"(%R0, %D394) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.256, %B1 = "arith.sub"(%C0.0, %R6, %D394) {round_mode = 0} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R5.256, %C-3.4028198694267105e+35, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.256, %C1.4426950216293335, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D394) {round_mode = 3} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.256, %R4.2048, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.cast"(%R9, %D394) {round_mode = 1} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.256, %C127, %D394) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R9, %C-127, %D394) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.256, %C127, %C23, %D394) {round_mode = 1} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, ui8, none) -> (memref<1x128x10x40xsi32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "sfu.taylor_4x"(%R4.2048, %R5.6656, %D394) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.256, %R9, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D394) {iter = 3} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D394) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R9, %R0, %R15.2592, %C0.0, %D394) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %G9830400, %D0 = "dma.tensor"(%R7, %B3515) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G6554880, %B3515) : (memref<1x512x8x40xf32, strides: [819200, 1600, 40, 1]>, none) -> (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D396) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.3072, %B1 = "arith.sub"(%C0.0, %R10, %D396) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R7.3072, %C-3.4028198694267105e+35, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R7.3072, %C1.4426950216293335, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R6.6144, %D396) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.sub"(%R7.3072, %R6.6144, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.cast"(%R9, %D396) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R7.3072, %C127, %D396) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R9, %C-127, %D396) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R7.3072, %C127, %C23, %D396) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "sfu.taylor_4x"(%R6.6144, %R8, %D396) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R7.3072, %R9, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add"(%R9, %C1.0, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R6.6144, %D396) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R10, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R4.2048, %R2.2048, %R15.2608, %C0.0, %D396) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D396) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.5120, %B1 = "arith.sub"(%C0.0, %R7, %D396) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R5.5120, %C-3.4028198694267105e+35, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R5.5120, %C1.4426950216293335, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.cast"(%R5, %D396) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R5.5120, %R5, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.cast"(%R8, %D396) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.min"(%R5.5120, %C127, %D396) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R8, %C-127, %D396) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.5120, %C127, %C23, %D396) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "sfu.taylor_4x"(%R5, %R6.2048, %D396) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R5.5120, %R8, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.add"(%R8, %C1.0, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R5, %D396) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R7, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.copy"(%R9, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.copy"(%R8, %D396) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R5, %R3.2048, %R15.2560, %C0.0, %D396) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R6.6144, %B1 = "tsbc.s_bc"(%R0, %D396) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R7, %D396) {round_mode = 0} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D396) {round_mode = 3} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.4096, %R4.2048, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R9, %D396) {round_mode = 1} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.4096, %C127, %D396) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R9, %C-127, %D396) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D396) {round_mode = 1} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, ui8, none) -> (memref<1x256x8x40xsi32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R4.2048, %R6.6144, %D396) : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.4096, %R9, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D396) {iter = 3} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R7, %D396) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R9, %R12, %R15.2624, %C0.0, %D396) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G17978368, %B3568) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.2048, %B1 = "tsbc.s_bc"(%R0, %D397) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.5120, %B1 = "arith.sub"(%C0.0, %R8, %D397) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R9.5120, %C-3.4028198694267105e+35, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9.5120, %C1.4426950216293335, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.cast"(%R9, %D397) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R6.6144, %C0.6931471824645996, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.sub"(%R9.5120, %R9, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.cast"(%R6.6144, %D397) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.min"(%R9.5120, %C127, %D397) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R6.6144, %C-127, %D397) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add_satu"(%R9.5120, %C127, %C23, %D397) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "sfu.taylor_4x"(%R9, %R10.2048, %D397) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9.5120, %R6.6144, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add"(%R6.6144, %C1.0, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.div"(%C1.0, %R9, %D397) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R6.6144, %R8, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R12.4096, %B1 = "arith.copy"(%R6.6144, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4100, %B1 = "arith.copy"(%R6.6144, %D397) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4416, %B1 = "arith.copy"(%R12.4096, %D397) {round_mode = 0} : (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none) -> (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none), %G11470080, %D0 = "dma.tensor"(%R6.6144, %B3585) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4096, %D398) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R9.4096, %B1 = "arith.copy"(%R4.2048, %D398) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R7, %R15, %R15.2640, %C0.0, %D398) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G5120, %D0 = "dma.tensor"(%R7, %B3590) : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none), %R10.4096, %B1 = "tsbc.s_bc"(%R0, %D399) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D399) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R9.2048, %C-3.4028198694267105e+35, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R9.2048, %C1.4426950216293335, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R8, %D399) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R9.2048, %R8, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.cast"(%R6, %D399) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R9.2048, %C127, %D399) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R6, %C-127, %D399) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R9.2048, %C127, %C23, %D399) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "sfu.taylor_4x"(%R8, %R10.4096, %D399) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R9.2048, %R6, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R6, %C1.0, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R8, %D399) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4.2048, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R6, %R15.2048, %R15.2656, %C0.0, %D399) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D399) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D399) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R9, %D399) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R7, %D399) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R10.2048, %C127, %D399) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R7, %C-127, %D399) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D399) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D399) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R10.2048, %R7, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R7, %C1.0, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R9, %D399) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R4.2048, %D399) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G21252448, %B3608) : (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.6656, %B1 = "tsbc.s_bc"(%R0, %D400) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.256, %B1 = "arith.sub"(%C0.0, %R6, %D400) {round_mode = 0} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R5.256, %C-3.4028198694267105e+35, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.256, %C1.4426950216293335, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D400) {round_mode = 3} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.256, %R4.2048, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.cast"(%R9, %D400) {round_mode = 1} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.256, %C127, %D400) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R9, %C-127, %D400) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.256, %C127, %C23, %D400) {round_mode = 1} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, ui8, none) -> (memref<1x128x10x40xsi32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "sfu.taylor_4x"(%R4.2048, %R5.6656, %D400) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.256, %R9, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D400) {iter = 3} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D400) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R9, %R0, %R15.2592, %C0.0, %D400) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %G9835520, %D0 = "dma.tensor"(%R7, %B3640) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G6556160, %B3640) : (memref<1x512x8x40xf32, strides: [819200, 1600, 40, 1]>, none) -> (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D402) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.3072, %B1 = "arith.sub"(%C0.0, %R10, %D402) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R7.3072, %C-3.4028198694267105e+35, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R7.3072, %C1.4426950216293335, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R6.6144, %D402) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.sub"(%R7.3072, %R6.6144, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.cast"(%R9, %D402) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R7.3072, %C127, %D402) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R9, %C-127, %D402) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R7.3072, %C127, %C23, %D402) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "sfu.taylor_4x"(%R6.6144, %R8, %D402) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R7.3072, %R9, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add"(%R9, %C1.0, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R6.6144, %D402) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R10, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R4.2048, %R2.2048, %R15.2608, %C0.0, %D402) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D402) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.5120, %B1 = "arith.sub"(%C0.0, %R7, %D402) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R5.5120, %C-3.4028198694267105e+35, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R5.5120, %C1.4426950216293335, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.cast"(%R5, %D402) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R5.5120, %R5, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.cast"(%R8, %D402) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.min"(%R5.5120, %C127, %D402) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R8, %C-127, %D402) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.5120, %C127, %C23, %D402) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "sfu.taylor_4x"(%R5, %R6.2048, %D402) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R5.5120, %R8, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.add"(%R8, %C1.0, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R5, %D402) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R7, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.copy"(%R9, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.copy"(%R8, %D402) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R5, %R3.2048, %R15.2560, %C0.0, %D402) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R6.6144, %B1 = "tsbc.s_bc"(%R0, %D402) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R7, %D402) {round_mode = 0} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D402) {round_mode = 3} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.4096, %R4.2048, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R9, %D402) {round_mode = 1} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.4096, %C127, %D402) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R9, %C-127, %D402) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D402) {round_mode = 1} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, ui8, none) -> (memref<1x256x8x40xsi32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R4.2048, %R6.6144, %D402) : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.4096, %R9, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D402) {iter = 3} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R7, %D402) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R9, %R12, %R15.2624, %C0.0, %D402) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G17983488, %B3693) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.2048, %B1 = "tsbc.s_bc"(%R0, %D403) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.5120, %B1 = "arith.sub"(%C0.0, %R8, %D403) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R9.5120, %C-3.4028198694267105e+35, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9.5120, %C1.4426950216293335, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.cast"(%R9, %D403) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R6.6144, %C0.6931471824645996, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.sub"(%R9.5120, %R9, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.cast"(%R6.6144, %D403) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.min"(%R9.5120, %C127, %D403) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R6.6144, %C-127, %D403) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add_satu"(%R9.5120, %C127, %C23, %D403) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "sfu.taylor_4x"(%R9, %R10.2048, %D403) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9.5120, %R6.6144, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add"(%R6.6144, %C1.0, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.div"(%C1.0, %R9, %D403) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R6.6144, %R8, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R12.4096, %B1 = "arith.copy"(%R6.6144, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4100, %B1 = "arith.copy"(%R6.6144, %D403) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4416, %B1 = "arith.copy"(%R12.4096, %D403) {round_mode = 0} : (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none) -> (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none), %G11471360, %D0 = "dma.tensor"(%R6.6144, %B3710) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4096, %D404) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R9.4096, %B1 = "arith.copy"(%R4.2048, %D404) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R7, %R15, %R15.2640, %C0.0, %D404) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G10240, %D0 = "dma.tensor"(%R7, %B3715) : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none), %R10.4096, %B1 = "tsbc.s_bc"(%R0, %D405) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D405) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R9.2048, %C-3.4028198694267105e+35, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R9.2048, %C1.4426950216293335, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R8, %D405) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R9.2048, %R8, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.cast"(%R6, %D405) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R9.2048, %C127, %D405) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R6, %C-127, %D405) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R9.2048, %C127, %C23, %D405) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "sfu.taylor_4x"(%R8, %R10.4096, %D405) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R9.2048, %R6, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R6, %C1.0, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R8, %D405) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4.2048, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R6, %R15.2048, %R15.2656, %C0.0, %D405) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D405) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D405) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R9, %D405) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R7, %D405) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R10.2048, %C127, %D405) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R7, %C-127, %D405) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D405) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D405) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R10.2048, %R7, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R7, %C1.0, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R9, %D405) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R4.2048, %D405) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G21253728, %B3733) : (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.6656, %B1 = "tsbc.s_bc"(%R0, %D406) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.256, %B1 = "arith.sub"(%C0.0, %R6, %D406) {round_mode = 0} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R5.256, %C-3.4028198694267105e+35, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.256, %C1.4426950216293335, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D406) {round_mode = 3} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.256, %R4.2048, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.cast"(%R9, %D406) {round_mode = 1} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.256, %C127, %D406) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "arith.max"(%R9, %C-127, %D406) {round_mode = 0} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, none) -> (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.256, %C127, %C23, %D406) {round_mode = 1} : (memref<1x128x10x40xsi16, strides: [1600, 400, 40, 1]>, si16, ui8, none) -> (memref<1x128x10x40xsi32, strides: [1600, 400, 40, 1]>, none), %R5.256, %B1 = "sfu.taylor_4x"(%R4.2048, %R5.6656, %D406) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.256, %R9, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D406) {iter = 3} : (f32, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D406) {round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R9, %R0, %R15.2592, %C0.0, %D406) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %G9840640, %D0 = "dma.tensor"(%R7, %B3765) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G6557440, %B3765) : (memref<1x512x8x40xf32, strides: [819200, 1600, 40, 1]>, none) -> (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D408) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.3072, %B1 = "arith.sub"(%C0.0, %R10, %D408) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R7.3072, %C-3.4028198694267105e+35, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R7.3072, %C1.4426950216293335, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R6.6144, %D408) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.sub"(%R7.3072, %R6.6144, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.cast"(%R9, %D408) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R7.3072, %C127, %D408) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R9, %C-127, %D408) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R7.3072, %C127, %C23, %D408) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "sfu.taylor_4x"(%R6.6144, %R8, %D408) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R7.3072, %R9, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add"(%R9, %C1.0, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R6.6144, %D408) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R10, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R4.2048, %R2.2048, %R15.2608, %C0.0, %D408) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D408) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.5120, %B1 = "arith.sub"(%C0.0, %R7, %D408) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R5.5120, %C-3.4028198694267105e+35, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R5.5120, %C1.4426950216293335, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.cast"(%R5, %D408) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R5.5120, %R5, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.cast"(%R8, %D408) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.min"(%R5.5120, %C127, %D408) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R8, %C-127, %D408) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.5120, %C127, %C23, %D408) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "sfu.taylor_4x"(%R5, %R6.2048, %D408) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R5.5120, %R8, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.add"(%R8, %C1.0, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R5, %D408) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R7, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.copy"(%R9, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.copy"(%R8, %D408) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R5, %R3.2048, %R15.2560, %C0.0, %D408) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R6.6144, %B1 = "tsbc.s_bc"(%R0, %D408) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R7, %D408) {round_mode = 0} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D408) {round_mode = 3} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.4096, %R4.2048, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R9, %D408) {round_mode = 1} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.4096, %C127, %D408) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R9, %C-127, %D408) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D408) {round_mode = 1} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, ui8, none) -> (memref<1x256x8x40xsi32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R4.2048, %R6.6144, %D408) : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.4096, %R9, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D408) {iter = 3} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R7, %D408) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R9, %R12, %R15.2624, %C0.0, %D408) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G17988608, %B3818) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.2048, %B1 = "tsbc.s_bc"(%R0, %D409) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.5120, %B1 = "arith.sub"(%C0.0, %R8, %D409) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R9.5120, %C-3.4028198694267105e+35, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9.5120, %C1.4426950216293335, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.cast"(%R9, %D409) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R6.6144, %C0.6931471824645996, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.sub"(%R9.5120, %R9, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.cast"(%R6.6144, %D409) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.min"(%R9.5120, %C127, %D409) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R6.6144, %C-127, %D409) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add_satu"(%R9.5120, %C127, %C23, %D409) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "sfu.taylor_4x"(%R9, %R10.2048, %D409) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9.5120, %R6.6144, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add"(%R6.6144, %C1.0, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.div"(%C1.0, %R9, %D409) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R6.6144, %R8, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R12.4096, %B1 = "arith.copy"(%R6.6144, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4100, %B1 = "arith.copy"(%R6.6144, %D409) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4416, %B1 = "arith.copy"(%R12.4096, %D409) {round_mode = 0} : (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none) -> (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none), %G11472640, %D0 = "dma.tensor"(%R6.6144, %B3835) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4096, %D410) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R9.4096, %B1 = "arith.copy"(%R4.2048, %D410) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R7, %R15, %R15.2640, %C0.0, %D410) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G15360, %D0 = "dma.tensor"(%R7, %B3840) : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none), %R10.4096, %B1 = "tsbc.s_bc"(%R0, %D411) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D411) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R9.2048, %C-3.4028198694267105e+35, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R9.2048, %C1.4426950216293335, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R8, %D411) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R9.2048, %R8, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.cast"(%R6, %D411) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R9.2048, %C127, %D411) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R6, %C-127, %D411) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R9.2048, %C127, %C23, %D411) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "sfu.taylor_4x"(%R8, %R10.4096, %D411) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R9.2048, %R6, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R6, %C1.0, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R8, %D411) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4.2048, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R6, %R15.2048, %R15.2656, %C0.0, %D411) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D411) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D411) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R9, %D411) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R7, %D411) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R10.2048, %C127, %D411) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R7, %C-127, %D411) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D411) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D411) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R10.2048, %R7, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R7, %C1.0, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R9, %D411) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R4.2048, %D411) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %D0 = "dma.tensor"(%G21255008, %B3858) : (memref<1x128x9x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R5.5376, %B1 = "tsbc.s_bc"(%R0, %D412) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R4.7808, %B1 = "arith.sub"(%C0.0, %R6, %D412) {round_mode = 0} : (f32, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.max"(%R4.7808, %C-3.4028198694267105e+35, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R4.7808, %C1.4426950216293335, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D412) {round_mode = 3} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R4.7808, %R4.2048, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.cast"(%R9, %D412) {round_mode = 1} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.min"(%R4.7808, %C127, %D412) {round_mode = 0} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "arith.max"(%R9, %C-127, %D412) {round_mode = 0} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, none) -> (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R4.7808, %C127, %C23, %D412) {round_mode = 1} : (memref<1x128x9x40xsi16, strides: [1440, 360, 40, 1]>, si16, ui8, none) -> (memref<1x128x9x40xsi32, strides: [1440, 360, 40, 1]>, none), %R4.7808, %B1 = "sfu.taylor_4x"(%R4.2048, %R5.5376, %D412) : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R4.7808, %R9, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, f32, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D412) {iter = 3} : (f32, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D412) {round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none) -> (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R9, %R0, %R15.2592, %C0.0, %D412) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x9x40xf32, strides: [1440, 360, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %G9845760, %D0 = "dma.tensor"(%R7, %B3890) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G6558720, %B3890) : (memref<1x512x8x40xf32, strides: [819200, 1600, 40, 1]>, none) -> (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, none), %R8, %B1 = "tsbc.s_bc"(%R0, %D414) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.3072, %B1 = "arith.sub"(%C0.0, %R10, %D414) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R7.3072, %C-3.4028198694267105e+35, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R7.3072, %C1.4426950216293335, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R6.6144, %D414) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.sub"(%R7.3072, %R6.6144, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.cast"(%R9, %D414) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R7.3072, %C127, %D414) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "arith.max"(%R9, %C-127, %D414) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R7.3072, %C127, %C23, %D414) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R7.3072, %B1 = "sfu.taylor_4x"(%R6.6144, %R8, %D414) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R7.3072, %R9, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add"(%R9, %C1.0, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R6.6144, %D414) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R10, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R4.2048, %R2.2048, %R15.2608, %C0.0, %D414) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x8x40xf32, strides: [5120, 320, 40, 1]>, memref<128x512x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.2048, %B1 = "tsbc.s_bc"(%R0, %D414) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.5120, %B1 = "arith.sub"(%C0.0, %R7, %D414) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R5.5120, %C-3.4028198694267105e+35, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R5.5120, %C1.4426950216293335, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.cast"(%R5, %D414) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.sub"(%R5.5120, %R5, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.cast"(%R8, %D414) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.min"(%R5.5120, %C127, %D414) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.max"(%R8, %C-127, %D414) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.5120, %C127, %C23, %D414) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R5.5120, %B1 = "sfu.taylor_4x"(%R5, %R6.2048, %D414) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R5.5120, %R8, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.add"(%R8, %C1.0, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R5, %D414) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R7, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R5, %B1 = "arith.copy"(%R9, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.5120, %B1 = "arith.copy"(%R8, %D414) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R7, %B1 = "conv.normal"(%R5, %R3.2048, %R15.2560, %C0.0, %D414) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R6.6144, %B1 = "tsbc.s_bc"(%R0, %D414) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4096, %B1 = "arith.sub"(%C0.0, %R7, %D414) {round_mode = 0} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R5.4096, %C-3.4028198694267105e+35, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R5.4096, %C1.4426950216293335, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.cast"(%R4.2048, %D414) {round_mode = 3} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.sub"(%R5.4096, %R4.2048, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.cast"(%R9, %D414) {round_mode = 1} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.min"(%R5.4096, %C127, %D414) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "arith.max"(%R9, %C-127, %D414) {round_mode = 0} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, none) -> (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.add_satu"(%R5.4096, %C127, %C23, %D414) {round_mode = 1} : (memref<1x256x8x40xsi16, strides: [2560, 320, 40, 1]>, si16, ui8, none) -> (memref<1x256x8x40xsi32, strides: [2560, 320, 40, 1]>, none), %R5.4096, %B1 = "sfu.taylor_4x"(%R4.2048, %R6.6144, %D414) : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R5.4096, %R9, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R4.2048, %B1 = "arith.add"(%R9, %C1.0, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, f32, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R4.2048, %D414) {iter = 3} : (f32, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R7, %D414) {round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none) -> (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R9, %R12, %R15.2624, %C0.0, %D414) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x8x40xf32, strides: [2560, 320, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R4.2048, %D0 = "dma.tensor"(%G17993728, %B3943) : (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R10.2048, %B1 = "tsbc.s_bc"(%R0, %D415) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.5120, %B1 = "arith.sub"(%C0.0, %R8, %D415) {round_mode = 0} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R9.5120, %C-3.4028198694267105e+35, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R9.5120, %C1.4426950216293335, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.cast"(%R9, %D415) {round_mode = 3} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.mul"(%R6.6144, %C0.6931471824645996, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.sub"(%R9.5120, %R9, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.cast"(%R6.6144, %D415) {round_mode = 1} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.min"(%R9.5120, %C127, %D415) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "arith.max"(%R6.6144, %C-127, %D415) {round_mode = 0} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, none) -> (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.add_satu"(%R9.5120, %C127, %C23, %D415) {round_mode = 1} : (memref<1x128x8x40xsi16, strides: [1280, 320, 40, 1]>, si16, ui8, none) -> (memref<1x128x8x40xsi32, strides: [1280, 320, 40, 1]>, none), %R9.5120, %B1 = "sfu.taylor_4x"(%R9, %R10.2048, %D415) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R9.5120, %R6.6144, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R9, %B1 = "arith.add"(%R6.6144, %C1.0, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, f32, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.div"(%C1.0, %R9, %D415) {iter = 3} : (f32, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R6.6144, %B1 = "arith.mul"(%R6.6144, %R8, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none), %R12.4096, %B1 = "arith.copy"(%R6.6144, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4100, %B1 = "arith.copy"(%R6.6144, %D415) {round_mode = 0} : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [5120, 1280, 160, 2]>, none), %R12.4416, %B1 = "arith.copy"(%R12.4096, %D415) {round_mode = 0} : (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none) -> (memref<1x128x8x80xf32, strides: [5120, 1280, 160, 1]>, none), %G11473920, %D0 = "dma.tensor"(%R6.6144, %B3960) : (memref<1x128x8x40xf32, strides: [1280, 320, 40, 1]>, none) -> (memref<1x128x8x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4096, %D416) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R9.4096, %B1 = "arith.copy"(%R4.2048, %D416) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R7, %R15, %R15.2640, %C0.0, %D416) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G20480, %D0 = "dma.tensor"(%R7, %B3965) : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none), %R10.4096, %B1 = "tsbc.s_bc"(%R0, %D417) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D417) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R9.2048, %C-3.4028198694267105e+35, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R9.2048, %C1.4426950216293335, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.cast"(%R8, %D417) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.sub"(%R9.2048, %R8, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.cast"(%R6, %D417) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.min"(%R9.2048, %C127, %D417) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "arith.max"(%R6, %C-127, %D417) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.add_satu"(%R9.2048, %C127, %C23, %D417) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R9.2048, %B1 = "sfu.taylor_4x"(%R8, %R10.4096, %D417) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R9.2048, %R6, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R8, %B1 = "arith.add"(%R6, %C1.0, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R8, %D417) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R4.2048, %D417) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R13, %D0 = "dma.tensor"(%G9830400, %B3966) : (memref<1x64x17x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, none), %R12, %D0 = "dma.tensor"(%G18997248, %B3966) : (memref<1x64x64x9xf32, strides: [36864, 576, 9, 1]>, none) -> (memref<1x64x64x9xf32, strides: [1152, 576, 9, 1]>, none), %R15, %D0 = "dma.tensor"(%G19144704, %B3966) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R4.2048, %B1 = "conv.normal"(%R6, %R15.2048, %R15.2656, %C0.0, %D420) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<64x64x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D420) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R4.2048, %D420) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.cast"(%R9, %D420) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R7, %C0.6931471824645996, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R7, %D420) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.min"(%R10.2048, %C127, %D420) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R7, %C-127, %D420) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D420) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D420) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R10.2048, %R7, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R7, %C1.0, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.div"(%C1.0, %R9, %D420) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R7, %B1 = "arith.mul"(%R7, %R4.2048, %D420) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G9850880, %D0 = "dma.tensor"(%R7, %B3999) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [409600, 6400, 80, 1]>, none), %R7, %B1 = "conv.normal"(%R13, %R12, %R15, %C0.0, %D421) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G0, %B3999) : (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D422) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R7, %D422) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R9, %D422) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R5, %C0.6931471824645996, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R5, %D422) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.min"(%R10.2048, %C127, %D422) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R5, %C-127, %D422) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D422) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D422) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R10.2048, %R5, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R5, %C1.0, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.div"(%C1.0, %R9, %D422) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R5, %R7, %D422) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R12.4608, %D0 = "dma.tensor"(%G19148800, %B4000) : (memref<1x64x256x1xf32, strides: [16384, 256, 1, 1]>, none) -> (memref<1x64x256x1xf32, strides: [512, 256, 1, 1]>, none), %R14.3328, %D0 = "dma.tensor"(%G19214336, %B4000) : (memref<1x64x1x1xf32, strides: [64, 1, 1, 1]>, none) -> (memref<1x64x1x1xf32, strides: [2, 1, 1, 1]>, none), %R6.2048, %B1 = "conv.normal"(%R0, %R12.4608, %R14.3328, %C0.0, %D424) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D424) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R6.2048, %D424) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.cast"(%R0, %D424) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R3, %D424) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.min"(%R1.2048, %C127, %D424) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R3, %C-127, %D424) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D424) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D424) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R1.2048, %R3, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R3, %C1.0, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R0, %D424) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R6.2048, %D424) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R11.4224, %D0 = "dma.tensor"(%G19218432, %B4017) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R0, %B1 = "arith.copy"(%R5, %D425) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R3, %D425) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R12.6656, %D0 = "dma.tensor"(%G19283968, %B4033) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R11.4224, %R12.6656, %C0.0, %D426) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R5, %B1 = "tsbc.s_bc"(%R0, %D426) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R2.4096, %B1 = "arith.sub"(%C0.0, %R6, %D426) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R2.4096, %C-3.4028198694267105e+35, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R2.4096, %C1.4426950216293335, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D426) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R2.4096, %R0, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.cast"(%R9, %D426) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.min"(%R2.4096, %C127, %D426) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R9, %C-127, %D426) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.add_satu"(%R2.4096, %C127, %C23, %D426) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "sfu.taylor_4x"(%R0, %R5, %D426) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R2.4096, %R9, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D426) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D426) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %D0 = "dma.tensor"(%G9835200, %B4036) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R7, %B1 = "conv.normal"(%R13, %R12, %R15, %C0.0, %D427) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17973248, %D0 = "dma.tensor"(%R9, %B4052) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G5120, %B4052) : (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D429) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R7, %D429) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R9, %D429) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R5, %C0.6931471824645996, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R5, %D429) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.min"(%R10.2048, %C127, %D429) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R5, %C-127, %D429) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D429) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D429) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R10.2048, %R5, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R5, %C1.0, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.div"(%C1.0, %R9, %D429) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R5, %R7, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "conv.normal"(%R0, %R12.4608, %R14.3328, %C0.0, %D429) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D429) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R6.2048, %D429) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.cast"(%R0, %D429) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R3, %D429) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.min"(%R1.2048, %C127, %D429) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R3, %C-127, %D429) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D429) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D429) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R1.2048, %R3, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R3, %C1.0, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R0, %D429) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R6.2048, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R5, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R3, %D429) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R11.4224, %R12.6656, %C0.0, %D429) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R5, %B1 = "tsbc.s_bc"(%R0, %D429) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R2.4096, %B1 = "arith.sub"(%C0.0, %R6, %D429) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R2.4096, %C-3.4028198694267105e+35, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R2.4096, %C1.4426950216293335, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D429) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R2.4096, %R0, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.cast"(%R9, %D429) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.min"(%R2.4096, %C127, %D429) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R9, %C-127, %D429) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.add_satu"(%R2.4096, %C127, %C23, %D429) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "sfu.taylor_4x"(%R0, %R5, %D429) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R2.4096, %R9, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D429) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D429) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %D0 = "dma.tensor"(%G9840320, %B4089) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R7, %B1 = "conv.normal"(%R13, %R12, %R15, %C0.0, %D430) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17978368, %D0 = "dma.tensor"(%R9, %B4105) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G10240, %B4105) : (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D432) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R7, %D432) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R9, %D432) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R5, %C0.6931471824645996, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R5, %D432) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.min"(%R10.2048, %C127, %D432) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R5, %C-127, %D432) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D432) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D432) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R10.2048, %R5, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R5, %C1.0, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.div"(%C1.0, %R9, %D432) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R5, %R7, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "conv.normal"(%R0, %R12.4608, %R14.3328, %C0.0, %D432) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D432) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R6.2048, %D432) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.cast"(%R0, %D432) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R3, %D432) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.min"(%R1.2048, %C127, %D432) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R3, %C-127, %D432) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D432) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D432) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R1.2048, %R3, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R3, %C1.0, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R0, %D432) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R6.2048, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R5, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R3, %D432) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R11.4224, %R12.6656, %C0.0, %D432) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R5, %B1 = "tsbc.s_bc"(%R0, %D432) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R2.4096, %B1 = "arith.sub"(%C0.0, %R6, %D432) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R2.4096, %C-3.4028198694267105e+35, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R2.4096, %C1.4426950216293335, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D432) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R2.4096, %R0, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.cast"(%R9, %D432) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.min"(%R2.4096, %C127, %D432) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R9, %C-127, %D432) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.add_satu"(%R2.4096, %C127, %C23, %D432) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "sfu.taylor_4x"(%R0, %R5, %D432) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R2.4096, %R9, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D432) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D432) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %D0 = "dma.tensor"(%G9845440, %B4142) : (memref<1x64x18x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, none), %R7, %B1 = "conv.normal"(%R13, %R12, %R15, %C0.0, %D433) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x18x80xf32, strides: [2880, 1440, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17983488, %D0 = "dma.tensor"(%R9, %B4158) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G15360, %B4158) : (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D435) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R7, %D435) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R9, %D435) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R5, %C0.6931471824645996, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R5, %D435) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.min"(%R10.2048, %C127, %D435) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R5, %C-127, %D435) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D435) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D435) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R10.2048, %R5, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R5, %C1.0, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.div"(%C1.0, %R9, %D435) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R5, %R7, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "conv.normal"(%R0, %R12.4608, %R14.3328, %C0.0, %D435) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D435) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R6.2048, %D435) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.cast"(%R0, %D435) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R3, %D435) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.min"(%R1.2048, %C127, %D435) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R3, %C-127, %D435) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D435) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D435) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R1.2048, %R3, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R3, %C1.0, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R0, %D435) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R6.2048, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R5, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R3, %D435) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R11.4224, %R12.6656, %C0.0, %D435) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R5, %B1 = "tsbc.s_bc"(%R0, %D435) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R2.4096, %B1 = "arith.sub"(%C0.0, %R6, %D435) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R2.4096, %C-3.4028198694267105e+35, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R2.4096, %C1.4426950216293335, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D435) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R2.4096, %R0, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.cast"(%R9, %D435) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.min"(%R2.4096, %C127, %D435) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R9, %C-127, %D435) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.add_satu"(%R2.4096, %C127, %C23, %D435) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "sfu.taylor_4x"(%R0, %R5, %D435) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R2.4096, %R9, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D435) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D435) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R13, %D0 = "dma.tensor"(%G9850560, %B4195) : (memref<1x64x17x80xf32, strides: [409600, 6400, 80, 1]>, none) -> (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, none), %R7, %B1 = "conv.normal"(%R13, %R12, %R15, %C0.0, %D436) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x64x17x80xf32, strides: [2720, 1360, 80, 1]>, memref<64x64x3x3xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %G17988608, %D0 = "dma.tensor"(%R9, %B4211) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G20480, %B4211) : (memref<1x256x16x80xf32, strides: [1638400, 6400, 80, 1]>, none) -> (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, none), %R11.4096, %B1 = "tsbc.s_bc"(%R0, %D438) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R10.2048, %B1 = "arith.sub"(%C0.0, %R7, %D438) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R10.2048, %C-3.4028198694267105e+35, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R10.2048, %C1.4426950216293335, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.cast"(%R9, %D438) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R5, %C0.6931471824645996, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.sub"(%R10.2048, %R9, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R5, %D438) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.min"(%R10.2048, %C127, %D438) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "arith.max"(%R5, %C-127, %D438) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.add_satu"(%R10.2048, %C127, %C23, %D438) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R10.2048, %B1 = "sfu.taylor_4x"(%R9, %R11.4096, %D438) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R10.2048, %R5, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R9, %B1 = "arith.add"(%R5, %C1.0, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.div"(%C1.0, %R9, %D438) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R5, %B1 = "arith.mul"(%R5, %R7, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R6.2048, %B1 = "conv.normal"(%R0, %R12.4608, %R14.3328, %C0.0, %D438) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x16x80xf32, strides: [10240, 1280, 80, 1]>, memref<64x256x1x1xf32>, memref<1x64x1x1xui32, strides: [2, 1, 1, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R2.4096, %B1 = "tsbc.s_bc"(%R0, %D438) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.2048, %B1 = "arith.sub"(%C0.0, %R6.2048, %D438) {round_mode = 0} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R1.2048, %C-3.4028198694267105e+35, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R1.2048, %C1.4426950216293335, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.cast"(%R0, %D438) {round_mode = 3} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R1.2048, %R0, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.cast"(%R3, %D438) {round_mode = 1} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.min"(%R1.2048, %C127, %D438) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.max"(%R3, %C-127, %D438) {round_mode = 0} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, none) -> (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.add_satu"(%R1.2048, %C127, %C23, %D438) {round_mode = 1} : (memref<1x64x16x80xsi16, strides: [2560, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x64x16x80xsi32, strides: [2560, 1280, 80, 1]>, none), %R1.2048, %B1 = "sfu.taylor_4x"(%R0, %R2.4096, %D438) : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R1.2048, %R3, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R3, %C1.0, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, f32, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R0, %D438) {iter = 3} : (f32, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R6.2048, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none), %R0, %B1 = "arith.copy"(%R5, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R1.2048, %B1 = "arith.copy"(%R3, %D438) {round_mode = 0} : (memref<1x64x16x80xf32, strides: [2560, 1280, 80, 1]>, none) -> (memref<1x64x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R11.4224, %R12.6656, %C0.0, %D438) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R5, %B1 = "tsbc.s_bc"(%R0, %D438) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R2.4096, %B1 = "arith.sub"(%C0.0, %R6, %D438) {round_mode = 0} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R2.4096, %C-3.4028198694267105e+35, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R2.4096, %C1.4426950216293335, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D438) {round_mode = 3} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.sub"(%R2.4096, %R0, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.cast"(%R9, %D438) {round_mode = 1} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.min"(%R2.4096, %C127, %D438) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "arith.max"(%R9, %C-127, %D438) {round_mode = 0} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, none) -> (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.add_satu"(%R2.4096, %C127, %C23, %D438) {round_mode = 1} : (memref<1x128x16x80xsi16, strides: [5120, 1280, 80, 1]>, si16, ui8, none) -> (memref<1x128x16x80xsi32, strides: [5120, 1280, 80, 1]>, none), %R2.4096, %B1 = "sfu.taylor_4x"(%R0, %R5, %D438) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R2.4096, %R9, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, f32, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D438) {iter = 3} : (f32, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R6, %D438) {round_mode = 0} : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none), %G17993728, %D0 = "dma.tensor"(%R9, %B4264) : (memref<1x128x16x80xf32, strides: [5120, 1280, 80, 1]>, none) -> (memref<1x128x16x80xf32, strides: [819200, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G19288064, %B4264) : (memref<1x128x1152x1xf32, strides: [147456, 1152, 1, 1]>, none) -> (memref<1x128x1152x1xf32, strides: [4608, 1152, 1, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G19877888, %B4264) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R3, %D0 = "dma.tensor"(%G17973248, %B4264) : (memref<1x128x20x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x20x80xf32, strides: [6400, 1600, 80, 1]>, none), %R11, %B1 = "conv.normal"(%R3, %R0, %R2.2048, %C0.0, %D442) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x80xf32, strides: [6400, 1600, 80, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %R7, %D0 = "dma.tensor"(%G17979328, %B4264) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R7, %R0, %R2.2048, %C0.0, %D443) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G0, %D0 = "dma.tensor"(%R11, %B4265) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R3, %D0 = "dma.tensor"(%G17985728, %B4265) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R11, %B1 = "conv.normal"(%R3, %R0, %R2.2048, %C0.0, %D445) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G1600, %D0 = "dma.tensor"(%R12, %B4266) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R7, %D0 = "dma.tensor"(%G17992128, %B4266) : (memref<1x128x21x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, none), %R12, %B1 = "conv.normal"(%R7, %R0, %R2.2048, %C0.0, %D447) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x80xf32, strides: [6720, 1680, 80, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none), %G3200, %D0 = "dma.tensor"(%R11, %B4267) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %G4800, %D0 = "dma.tensor"(%R12, %B4268) : (memref<1x128x10x40xf32, strides: [1600, 400, 40, 1]>, none) -> (memref<1x128x10x40xf32, strides: [204800, 1600, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G0, %B4268) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D450) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R0, %D450) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.cast"(%R2, %D450) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R6, %D450) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.min"(%R3.4608, %C127, %D450) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R6, %C-127, %D450) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D450) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D450) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R3.4608, %R6, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R6, %C1.0, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R2, %D450) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R0, %D450) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %D0 = "dma.tensor"(%G11468800, %B4268) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.4608, %D0 = "dma.tensor"(%G19881984, %B4268) : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D452) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.copy"(%R8, %D452) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R11.7168, %D0 = "dma.tensor"(%G20013056, %B4284) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R9.4608, %R11.7168, %C0.0, %D453) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G1638400, %D0 = "dma.tensor"(%R0, %B4286) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D454) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R6, %D454) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D454) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R0, %D454) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R3.4608, %C127, %D454) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R0, %C-127, %D454) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D454) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D454) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R3.4608, %R0, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D454) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R6, %D454) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.512, %D0 = "dma.tensor"(%G20017152, %B4287) : (memref<1x128x128x1xf32, strides: [16384, 128, 1, 1]>, none) -> (memref<1x128x128x1xf32, strides: [512, 128, 1, 1]>, none), %R11.7184, %D0 = "dma.tensor"(%G20082688, %B4287) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R2, %B1 = "conv.normal"(%R0, %R10.512, %R11.7184, %C0.0, %D456) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D456) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R2, %D456) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.cast"(%R4, %D456) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R10.2560, %C0.6931471824645996, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R10.2560, %D456) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.min"(%R5.4608, %C127, %D456) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R10.2560, %C-127, %D456) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D456) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D456) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.mul"(%R5.4608, %R10.2560, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R10.2560, %C1.0, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.div"(%C1.0, %R4, %D456) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.mul"(%R10.2560, %R2, %D456) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G3200, %B4304) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D457) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R0, %D457) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.cast"(%R2, %D457) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R6, %C0.6931471824645996, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R6, %D457) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.min"(%R3.4608, %C127, %D457) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R6, %C-127, %D457) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D457) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D457) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R3.4608, %R6, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R6, %C1.0, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.div"(%C1.0, %R2, %D457) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R6, %R0, %D457) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R8, %D0 = "dma.tensor"(%G11472000, %B4320) : (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G3276800, %D0 = "dma.tensor"(%R10.2560, %B4320) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R0, %B1 = "arith.copy"(%R6, %D459) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R1.4608, %B1 = "arith.copy"(%R8, %D459) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R6, %B1 = "conv.normal"(%R0, %R9.4608, %R11.7168, %C0.0, %D459) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G1641600, %D0 = "dma.tensor"(%R0, %B4338) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R5.1024, %B1 = "tsbc.s_bc"(%R0, %D460) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R3.4608, %B1 = "arith.sub"(%C0.0, %R6, %D460) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R3.4608, %C-3.4028198694267105e+35, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R3.4608, %C1.4426950216293335, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.cast"(%R2, %D460) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.mul"(%R0, %C0.6931471824645996, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.sub"(%R3.4608, %R2, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.cast"(%R0, %D460) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.min"(%R3.4608, %C127, %D460) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "arith.max"(%R0, %C-127, %D460) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.add_satu"(%R3.4608, %C127, %C23, %D460) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R3.4608, %B1 = "sfu.taylor_4x"(%R2, %R5.1024, %D460) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R3.4608, %R0, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "arith.add"(%R0, %C1.0, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.div"(%C1.0, %R2, %D460) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R0, %B1 = "arith.mul"(%R0, %R6, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2, %B1 = "conv.normal"(%R0, %R10.512, %R11.7184, %C0.0, %D460) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<128x128x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D460) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R2, %D460) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.cast"(%R4, %D460) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R10.2560, %C0.6931471824645996, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R10.2560, %D460) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.min"(%R5.4608, %C127, %D460) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R10.2560, %C-127, %D460) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D460) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D460) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.mul"(%R5.4608, %R10.2560, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R10.2560, %C1.0, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.div"(%C1.0, %R4, %D460) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R10.2560, %B1 = "arith.mul"(%R10.2560, %R2, %D460) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G3280000, %D0 = "dma.tensor"(%R10.2560, %B4372) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [204800, 1600, 40, 1]>, none), %R13, %D0 = "dma.tensor"(%G3276800, %B4372) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G20086784, %B4372) : (memref<1x128x128x9xf32, strides: [147456, 1152, 9, 1]>, none) -> (memref<1x128x128x9xf32, strides: [4608, 1152, 9, 1]>, none), %R14.5264, %D0 = "dma.tensor"(%G20676608, %B4372) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R10, %B1 = "conv.normal"(%R13, %R0, %R14.5264, %C0.0, %D464) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R9.1024, %B1 = "tsbc.s_bc"(%R0, %D464) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.4608, %B1 = "arith.sub"(%C0.0, %R10, %D464) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R7.4608, %C-3.4028198694267105e+35, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R7.4608, %C1.4426950216293335, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.cast"(%R6, %D464) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R12.4608, %C0.6931471824645996, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.sub"(%R7.4608, %R6, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.cast"(%R12.4608, %D464) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.min"(%R7.4608, %C127, %D464) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R12.4608, %C-127, %D464) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.add_satu"(%R7.4608, %C127, %C23, %D464) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "sfu.taylor_4x"(%R6, %R9.1024, %D464) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R7.4608, %R12.4608, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R12.4608, %C1.0, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.div"(%C1.0, %R6, %D464) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R12.4608, %R10, %D464) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G1638400, %B4373) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R15, %D0 = "dma.tensor"(%G20680704, %B4373) : (memref<1x128x256x1xf32, strides: [32768, 256, 1, 1]>, none) -> (memref<1x128x256x1xf32, strides: [1024, 256, 1, 1]>, none), %R14.5248, %D0 = "dma.tensor"(%G20811776, %B4373) : (memref<1x128x1x1xf32, strides: [128, 1, 1, 1]>, none) -> (memref<1x128x1x1xf32, strides: [4, 1, 1, 1]>, none), %R8, %B1 = "conv.normal"(%R2.2048, %R15, %R14.5248, %C0.0, %D467) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D467) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R8, %D467) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.cast"(%R4, %D467) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R2.2048, %C0.6931471824645996, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R2.2048, %D467) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.min"(%R5.4608, %C127, %D467) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R2.2048, %C-127, %D467) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D467) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D467) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.mul"(%R5.4608, %R2.2048, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R2.2048, %C1.0, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.div"(%C1.0, %R4, %D467) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.mul"(%R2.2048, %R8, %D467) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R11.4608, %D0 = "dma.tensor"(%G20815872, %B4390) : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4608, %D468) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.copy"(%R2.2048, %D468) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R15.4096, %D0 = "dma.tensor"(%G21078016, %B4406) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R3, %B1 = "conv.normal"(%R7, %R11.4608, %R15.4096, %C0.0, %D469) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R13, %D0 = "dma.tensor"(%G3279840, %B4408) : (memref<1x128x21x40xf32, strides: [204800, 1600, 40, 1]>, none) -> (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, none), %R10, %B1 = "conv.normal"(%R13, %R0, %R14.5264, %C0.0, %D470) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x21x40xf32, strides: [3360, 840, 40, 1]>, memref<128x128x3x3xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %G0, %D0 = "dma.tensor"(%R3, %B4409) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R9.1024, %B1 = "tsbc.s_bc"(%R0, %D471) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R7.4608, %B1 = "arith.sub"(%C0.0, %R10, %D471) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R7.4608, %C-3.4028198694267105e+35, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R7.4608, %C1.4426950216293335, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.cast"(%R6, %D471) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.mul"(%R12.4608, %C0.6931471824645996, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.sub"(%R7.4608, %R6, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.cast"(%R12.4608, %D471) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.min"(%R7.4608, %C127, %D471) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "arith.max"(%R12.4608, %C-127, %D471) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.add_satu"(%R7.4608, %C127, %C23, %D471) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R7.4608, %B1 = "sfu.taylor_4x"(%R6, %R9.1024, %D471) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R7.4608, %R12.4608, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R6, %B1 = "arith.add"(%R12.4608, %C1.0, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.div"(%C1.0, %R6, %D471) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R12.4608, %R10, %D471) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %D0 = "dma.tensor"(%G1641600, %B4410) : (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R2.2048, %R15, %R14.5248, %C0.0, %D472) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<128x256x1x1xf32>, memref<1x128x1x1xui32, strides: [4, 1, 1, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D472) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R8, %D472) {round_mode = 0} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.cast"(%R4, %D472) {round_mode = 3} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.mul"(%R2.2048, %C0.6931471824645996, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R2.2048, %D472) {round_mode = 1} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.min"(%R5.4608, %C127, %D472) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "arith.max"(%R2.2048, %C-127, %D472) {round_mode = 0} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, none) -> (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D472) {round_mode = 1} : (memref<1x128x20x40xsi16, strides: [3200, 800, 40, 1]>, si16, ui8, none) -> (memref<1x128x20x40xsi32, strides: [3200, 800, 40, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D472) : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.mul"(%R5.4608, %R2.2048, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R4, %B1 = "arith.add"(%R2.2048, %C1.0, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, f32, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.div"(%C1.0, %R4, %D472) {iter = 3} : (f32, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R2.2048, %B1 = "arith.mul"(%R2.2048, %R8, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none), %R7, %B1 = "arith.copy"(%R12.4608, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R8.4608, %B1 = "arith.copy"(%R2.2048, %D472) {round_mode = 0} : (memref<1x128x20x40xf32, strides: [3200, 800, 40, 1]>, none) -> (memref<1x128x20x40xf32, strides: [6400, 800, 40, 1]>, none), %R3, %B1 = "conv.normal"(%R7, %R11.4608, %R15.4096, %C0.0, %D472) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none), %G3200, %D0 = "dma.tensor"(%R3, %B4446) : (memref<1x256x20x40xf32, strides: [6400, 800, 40, 1]>, none) -> (memref<1x256x20x40xf32, strides: [409600, 1600, 40, 1]>, none), %R0, %D0 = "dma.matrix"(%G0, %B4446) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D474) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D474) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D474) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D474) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D474) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D474) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D474) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D474) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D474) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D474) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R1, %D0 = "dma.matrix"(%G262144, %B4446) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D475) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D475) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D475) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D475) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D475) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D475) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D475) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D475) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D475) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D475) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G21250048, %D0 = "dma.matrix"(%R2, %B4462) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G524288, %B4462) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D477) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D477) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D477) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D477) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D477) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D477) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D477) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D477) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D477) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D477) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G21512192, %D0 = "dma.matrix"(%R3, %B4478) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R1, %D0 = "dma.matrix"(%G786432, %B4478) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D479) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D479) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D479) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D479) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D479) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D479) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D479) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D479) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D479) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D479) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G21774336, %D0 = "dma.matrix"(%R2, %B4494) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G1048576, %B4494) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D481) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D481) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D481) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D481) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D481) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D481) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D481) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D481) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D481) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D481) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G22036480, %D0 = "dma.matrix"(%R3, %B4510) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R1, %D0 = "dma.matrix"(%G1310720, %B4510) : (memref<1x1x4x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<4x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D483) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R1, %D483) {round_mode = 0} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.cast"(%R4, %D483) {round_mode = 3} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R3, %C0.6931471824645996, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R3, %D483) {round_mode = 1} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.min"(%R5, %C127, %D483) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R3, %C-127, %D483) {round_mode = 0} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D483) {round_mode = 1} : (memref<4x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<4x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D483) : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R5, %R3, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R3, %C1.0, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.div"(%C1.0, %R4, %D483) {iter = 3} : (f32, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R3, %B1 = "arith.mul"(%R3, %R1, %D483) {round_mode = 0} : (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<4x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G22298624, %D0 = "dma.matrix"(%R2, %B4526) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R0, %D0 = "dma.matrix"(%G1572864, %B4526) : (memref<1x1x1x16384xf32, strides: [0, 0, 16384, 1]>, none) -> (memref<1x16384xf32, strides: [512, 512, 0, 1]>, none), %R6, %B1 = "tsbc.s_bc"(%R0, %D485) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5, %B1 = "arith.sub"(%C0.0, %R0, %D485) {round_mode = 0} : (f32, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R5, %C-3.4028198694267105e+35, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R5, %C1.4426950216293335, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.cast"(%R4, %D485) {round_mode = 3} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.mul"(%R2, %C0.6931471824645996, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.sub"(%R5, %R4, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.cast"(%R2, %D485) {round_mode = 1} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.min"(%R5, %C127, %D485) {round_mode = 0} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "arith.max"(%R2, %C-127, %D485) {round_mode = 0} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, none) -> (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.add_satu"(%R5, %C127, %C23, %D485) {round_mode = 1} : (memref<1x32x1x512xsi16, strides: [512, 512, 512, 1]>, si16, ui8, none) -> (memref<1x32x1x512xsi32, strides: [512, 512, 512, 1]>, none), %R5, %B1 = "sfu.taylor_4x"(%R4, %R6, %D485) : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R5, %R2, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R4, %B1 = "arith.add"(%R2, %C1.0, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, f32, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.div"(%C1.0, %R4, %D485) {iter = 3} : (f32, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %R2, %B1 = "arith.mul"(%R2, %R0, %D485) {round_mode = 0} : (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none) -> (memref<1x32x1x512xf32, strides: [512, 512, 512, 1]>, none), %G22560768, %D0 = "dma.matrix"(%R3, %B4542) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %G22822912, %D0 = "dma.matrix"(%R2, %B4558) : (memref<1x1x0x512xf32, strides: [0, 0, 0, 1]>, none) -> (memref<0x512xf32, strides: [0, 0, 0, 1]>, none), %R15, %D0 = "dma.tensor"(%G21250048, %B4558) : (memref<1x256x4x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x4x40xf32, strides: [1280, 160, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G21082112, %B4558) : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [18432, 2304, 9, 1]>, none), %R10.5920, %D0 = "dma.tensor"(%G23441408, %B4558) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D490) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x4x40xf32, strides: [1280, 160, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R9, %D0 = "dma.tensor"(%G17973248, %B4558) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R14, %D0 = "dma.tensor"(%G23445504, %B4558) : (memref<1x255x128x1xf32, strides: [32640, 128, 1, 1]>, none) -> (memref<1x255x128x1xf32, strides: [1024, 128, 1, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D492) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D492) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D492) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D492) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D492) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D492) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D492) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D492) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D492) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D492) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23707648, %B4559) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.5888, %D0 = "dma.tensor"(%G23576576, %B4559) : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [8, 1, 1, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D494) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D494) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D494) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22888448, %D0 = "dma.tensor"(%R10.3328, %B4577) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21250528, %B4577) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D496) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6529024, %D0 = "dma.tensor"(%R11, %B4578) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17975808, %B4578) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D498) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D498) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D498) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D498) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D498) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D498) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D498) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D498) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D498) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D498) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23707808, %B4579) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D499) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D499) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D499) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22888608, %D0 = "dma.tensor"(%R10.3328, %B4597) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21251168, %B4597) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D501) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6531584, %D0 = "dma.tensor"(%R11, %B4598) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17978368, %B4598) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D503) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D503) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D503) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D503) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D503) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D503) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D503) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D503) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D503) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D503) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23707968, %B4599) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D504) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D504) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D504) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22888768, %D0 = "dma.tensor"(%R10.3328, %B4617) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21251808, %B4617) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D506) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6534144, %D0 = "dma.tensor"(%R11, %B4618) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17980928, %B4618) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D508) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D508) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D508) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D508) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D508) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D508) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D508) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D508) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D508) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D508) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708128, %B4619) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D509) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D509) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D509) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22888928, %D0 = "dma.tensor"(%R10.3328, %B4637) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21252448, %B4637) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D511) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6536704, %D0 = "dma.tensor"(%R11, %B4638) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17983488, %B4638) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D513) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D513) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D513) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D513) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D513) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D513) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D513) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D513) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D513) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D513) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708288, %B4639) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D514) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D514) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D514) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889088, %D0 = "dma.tensor"(%R10.3328, %B4657) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21253088, %B4657) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D516) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6539264, %D0 = "dma.tensor"(%R11, %B4658) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17986048, %B4658) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D518) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D518) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D518) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D518) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D518) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D518) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D518) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D518) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D518) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D518) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708448, %B4659) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D519) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D519) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D519) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889248, %D0 = "dma.tensor"(%R10.3328, %B4677) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21253728, %B4677) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D521) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6541824, %D0 = "dma.tensor"(%R11, %B4678) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17988608, %B4678) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D523) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D523) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D523) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D523) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D523) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D523) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D523) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D523) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D523) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D523) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708608, %B4679) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D524) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D524) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D524) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889408, %D0 = "dma.tensor"(%R10.3328, %B4697) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21254368, %B4697) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D526) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6544384, %D0 = "dma.tensor"(%R11, %B4698) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17991168, %B4698) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D528) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D528) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D528) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D528) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D528) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D528) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D528) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D528) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D528) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D528) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708768, %B4699) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D529) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D529) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D529) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889568, %D0 = "dma.tensor"(%R10.3328, %B4717) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21255008, %B4717) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D531) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6546944, %D0 = "dma.tensor"(%R11, %B4718) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17993728, %B4718) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D533) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D533) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D533) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D533) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D533) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D533) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D533) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D533) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D533) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D533) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23708928, %B4719) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D534) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D534) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D534) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889728, %D0 = "dma.tensor"(%R10.3328, %B4737) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %R15, %D0 = "dma.tensor"(%G21255648, %B4737) : (memref<1x256x5x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, none), %R10.3328, %B1 = "conv.normal"(%R15, %R0, %R10.5920, %C0.0, %D536) {kernel = [3, 3], stride = [2, 2], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x5x40xf32, strides: [1600, 200, 40, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %G6549504, %D0 = "dma.tensor"(%R11, %B4738) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R9, %D0 = "dma.tensor"(%G17996288, %B4738) : (memref<1x128x8x80xf32, strides: [819200, 6400, 80, 1]>, none) -> (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, none), %R12.2560, %B1 = "tsbc.s_bc"(%R0, %D538) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R12.1280, %B1 = "arith.sub"(%C0.0, %R10.3328, %D538) {round_mode = 0} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R12.1280, %C-3.4028198694267105e+35, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12.1280, %C1.4426950216293335, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.cast"(%R12, %D538) {round_mode = 3} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R10.2048, %C0.6931471824645996, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R12.1280, %R12, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.cast"(%R10.2048, %D538) {round_mode = 1} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.min"(%R12.1280, %C127, %D538) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "arith.max"(%R10.2048, %C-127, %D538) {round_mode = 0} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, none) -> (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.add_satu"(%R12.1280, %C127, %C23, %D538) {round_mode = 1} : (memref<1x256x2x20xsi16, strides: [320, 40, 20, 1]>, si16, ui8, none) -> (memref<1x256x2x20xsi32, strides: [320, 40, 20, 1]>, none), %R12.1280, %B1 = "sfu.taylor_4x"(%R12, %R12.2560, %D538) : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R12.1280, %R10.2048, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R12, %B1 = "arith.add"(%R10.2048, %C1.0, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, f32, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.div"(%C1.0, %R12, %D538) {iter = 3} : (f32, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.2048, %B1 = "arith.mul"(%R10.2048, %R10.3328, %D538) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G23709088, %B4739) : (memref<1x256x2x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none), %R10.3328, %B1 = "arith.copy"(%R10.2048, %D539) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R10.4608, %B1 = "arith.copy"(%R11, %D539) {round_mode = 0} : (memref<1x256x2x20xf32, strides: [320, 40, 20, 1]>, none) -> (memref<1x256x2x20xf32, strides: [640, 40, 20, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R14, %R10.5888, %C0.0, %D539) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x128x8x80xf32, strides: [2560, 640, 80, 1]>, memref<255x128x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none), %G22889888, %D0 = "dma.tensor"(%R10.3328, %B4757) : (memref<1x512x2x20xf32, strides: [640, 40, 20, 1]>, none) -> (memref<1x512x2x20xf32, strides: [204800, 400, 20, 1]>, none), %G6552064, %D0 = "dma.tensor"(%R11, %B4758) : (memref<1x255x8x80xf32, strides: [5120, 640, 80, 1]>, none) -> (memref<1x255x8x80xf32, strides: [1632000, 6400, 80, 1]>, none), %R0, %D0 = "dma.tensor"(%G6529024, %B4758) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R8, %B1 = "tsbc.wc_ts"(%R0, %D542) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %R4, %D0 = "dma.tensor"(%G6532608, %B4758) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R12, %B1 = "tsbc.wc_ts"(%R4, %D543) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G0, %D0 = "dma.tensor"(%R8, %B4759) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R0, %D0 = "dma.tensor"(%G6536192, %B4759) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R8, %B1 = "tsbc.wc_ts"(%R0, %D545) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G304640, %D0 = "dma.tensor"(%R12, %B4760) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R4, %D0 = "dma.tensor"(%G6539776, %B4760) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R12, %B1 = "tsbc.wc_ts"(%R4, %D547) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G609280, %D0 = "dma.tensor"(%R8, %B4761) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R0, %D0 = "dma.tensor"(%G6543360, %B4761) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R8, %B1 = "tsbc.wc_ts"(%R0, %D549) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G913920, %D0 = "dma.tensor"(%R12, %B4762) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R4, %D0 = "dma.tensor"(%G6546944, %B4762) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R12, %B1 = "tsbc.wc_ts"(%R4, %D551) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G1218560, %D0 = "dma.tensor"(%R8, %B4763) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R0, %D0 = "dma.tensor"(%G6550528, %B4763) : (memref<3x85x1x896xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none), %R8, %B1 = "tsbc.wc_ts"(%R0, %D553) : (memref<3x85x1x896xf32, strides: [2688, 896, 896, 1]>, none) -> (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none), %G1523200, %D0 = "dma.tensor"(%R12, %B4764) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R4, %D0 = "dma.tensor"(%G6554112, %B4764) : (memref<3x85x1x128xf32, strides: [544000, 6400, 6400, 1]>, none) -> (memref<3x85x1x128xf32, strides: [384, 128, 128, 1]>, none), %R12, %B1 = "tsbc.wc_ts"(%R4, %D555) : (memref<3x85x1x128xf32, strides: [384, 128, 128, 1]>, none) -> (memref<3x128x1x85xf32, strides: [352, 88, 85, 1]>, none), %G1827840, %D0 = "dma.tensor"(%R8, %B4765) : (memref<3x896x1x85xf32, strides: [2464, 88, 85, 1]>, none) -> (memref<3x896x1x85xf32, strides: [544000, 85, 85, 1]>, none), %G2132480, %D0 = "dma.tensor"(%R12, %B4766) : (memref<3x128x1x85xf32, strides: [352, 88, 85, 1]>, none) -> (memref<3x128x1x85xf32, strides: [544000, 85, 85, 1]>, none), %R0, %D0 = "dma.tensor"(%G23580672, %B4766) : (memref<1x255x256x1xf32, strides: [65280, 256, 1, 1]>, none) -> (memref<1x255x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R1, %D0 = "dma.tensor"(%G23842816, %B4766) : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [8, 1, 1, 1]>, none), %R2, %D0 = "dma.tensor"(%G21250048, %B4766) : (memref<1x256x14x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x14x40xf32, strides: [4480, 560, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R2, %R0, %R1, %C0.0, %D560) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x14x40xf32, strides: [4480, 560, 40, 1]>, memref<255x256x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x14x40xf32, strides: [4480, 560, 40, 1]>, none), %R5, %D0 = "dma.tensor"(%G21252288, %B4766) : (memref<1x256x13x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x13x40xf32, strides: [4160, 520, 40, 1]>, none), %R11, %B1 = "conv.normal"(%R5, %R0, %R1, %C0.0, %D561) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x13x40xf32, strides: [4160, 520, 40, 1]>, memref<255x256x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x13x40xf32, strides: [4160, 520, 40, 1]>, none), %G17973248, %D0 = "dma.tensor"(%R8, %B4767) : (memref<1x255x14x40xf32, strides: [4480, 560, 40, 1]>, none) -> (memref<1x255x14x40xf32, strides: [408000, 1600, 40, 1]>, none), %R2, %D0 = "dma.tensor"(%G21254368, %B4767) : (memref<1x256x13x40xf32, strides: [409600, 1600, 40, 1]>, none) -> (memref<1x256x13x40xf32, strides: [4160, 520, 40, 1]>, none), %R8, %B1 = "conv.normal"(%R2, %R0, %R1, %C0.0, %D563) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x13x40xf32, strides: [4160, 520, 40, 1]>, memref<255x256x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x13x40xf32, strides: [4160, 520, 40, 1]>, none), %G17975488, %D0 = "dma.tensor"(%R11, %B4768) : (memref<1x255x13x40xf32, strides: [4160, 520, 40, 1]>, none) -> (memref<1x255x13x40xf32, strides: [408000, 1600, 40, 1]>, none), %G17977568, %D0 = "dma.tensor"(%R8, %B4769) : (memref<1x255x13x40xf32, strides: [4160, 520, 40, 1]>, none) -> (memref<1x255x13x40xf32, strides: [408000, 1600, 40, 1]>, none), %R0, %D0 = "dma.tensor"(%G17973248, %B4769) : (memref<85x32x50x1xf32, strides: [1600, 50, 1, 1]>, none) -> (memref<85x32x50x1xf32, strides: [50, 50, 1, 1]>, none), %R8, %B1 = "arith.copy"(%R0, %D566) {round_mode = 0} : (memref<1x32x50x85xf32, strides: [4250, 50, 1, 50]>, none) -> (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none), %R4, %D0 = "dma.tensor"(%G18517248, %B4769) : (memref<85x32x50x1xf32, strides: [1600, 50, 1, 1]>, none) -> (memref<85x32x50x1xf32, strides: [50, 50, 1, 1]>, none), %R12, %B1 = "arith.copy"(%R4, %D567) {round_mode = 0} : (memref<1x32x50x85xf32, strides: [4250, 50, 1, 50]>, none) -> (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none), %G6529024, %D0 = "dma.tensor"(%R8, %B4770) : (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none) -> (memref<1x32x50x85xf32, strides: [136000, 4250, 85, 1]>, none), %R0, %D0 = "dma.tensor"(%G19061248, %B4770) : (memref<85x32x50x1xf32, strides: [1600, 50, 1, 1]>, none) -> (memref<85x32x50x1xf32, strides: [50, 50, 1, 1]>, none), %R8, %B1 = "arith.copy"(%R0, %D569) {round_mode = 0} : (memref<1x32x50x85xf32, strides: [4250, 50, 1, 50]>, none) -> (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none), %G7073024, %D0 = "dma.tensor"(%R12, %B4771) : (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none) -> (memref<1x32x50x85xf32, strides: [136000, 4250, 85, 1]>, none), %G7617024, %D0 = "dma.tensor"(%R8, %B4772) : (memref<1x32x50x85xf32, strides: [4250, 4250, 85, 1]>, none) -> (memref<1x32x50x85xf32, strides: [136000, 4250, 85, 1]>, none), %R11, %D0 = "dma.tensor"(%G23846912, %B4772) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R0, %D0 = "dma.tensor"(%G22888448, %B4772) : (memref<1x512x20x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x20x20xf32, strides: [6400, 400, 20, 1]>, none), %R10, %D0 = "dma.tensor"(%G24371200, %B4772) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R4, %B1 = "conv.normal"(%R0, %R11, %R10, %C0.0, %D574) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x20x20xf32, strides: [6400, 400, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R3.1024, %B1 = "tsbc.s_bc"(%R0, %D574) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R1.4608, %B1 = "arith.sub"(%C0.0, %R4, %D574) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.max"(%R1.4608, %C-3.4028198694267105e+35, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R1.4608, %C1.4426950216293335, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.cast"(%R0, %D574) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.sub"(%R1.4608, %R0, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.cast"(%R9, %D574) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.min"(%R1.4608, %C127, %D574) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "arith.max"(%R9, %C-127, %D574) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.add_satu"(%R1.4608, %C127, %C23, %D574) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R1.4608, %B1 = "sfu.taylor_4x"(%R0, %R3.1024, %D574) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R1.4608, %R9, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %B1 = "arith.add"(%R9, %C1.0, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R0, %D574) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R4, %D574) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R13, %D0 = "dma.tensor"(%G24375296, %B4773) : (memref<1x256x256x1xf32, strides: [65536, 256, 1, 1]>, none) -> (memref<1x256x256x1xf32, strides: [2048, 256, 1, 1]>, none), %R14, %D0 = "dma.tensor"(%G24637440, %B4773) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R11, %B1 = "conv.normal"(%R9, %R13, %R14, %C0.0, %D576) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x256x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G24641536, %B4789) : (memref<1x256x256x9xf32, strides: [589824, 2304, 9, 1]>, none) -> (memref<1x256x256x9xf32, strides: [18432, 2304, 9, 1]>, none), %R15.5632, %B1 = "tsbc.s_bc"(%R0, %D577) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R14.1024, %B1 = "arith.sub"(%C0.0, %R11, %D577) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.max"(%R14.1024, %C-3.4028198694267105e+35, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R14.1024, %C1.4426950216293335, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.cast"(%R12.4608, %D577) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.mul"(%R9, %C0.6931471824645996, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.sub"(%R14.1024, %R12.4608, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.cast"(%R9, %D577) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.min"(%R14.1024, %C127, %D577) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "arith.max"(%R9, %C-127, %D577) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.add_satu"(%R14.1024, %C127, %C23, %D577) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R14.1024, %B1 = "sfu.taylor_4x"(%R12.4608, %R15.5632, %D577) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R14.1024, %R9, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R12.4608, %B1 = "arith.add"(%R9, %C1.0, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.div"(%C1.0, %R12.4608, %D577) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9, %R11, %D577) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R10.4608, %D0 = "dma.tensor"(%G27000832, %B4790) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R13, %B1 = "conv.normal"(%R9, %R0, %R10.4608, %C0.0, %D578) {kernel = [3, 3], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [1, 1, 1, 1], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<256x256x3x3xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R7.1024, %B1 = "tsbc.s_bc"(%R0, %D578) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R5.4608, %B1 = "arith.sub"(%C0.0, %R13, %D578) {round_mode = 0} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.max"(%R5.4608, %C-3.4028198694267105e+35, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.mul"(%R5.4608, %C1.4426950216293335, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.cast"(%R4, %D578) {round_mode = 3} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.sub"(%R5.4608, %R4, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.cast"(%R8, %D578) {round_mode = 1} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.min"(%R5.4608, %C127, %D578) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "arith.max"(%R8, %C-127, %D578) {round_mode = 0} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, none) -> (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.add_satu"(%R5.4608, %C127, %C23, %D578) {round_mode = 1} : (memref<1x256x20x20xsi16, strides: [3200, 400, 20, 1]>, si16, ui8, none) -> (memref<1x256x20x20xsi32, strides: [3200, 400, 20, 1]>, none), %R5.4608, %B1 = "sfu.taylor_4x"(%R4, %R7.1024, %D578) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R5.4608, %R8, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R4, %B1 = "arith.add"(%R8, %C1.0, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, f32, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R4, %D578) {iter = 3} : (f32, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R13, %D578) {round_mode = 0} : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none), %G8163328, %D0 = "dma.tensor"(%R8, %B4823) : (memref<1x256x20x20xf32, strides: [3200, 400, 20, 1]>, none) -> (memref<1x256x20x20xf32, strides: [102400, 400, 20, 1]>, none), %R10, %D0 = "dma.tensor"(%G22888448, %B4823) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R6, %D0 = "dma.tensor"(%G27004928, %B4823) : (memref<1x256x512x1xf32, strides: [131072, 512, 1, 1]>, none) -> (memref<1x256x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R15.1216, %D0 = "dma.tensor"(%G27529216, %B4823) : (memref<1x256x1x1xf32, strides: [256, 1, 1, 1]>, none) -> (memref<1x256x1x1xf32, strides: [8, 1, 1, 1]>, none), %R8, %B1 = "conv.normal"(%R10, %R6, %R15.1216, %C0.0, %D582) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G27533312, %B4823) : (memref<1x512x512x1xf32, strides: [262144, 512, 1, 1]>, none) -> (memref<1x512x512x1xf32, strides: [8192, 512, 1, 1]>, none), %R10.4608, %B1 = "tsbc.s_bc"(%R0, %D583) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.6400, %B1 = "arith.sub"(%C0.0, %R8, %D583) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.max"(%R9.6400, %C-3.4028198694267105e+35, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9.6400, %C1.4426950216293335, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.cast"(%R9, %D583) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R12, %C0.6931471824645996, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.sub"(%R9.6400, %R9, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.cast"(%R12, %D583) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.min"(%R9.6400, %C127, %D583) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.max"(%R12, %C-127, %D583) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.add_satu"(%R9.6400, %C127, %C23, %D583) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "sfu.taylor_4x"(%R9, %R10.4608, %D583) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R9.6400, %R12, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.add"(%R12, %C1.0, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.div"(%C1.0, %R9, %D583) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12, %R8, %D583) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G8163328, %B4824) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R15.1152, %D0 = "dma.tensor"(%G28581888, %B4824) : (memref<1x512x1x1xf32, strides: [512, 1, 1, 1]>, none) -> (memref<1x512x1x1xf32, strides: [16, 1, 1, 1]>, none), %R8, %B1 = "arith.copy"(%R11, %D585) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.copy"(%R12, %D585) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R0, %R15.1152, %C0.0, %D585) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R15.1024, %B1 = "tsbc.s_bc"(%R0, %D585) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.4608, %B1 = "arith.sub"(%C0.0, %R10, %D585) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R13.4608, %C-3.4028198694267105e+35, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R13.4608, %C1.4426950216293335, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.cast"(%R12, %D585) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R13.4608, %R12, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.cast"(%R8, %D585) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.min"(%R13.4608, %C127, %D585) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R8, %C-127, %D585) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.4608, %C127, %C23, %D585) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "sfu.taylor_4x"(%R12, %R15.1024, %D585) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R13.4608, %R8, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.add"(%R8, %C1.0, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R12, %D585) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R10, %D585) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R4, %D0 = "dma.tensor"(%G28585984, %B4843) : (memref<1x255x512x1xf32, strides: [130560, 512, 1, 1]>, none) -> (memref<1x255x512x1xf32, strides: [4096, 512, 1, 1]>, none), %R11.6400, %D0 = "dma.tensor"(%G29110272, %B4843) : (memref<1x255x1x1xf32, strides: [255, 1, 1, 1]>, none) -> (memref<1x255x1x1xf32, strides: [8, 1, 1, 1]>, none), %R12, %B1 = "conv.normal"(%R8, %R4, %R11.6400, %C0.0, %D587) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<255x512x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R10, %D0 = "dma.tensor"(%G22889248, %B4859) : (memref<1x512x10x20xf32, strides: [204800, 400, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "conv.normal"(%R10, %R6, %R15.1216, %C0.0, %D588) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<256x512x1x1xf32>, memref<1x256x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G8572928, %D0 = "dma.tensor"(%R12, %B4860) : (memref<1x255x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x255x10x20xf32, strides: [102000, 400, 20, 1]>, none), %R10.4608, %B1 = "tsbc.s_bc"(%R0, %D589) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R9.6400, %B1 = "arith.sub"(%C0.0, %R8, %D589) {round_mode = 0} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.max"(%R9.6400, %C-3.4028198694267105e+35, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R9.6400, %C1.4426950216293335, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.cast"(%R9, %D589) {round_mode = 3} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.mul"(%R12, %C0.6931471824645996, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.sub"(%R9.6400, %R9, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.cast"(%R12, %D589) {round_mode = 1} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.min"(%R9.6400, %C127, %D589) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "arith.max"(%R12, %C-127, %D589) {round_mode = 0} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, none) -> (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.add_satu"(%R9.6400, %C127, %C23, %D589) {round_mode = 1} : (memref<1x256x10x20xsi16, strides: [1600, 200, 20, 1]>, si16, ui8, none) -> (memref<1x256x10x20xsi32, strides: [1600, 200, 20, 1]>, none), %R9.6400, %B1 = "sfu.taylor_4x"(%R9, %R10.4608, %D589) : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R9.6400, %R12, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R9, %B1 = "arith.add"(%R12, %C1.0, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, f32, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.div"(%C1.0, %R9, %D589) {iter = 3} : (f32, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R12, %R8, %D589) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R11, %D0 = "dma.tensor"(%G8164128, %B4861) : (memref<1x256x10x20xf32, strides: [102400, 400, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none), %R8, %B1 = "arith.copy"(%R11, %D590) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8.6400, %B1 = "arith.copy"(%R12, %D590) {round_mode = 0} : (memref<1x256x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x256x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R10, %B1 = "conv.normal"(%R8, %R0, %R15.1152, %C0.0, %D590) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<512x512x1x1xf32>, memref<1x512x1x1xui32, strides: [16, 1, 1, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R15.1024, %B1 = "tsbc.s_bc"(%R0, %D590) : (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none) -> (memref<1x32x1x10xf32, strides: [12, 12, 10, 1]>, none), %R13.4608, %B1 = "arith.sub"(%C0.0, %R10, %D590) {round_mode = 0} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R13.4608, %C-3.4028198694267105e+35, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R13.4608, %C1.4426950216293335, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.cast"(%R12, %D590) {round_mode = 3} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.mul"(%R8, %C0.6931471824645996, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.sub"(%R13.4608, %R12, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.cast"(%R8, %D590) {round_mode = 1} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.min"(%R13.4608, %C127, %D590) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "arith.max"(%R8, %C-127, %D590) {round_mode = 0} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, none) -> (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.add_satu"(%R13.4608, %C127, %C23, %D590) {round_mode = 1} : (memref<1x512x10x20xsi16, strides: [3200, 200, 20, 1]>, si16, ui8, none) -> (memref<1x512x10x20xsi32, strides: [3200, 200, 20, 1]>, none), %R13.4608, %B1 = "sfu.taylor_4x"(%R12, %R15.1024, %D590) : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x64x1x10xf32, strides: [0, 0, 10, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R13.4608, %R8, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "arith.add"(%R8, %C1.0, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, f32, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.div"(%C1.0, %R12, %D590) {iter = 3} : (f32, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R8, %B1 = "arith.mul"(%R8, %R10, %D590) {round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none) -> (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, none), %R12, %B1 = "conv.normal"(%R8, %R4, %R11.6400, %C0.0, %D590) {kernel = [1, 1], stride = [1, 1], in_zero = [0, 0], ke_zero = [0, 0], opt_kernel_rotate = False, pad_mode = 0, pad = [0, 0, 0, 0], opt_res_add = False, do_relu = False, sym_range = False, do_rq = False, round_mode = 0} : (memref<1x512x10x20xf32, strides: [3200, 200, 20, 1]>, memref<255x512x1x1xf32>, memref<1x255x1x1xui32, strides: [8, 1, 1, 1]>, f32, none) -> (memref<1x255x10x20xf32, strides: [1600, 200, 20, 1]>, none), %G8573728, %D0 = "dma.tensor"(%R12, %B4897) : (memref<1x255x10x20xf32, strides: [1600, 200, 20, 1]>, none) -> (memref<1x255x10x20xf32, strides: [102000, 400, 20, 1]>, none), %R0, %D0 = "dma.tensor"(%G8572928, %B4897) : (memref<255x25x16x1xf32, strides: [400, 16, 1, 1]>, none) -> (memref<255x25x16x1xf32, strides: [16, 16, 1, 1]>, none), %R8, %B1 = "arith.copy"(%R0, %D592) {round_mode = 0} : (memref<3x25x16x85xf32, strides: [1360, 16, 1, 16]>, none) -> (memref<3x25x16x85xf32, strides: [1360, 1360, 85, 1]>, none), %G8163328, %D0 = "dma.tensor"(%R8, %B4898) : (memref<3x25x16x85xf32, strides: [1360, 1360, 85, 1]>, none) -> (memref<3x25x16x85xf32, strides: [34000, 1360, 85, 1]>, none)]