[DEFAULT]
model_path=${MODEL_ZOO_PATH}/language/nlp//Huggingface_bert_squadv1/bert_large_traced-4.23.0.pt
#model_path=${REGRESSION_PATH}/model/bert_large_traced-4.23.0.pt
test_input=${REGRESSION_PATH}/npz_input/bert-base_0.npz
input_shapes=[[1,384],[1,384],[1,384]]
input_types=int32,int32,int32
excepts="attention_mask0.1,input.4,input.10,input.16,input.22,input.28,input.34,input.40,input.46,input.52,input.58,input.64,input.70,input.76,input.82,input.88,input.94,input.100,input.106,input.112,input.118,input.124,input.130,input.136,input.2,1358,1448,1538,1628,1718,1808,1898,1911,1919,input.122,1988,2001,2009,input.128,2078,2091,2099,input.134,2168,2181,2189,input.140,2258,input0.75,context_layer.28,input1.26,context_layer.30,input1.28,context_layer.32,input1.30,context_layer.34,input1.32,context_layer.36,input1.34,context_layer.38,input1.36,context_layer.40,input1.38,input0.123,1975,context_layer.44,input1.42,input0.135,2155,2224,value_layer.1,context_layer.1,input1.1,input0.81,input0.87,input0.93,input0.99,input0.105,input0.111,input0.117,input1.40,input0.129,input1.44,input0.2,1345,1525,1705,1885,context_layer.42,2065,context_layer.46,2245,1435,1795,x0.1,1615,2090,attention_scores0.1,attention_scores.1,1649"

do_int8_sym=1
do_int8_asym=0
do_f16=1
do_bf16=1
do_f32=0
do_f8e4m3=0
do_f8e5m2=0

dataset=${REGRESSION_PATH}/dataset/BertBase/
debug_cmd=percentile9999
tune_num=0
input_num=20
use_quantize_table=1

[bm1684x]
int8_sym_tolerance=0.89,0.51
int8_asym_tolerance=0.71,0.17
